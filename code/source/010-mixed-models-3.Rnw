% !Rnw root = master.Rnw

<<cache=FALSE, echo=FALSE>>=
## trying this out to use num signif digits (which is what
## tidy print does by default) instead of decimal
options(knitr.digits.signif = TRUE)
options(digits = 2)
@

\chapter{Mixed-effects models 3: Practical and advanced topics}
\label{chap:mem-3}

This chapter covers practical aspects of using mixed-effects models which were abstracted away from in the previous two chapters: dealing with fitting problems (convergence/singularity: Section~\ref{sec:lmm-convergence}--\ref{sec:singular-ch10}), and choosing the terms to include in a model (model selection: Section~\ref{sec:lmm-model-selection}).  We also cover some topics which go beyond the standard use case for mixed-effects models in linguistics, where  fixed effects are of primary interest:
%; crossed subject/item random effects): 
additional types/uses of random effects (Section~\ref{sec:ranef-further-uses}), and obtaining model predictions/uncertainties for individual levels (i.e., by speaker) via simulation or bootstrapping (Section~\ref{sec:by-sub-uncert}), including for nonlinear effects (Section~\ref{sec:nonlin-mems}).

%of fitting/selecting models.

% - Includes intro to appendix: these topics involve a lot of refitting models + code output, in practice; takes a lot of time and space. We've put much of this in Appendix, to streamline presentation, and refer there as needed.  You should be able to just run 

% 
% - One general goal in this chapter: discussion of these issues that takes into account observational data and/or complex models. 
% 
% - For example, all resources so far (Seedorf, Brauer \& Curtin, Matushek...) assume fairly simple models of data from laboratory experiments, which do not extend straghtforwardly to e.g.,\ corpus data, with many fixed effects, or large datasets which take hours to fit. 

\section{Preliminaries}

\subsection{Packages}

This chapter assumes that you have loaded various packages from previous chapters. 

<<cache=FALSE, echo=1:9>>=
library(tidyverse)
library(broom)
library(arm)
library(lme4)
library(car)
library(broom.mixed)
library(languageR)
library(splines)
rescale <- arm::rescale

## RData file contains pre-fitted models for this chapter, from
## online appendix. evaluate library allows me to recreate
## warnings/messages in fitting without actually having to rerun the
## models when this chapter is compiled.
## NOTE: if you are compiling this Rnw file, you'll need the RData
## file to exist.
library(evaluate)
load("objects/convergence_appendix_mods.RData")
@


\subsection{Data}
\label{sec:givenness-data}

It also assumes you have set the default contrasts for factors to helmert contrasts (Section~\ref{sec:factors-discussion}), and loaded various datasets used in the past two chapters, with associated processing (Section~\ref{sec:lmm-prelim}, \ref{sec:melr-prelim} \ref{sec:cdi-data-new}):

<<>>=
options(contrasts = c("contr.helmert", "contr.poly"))

vot <- read.csv("data/vot_rmld.csv", stringsAsFactors = TRUE) %>%
  mutate(
    place = fct_relevel(place, "labial"),
    log_corpus_freq = rescale(log_corpus_freq),
    speaking_rate_dev = rescale(speaking_rate_dev),
    foll_high_vowel = rescale(foll_high_vowel),
    cons_cluster = rescale(cons_cluster),
    gender = rescale(gender),
    stress = rescale(stress)
  )

contrasts(vot$place) <- contr.helmert(3)

diatones <- read.csv("data/diatones_rmld.csv", 
  stringsAsFactors = TRUE) %>%
  mutate(
    syll1_coda_orig = syll1_coda,
    syll2_coda_orig = syll2_coda,
    syll2_td_orig = syll2_td,
    syll1_coda = rescale(syll1_coda_orig),
    syll2_coda = rescale(str_count(syll2_coda_orig, "C")),
    syll2_td = rescale(syll2_td_orig),
    frequency = rescale(frequency)
  )

neutralization <- read.csv("data/neutralization_rmld.csv", 
  stringsAsFactors = TRUE) %>%
  mutate(voicing_fact = fct_relevel(voicing, "voiceless")) %>%
  filter(!is.na(prosodic_boundary)) %>%
  mutate(
    prosodic_boundary = rescale(prosodic_boundary),
    voicing = rescale(voicing_fact),
    item_pair = as.factor(item_pair),
    subject = as.factor(subject)
  )

contrasts(neutralization$voicing_fact) <- contr.helmert(2)
contrasts(neutralization$place) <- contr.helmert(3)
contrasts(neutralization$vowel) <- contr.helmert(5)

givenness <- read.csv("data/givenness_rmld.csv", 
  stringsAsFactors = TRUE) %>%
  mutate(
    clabel.williams = rescale(conditionLabel),
    npType.pronoun = rescale(npType),
    voice.passive = rescale(voice),
    order.std = rescale(order),
    shifted = (as.numeric(stressshift) - 1),
    item = as.factor(item),
    participant = as.factor(participant)
  )
@


\subsubsection*{The \ttt{turkish\_if0} dataset}

We also assume you have loaded the \ttt{turkish\_if0} dataset:

<<>>=
turkish_if0 <- read.csv("data/turkish_if0_rmld.csv", 
                        stringsAsFactors = TRUE)
@


<<echo=FALSE>>=
n_turkish <- nrow(turkish_if0)
n_turkish_speaker <- length(unique(turkish_if0)$speaker)
n_turkish_word <- length(unique(turkish_if0)$word)
n_turkish_cons <- length(unique(turkish_if0)$consonant)
@

<<echo=FALSE>>=
n_turkish <- nrow(turkish_if0)
n_turkish_speaker <- length(unique(turkish_if0$speaker))
n_turkish_word <- length(unique(turkish_if0$word))
n_turkish_cons <- length(unique(turkish_if0$consonant))
@

This dataset comes from a cross-linguistic corpus study of `intrinsic F0' (IF0: \citealp{sonderegger2017segmental}): small perturbations in a vowel's pitch (F0) due to the identity of the vowel (V) or the preceding consonant (C). The dataset contains \Sexpr{n_turkish} measurements of F0 (\ttt{f0}, in semitones) from the first 50 msec of the vowel in CV-initial, utterance initial words (\Sexpr{n_turkish_word} types), from the GlobalPhone Turkish corpus, of read sentences from \Sexpr{n_turkish_speaker} speakers. 


Of primary interest is the effect of \ttt{Voicing} of the C---F0 may be higher after \tsc{voiceless} versus \tsc{voiced} consonants---and how robust this effect is across speakers. The C is one of \Sexpr{n_turkish_cons} consonants (column \ttt{consonant}).   %`f0` is thus the average pitch (in semitones) in the *first 50 msec* of the V.  
Of secondary interest is how V (\ttt{base\_vowel}: levels \tsc{a}, \tsc{i}, \tsc{u}) affects F0, which may be higher after high (\tsc{i}, \tsc{u}) versus low (\tsc{a}) vowels, and may modulate the effect of \ttt{Voicing}.
%\footnote{IF0 effects are generally found to be larger at higher F0, within-speaker, so we might expect any \ttt{Voicing} effect to be larger for high vowels, which have higher F0 than low vowels.}
We therefore code \ttt{base\_vowel} using Helmert contrasts \ttt{base\_vowel\_AvIU} (= vowel height) and \ttt{base\_vowel\_IvU}:

<<>>=
## Redefine contrast matrix for base_vowel
contrMat <- matrix(c(-2 / 3, 1 / 3, 1 / 3, 0, -1 / 2, 1 / 2), ncol = 2)
rownames(contrMat) <- c("A", "I", "U")
colnames(contrMat) <- c("AvIU", "IvU")
contrasts(turkish_if0$base_vowel) <- contrMat
@


IF0 effects are small perturbations from the F0 that is expected given the speaker (which we will capture by \ttt{gender}, and a by-speaker random intercept) and the intonational context, which we capture using two predictors: F0 averaged over the next three syllables in the utterance (\ttt{local\_f0}), and length of the utterance (\ttt{utterance\_num\_syllables}), given that longer utterances may start with higher pitch \cite[e.g.,][]{cooper1980fundamental}. 

%the speaker's mean pitch (\ttt{speaker\_mean\_f0}, calculated over all speech in the corpus); 

% FUTURE: JPK - semitone values in Fig 10.1 seem suspicious.. why not centered around 0? (I assume he means, values of 11-14 seem very high?)

Figure~\ref{fig:turkish-if0-emp} (left) shows the empirical effect of \ttt{Voicing} and \ttt{base\_vowel} on \ttt{f0}.
%(these are means and 95\% CIs over by-word averages).
It looks like \ttt{Voicing} may have the expected effect, 
%(voiceless $>$ voiced), 
possibly modulated by vowel height.
%(/i/, /u/ vs. /a/). 
There may also be substantial interspeaker variability in the \ttt{Voicing} effect (Figure~\ref{fig:turkish-if0-emp} right). But these effects are all very small  relative to other factors, such as \ttt{local\_f0} (Figure~\ref{fig:turkish-if0-emp} center), so a statistical model is necessary for accurate estimates.




<<turkish-if0-emp, echo=FALSE, fig.asp=1, out.width='30%', fig.width=default_fig.width*.30/default_out.width,fig.cap='Emprical effects on \\ttt{f0} for the \\ttt{turkish\\_if0} dataset.  Left: \\ttt{Voicing} and  \\ttt{base\\_vowel} interaction, calculcated over by-word means. Center: \\ttt{local\\_f0} effect, calculated over observations.  Right: by-speaker estimate of \\ttt{Voicing} effect (\\tsc{voiceless}-\\tsc{voiced}), calculated over means for each speaker-word pair.'>>=

# turkish_if0 %>% group_by(speaker, speaker_mean_f0) %>% summarize(f0=mean(f0)) %>% ggplot(aes(x=speaker_mean_f0, y=f0)) + geom_smooth() + geom_point() + xlab("Speaker average f0 in corpus (st)") + ylab("Vowel F0 (st)")

turkish_if0 %>%
  group_by(word, Voicing, base_vowel) %>%
  summarise(f0_m = mean(f0)) %>%
  ggplot(aes(x = Voicing, y = f0_m, shape = base_vowel)) +
  stat_summary(position = position_dodge(width = 0.2)) +
  stat_summary(aes(group = base_vowel), geom = "line", position = position_dodge(width = 0.2)) +
  ylab("Vowel F0 (st)") + 
  theme(legend.position='bottom')

turkish_if0 %>% ggplot(aes(x = local_f0, y = f0)) +
  geom_point(size = 0.25, alpha = 0.1, color='darkgrey') +
    geom_smooth(color=default_line_color) +
  xlab("F0 average in\nnext 3 syllables") +
  ylab("Vowel F0 (st)")

turkish_if0 %>%
  group_by(word, Voicing, speaker) %>%
  summarise(f0_m = mean(f0)) %>%
  group_by(Voicing, speaker) %>%
  summarise(f0_mm = mean(f0_m)) %>%
  group_by(speaker) %>%
  pivot_wider(names_from = Voicing, values_from = f0_mm) %>%
  mutate(diff = voiceless - voiced) %>%
  ggplot(aes(x = diff)) +
  geom_histogram() +
  geom_vline(aes(xintercept = 0), lty = 2) +
  xlab("By-speaker voicing\neffect estimate (st)")
@

We also use numeric and standardized versions of all predictors:

<<>>=
turkish_if0 <- turkish_if0 %>%
  mutate(
    utterance_num_syllables = rescale(utterance_num_syllables),
    local_f0 = rescale(local_f0),
    ## voiceless = pos, voiced = neg
    Voicing.vl = rescale(Voicing),
    ## male = pos, female = neg
    gender.male = rescale(gender)
  )
@


\section{More on random effects}
\label{sec:ranef-further-uses}

We have so far used random effects to account for the data coming from many `random' levels of 1 grouping factor or 2+ `crossed' grouping factors, such as speakers or words, resulting in non-independent clusters of observations. We want to generalize to the population, rather than these random samples of speakers or words (e.g., beyond the \Sexpr{n_turkish_speaker} Turkish speakers in this dataset). 

Random effects can also be used to model non-independence of observations which does not (conceptually) come from drawing random levels from a population, or additional kinds of grouping structure.  We use the \ttt{turkish\_if0} dataset to illustrate two cases (see Section~\ref{sec:other-reading-ch10} for others).


\subsection{Controlling for a nuisance factor}
\label{sec:controlling-nuisance}

A minimal model for this data, given the variables introduced above, could be:

<<output.lines=13:29>>=
if0_m00 <- lmer(f0 ~ Voicing.vl * base_vowel + gender.male + 
    local_f0 + (1 | word) + (1 | speaker), data = turkish_if0)
summary(if0_m00, correlation = FALSE)
@

Of primary interest is the \ttt{Voicing.vl} fixed effect and its interaction with \ttt{base\_vowel}.  Note that both are word-level predictors.  Including a by-word random intercept means this model asks, ``does a word's \ttt{Voicing} affect \ttt{f0}, after controlling for by-word variation?''

However, this may not be enough to control for variation among words,
%beyond C \ttt{Voicing}, 
as Figure~\ref{fig:if0-cons-emp} illustrates.  It looks like \ttt{consonant} affects \ttt{f0}, beyond just whether the consonant is \tsc{voiced} or \tsc{voiceless}.  Not somehow accounting for this in our model could result in the estimate of \ttt{Voicing} being overconfident, for example due to \tsc{z} having very low F0, or some consonants being much more common than others (as this is corpus data).\footnote{\tsc{z} is the phone \textipa{[Z]}.}  

We could do this using more fixed-effect terms coding consonant identity, like \ttt{Manner} or \ttt{Place} of articulation, but these effects are not of direct interest, and not all manner/place/voicing combinations exist in Turkish (or most languages). We could alternatively include \ttt{consonant} as a factor with many levels (14) and make one contrast correspond to \ttt{Voicing}. Either approach loses power to detect the effect of interest and makes the model hard to interpret, when all we want to do is control for consonant-level variation beyond \ttt{Voicing}. 
%and interpret. We could include a giant \ttt{consonant} factor (14 levels) and making one contrast correspond to \ttt{Voicing}. Either approach loses power to detect the effect of interest and makes the model hard to interpret, when all we want to do is control for consonant-level variation beyond \ttt{Voicing}. 


<<if0-cons-emp, echo=FALSE,  fig.asp=0.9, out.width='55%', fig.width=default_fig.width*.55/default_out.width, fig.cap='Empirical effect of preceding \\ttt{consonant} on \\ttt{f0}, for the \\ttt{turkish\\_if0} data. Means and 95\\% confidence intervals are over by-word averages.'>>=
turkish_if0 %>%
  group_by(word, Voicing, Voicing.vl, consonant) %>%
  summarise(f0_m = mean(f0)) %>%
  ggplot(aes(x = fct_reorder(consonant, Voicing.vl), y = f0_m)) +
  stat_summary(aes(shape = Voicing), size=0.75) +
  xlab("Preceding consonant") +
  ylab("F0 (semitones)") + 
  geom_jitter(size=0.1, width=0.2, height=0.1, alpha=0.25) + 
  theme(legend.position='bottom')
@

A more elegant solution for a `nuisance' factor with many levels is to code it as a grouping factor; here, by including a by-\ttt{consonant} random intercept in the model.\footnote{This kind of situation is common for corpus data, e.g., \citet[][]{sonderegger2017medium} use random intercepts to control for effects of phonological context on vowel formants.  Note that this use of random intercepts is conceptually different from (for example) a by-speaker random intercept----the data contains \textbf{all} values of \ttt{consonant}, not a random draw of consonants from Turkish---but both are valid, they just differ in conceptual interpretation \citep[][\S13.3]{bolker2015linear}.} This is OK as long as levels of the nuisance factor can be reasonably approximated as normally-distributed deviations from the grand mean, which is all that the model  assumes about random intercepts.
%A more elegant solution for a `nuisance' factor with many levels is to code it as a grouping factor: here, by including a by-\ttt{consonant} random intercept in the model. (Sonderegger et al BB paper -- another example)  
The \ttt{Voicing.vl} fixed effect now asks,
``what is the effect of \textbf{consonant}-level predictor \ttt{Voicing.vl}, controlling for consonant differences?'' 
%The estimate for this term will be more conservative (Type I error less likely) than the model using just by-word random effects.  


<<>>=
if0_m0 <- update(if0_m00, . ~ . + (1 | consonant))
@

% We refit with a better start value to get the model to converge (see below, Section~\ref{sec:changing-optimizer-parameters}):
% 
% <<>>=
% if0_m0 <- update(if0_m0, start=getME(if0_m0,"theta")) 
% @

We can verify, using a likelihood-ratio test of REML fits (following Section~\ref{sec:lmm-ht-ranef}), that adding this term improves the model:

<<output.lines=5:7>>=
anova(if0_m00, if0_m0, refit = FALSE)
@

The new model's output is:

<<output.lines=13:31>>=
summary(if0_m0, correlation = FALSE)
@

Note the \ttt{Voicing.vl} fixed effect in particular: the SE is higher than in \ttt{if0\_m00}, as expected, resulting in a more conservative estimate (lower $t$-value).

This example gets at a more general point, on avoiding `pseudoreplication' in grouped data: \textbf{if there is non-independence in the data as a function of variable $x$, this should somehow be accounted for in the model}, whether by coding $x$ as a fixed effect or random intercept (Boxes~\ref{box:shrinkage-i}, \ref{box:fixed-vs-random}).  Otherwise we run the risk of overconfident estimates (including Type I errors), especially for $x$-level predictors. Our first model of the \ttt{turkish\_if0} data did not account for non-independence of data from the same \ttt{consonant} (= $x$), so the \ttt{Voicing.vl} coefficient estimate was overconfident ($t$-value too high). As another example, a model containing a fixed effect of speaker \ttt{gender} requires a by-speaker random intercept \citep{judd2017experiments,baayen2008mixed}.
%- Regardless of whether X is coded as a fixed effect or random intercept, any (important) source of non-indpeendence must be somehow accounted for in the model. Otherwise, pseudoreplication, esp. for X-level predictors. For example: our first model of the IF0 data did not account for non-independence of data from the same *consonant*; this risks overconfident estimate for Voicing, a consonant-level predictor. A fixed effect of speaker gender *requires* a by-speaker random intercept 

\subsection{Nested random effects}

This example also shows a \emph{nested} random effect, reflecting hierarchical grouping structure in the data.  Each observation comes from one  word (and each word can contain several observations), and each word begins with one consonant.  The grouping factor \ttt{word} is `nested' within grouping factor \ttt{consonant}: F0 is modeled as varying by-consonant, and then by-word {within} consonant. 

The estimated $\hat{\sigma}$ (variance components) for these two kinds of (normally-distributed) variation are \Sexpr{extractRanefEst(if0_m0, 'consonant', 'sd__(Intercept)')} (by-consonant) and \Sexpr{extractRanefEst(if0_m0, 'word', 'sd__(Intercept)')} (by-word).  Note that the by-word random intercept estimate $\hat{\sigma}$ is lower than for the previous model (\Sexpr{extractRanefEst(if0_m00, 'word', 'sd__(Intercept)')}), because some by-word variation is now estimated as by-consonant variation.

Such hierarchical structure is common in linguistic data, for example from corpus linguistics (e.g.,\ text nested within register: \citealp{th2015most}) or dialectology (e.g.,\ speaker nested within dialect: \citealp{tanner2020toward}), though explicitly modeling it with nested random effects remains uncommon.\footnote{More than two levels of nesting are possible.  In \citep{th2015most}, texts are actually nested within subregisters,  nested within registers,  nested within modes.}

These three ways of writing nested random effects in an {lme4} formula are equivalent:

\begin{enumerate}
\item \ttt{(1|x) + (1|y)} (as in \ttt{if0\_m0})
\item \ttt{(1|x:y) + (1|y)}
\item \ttt{(1|y/x)}
\end{enumerate}
(For our example, \ttt{x} is \ttt{word} and \ttt{y} is \ttt{consonant}.)
(2) makes the model structure explicit: \ttt{word} modulates the effect of \ttt{consonant}. (1) is equivalent, as long
as \ttt{x} and \ttt{y} are coded as factors.
%\footnote{As opposed to numeric or character vectors.} 
(3) is the notation for `nesting', and just expands to (2).  We use (1) in this chapter for simplicity.

%- Nested random effects notation:  Equivalent \ttt{(1|consonant/word)}. Also OK to just do \ttt{(1|consonant) + (1|word)}; this just doesn't make nesting structure explicit.  We do this for simplicity, for models of this data in this chapter:
% 
% <<>>=
% if0_m0 <- update(if0_m0, . ~ . - (1|word:consonant) + (1|word))
% @
% 
% So the random effects are now:
% 
% <<>>=
% summary(if0_m0)$varcor
% @
% 

\begin{boxedtext}{Broader context: Coding as a fixed versus random effect}
\label{box:fixed-vs-random}

% The above discussion gets at an important point that has not been emphasized so far: the importance of accounting for \underline{all} major sources of non-independence.

Above we mentioned that grouping structure indexed by a variable $x$ (e.g.,\ \ttt{consonant}) can, in principle, be accounted for using a fixed effect of $x$ or a random intercept. These options are called `no pooling' and `partial pooling' by \citet[][\S11.4]{gelman2007data}:
\begin{itemize}
\item `No pooling': \ttt{f0 $\sim$ Voicing.vl*base\_vowel+ gender.male + local\_f0 + consonant + (1 | 
    word) + (1 | speaker)}
    \item `Partial pooling': replace \ttt{consonant} by \ttt{(1|consonant)}.
\end{itemize}

There are conceptual and practical reasons to choose one option or the other,
%(\citealp[][13.3]{bolker2015linear}, \citealp[][11.4]{gelman2007data}), 
including whether one wants the model to generalize beyond observed levels of $x$, whether $x$ has many levels, and whether $x$ is highly imbalanced across levels---all of which favor `partial pooling'.  A natural question is, {how many levels does a factor need to have to be included as a random effect}?  Guidelines vary, but suggest a minimum of 5--10 levels, to avoid inflated Type I and Type II errors (lower power) and unstable estimates. This issue matters more if $x$ is related to the research questions. See \citet[][\S13.3]{bolker2015linear}, \citet[][\S11.4]{gelman2007data}, \citet{brauer2018linear}.
%for further discussion and references.
% 
% 
% - A natural question is *how many levels a factor needs to have to be included as a random rather than fixed effect*.  Guidelines suggest 5--10, before getting into fitting problems and higher Type I/II errors (see your notes), but in general it's impossible to suggest clear guidelines (Brauer/Curtin).  Whether the term is critical or control seems more important.

For example, for the \ttt{neutralization} data, three item-level predictors are relevant: the effect of the consonant's \ttt{voicing} (2 levels) is of primary interest; and we want to control for \ttt{vowel} (5 levels) and for consonant \ttt{place} (3 levels). Our models so far do this using fixed effects of \ttt{voicing}, \ttt{vowel}, and \ttt{place}, and a random intercept for \ttt{item\_pair}, so there are 6 `nuisance' rows of the fixed-effect table.  We could alternatively account for idiosyncrasies of the item's final VC (vowel-consonant) by coding \ttt{vowel:place} as a new variable \ttt{VC} (15 levels), and  coding \ttt{VC} as a random intercept, within which \ttt{item\_pair} is nested (Exercise~\ref{ex:neutralization-vc}). This gives a model with cleaner output, with near-identical results for all terms of interest (terms involving \ttt{voicing}).
%% also, better conceptual interpretation: generalizes to all german VCs, not just ``random items from this population''
% neutralization$VC <- interaction(neutralization$place, neutralization$vowel)
% update(neut_mod3, . ~ . - place - vowel + (1|VC)) -> k


\end{boxedtext}


% - Regardless of whether X is coded as a fixed effect or random intercept, any (important) source of non-indpeendence must be somehow accounted for in the model. Otherwise, pseudoreplication, esp. for X-level predictors. For example: our first model of the IF0 data did not account for non-independence of data from the same *consonant*; this risks overconfident estimate for Voicing, a consonant-level predictor. A fixed effect of speaker gender *requires* a by-speaker random intercept \citep{judd2017experiments,baayen2008mixed}.




<<echo=FALSE, cache=FALSE>>=
opts_chunk$set(warning = TRUE, message = TRUE, cache = TRUE)
@

\section{Model convergence}
\label{sec:lmm-convergence}

Two kinds of fitting problems are common for {lme4} models: convergence issues and singular models.  We first cover sources and fixes for each (Section~\ref{sec:lmm-convergence}, \ref{sec:singular-ch10}), then turn to model selection (Section~\ref{sec:lmm-model-selection}), which in practice means selecting among convergent and (ideally) non-singular models.   In these sections we show model fitting messages/warnings, which we have typically suppressed in previous chapters.  Our examples use the `final' models for several datasets from previous chapters---\ttt{neut\_mod3},  \ttt{vot\_mod3}, \ttt{diatones\_melr}, \ttt{givenness\_m1}---which you should review and refit if you're reading this chapter in isolation (Section~\ref{sec:lmm-multiple-random-slopes}, \ref{sec:ranef-corrs}, \ref{sec:diatones-melr}, \ref{sec:givenness-m1}).


% - Common for models to have problems fitting: converging (this section), or singular (next section).  The general issues here are common to most complex statistical models: finding optimal parameter values, and avoiding parameter values which are on the boundaries. 
% 
% - Crucial for model selection, which in practice means selecting among convergent and (hopefully) non-singular models.
% 
% - For each, define, sources, fixes.

Throughout our discussion, we will assume a distinction between \emph{critical predictors}---those whose effects are directly related to the research questions, or associated with them (i.e.,\ interactions, random slopes)---and \emph{control predictors}. For example, in our model \ttt{givenness\_m1} of the \ttt{givenness} data  (Section~\ref{sec:givenness-m1}),
% (REF), 
since the research question involves condition label, \ttt{clabel.williams} and its interaction with \ttt{voice.passive} are critical; other predictors are controls. This means that the random slopes for these two predictors, as well as all correlation terms with these random slopes, are `related to critical predictors', while other random-effect terms are not.

We cover these issues in some depth both because they come up frequently in practice using {lme4}, and because the underlying issues are common to fitting any complex statistical model: solving a complex optimization problem using off-the-shelf algorithms, defining optimal and valid parameter values, and the consequences of a too-simple or too-complex model given the data.

\paragraph{Practical note}

Giving worked examples for these topics (fitting issues, model selection)
%Giving worked examples for these topics, especially fitting issues and model selection,
requires refitting many models and examining code output. These steps are not hard, but take a lot of space. We have moved much of this to an online Appendix to streamline our presentation. You should be able to work through this chapter just executing code in the text, as usual, and refer to the  Appendix as needed to see full worked examples.



%important to recognize that convergence/singularity issues often reflect more general facts about the data, which are not particular to {lme4}.


%- Throughout discussion, will assume a distinction between `critical' predictors---those which are directly related to RQs, or associated (interactions, random slopes)---and `control' predictors.  For example, in givenness\_m1 : clabel and interaction with voice are critical; others are control. This means that the random slopes for clabel and ixn, as well as all correlation terms involving these, and random intercepts, are `related to critical predictors'; other random effect terms are not.  




\subsection{Sources of non-convergence and fixes}
\label{sec:sources-non-convergence}
% (Text that was in a box in Ch 8:)

To understand fixes for non-convergent models, 
%convergence
%Because error messages for non-convergentconvergence issues are common when fitting {lme4} models, and error messages tend to be opaque,
it is useful to know a bit about optimization in 
%As `convergence issues' are the most common problem with fitting {lme4} models in practice, and error messages are opaque, it is very useful to know a bit about optimization in 
{lme4}  (see \citealp{bates2015fitting,bolker2015linear} for more).

Optimizing an {lme4} model means estimating the fixed and random-effect parameters of the model $\hat{M}$ that maximizes a likelihood function (ML or REML), by iterating from a start model ($M_0$) to fit a model progressively closer to the $\hat{M}$, until convergence is reached---defined by some criterion. Fitting the random effects (= variance components) is particularly tricky because they are restricted to be positive, and a large number of latent variables are involved (the random intercept for each participant, and so on).  

This is a complex but standard optimization problem, and {lme4} supports several powerful general-purpose nonlinear optimizers, including Nelder-Mead and BOBYQA, each of which has a range of parameters, including:
\begin{itemize}
\item The \emph{starting values} of model parameters ($M_0$)
\item \emph{Tolerances} specifying what a small enough change in the model (likelihood or parameter values) is to stop
\item \emph{Maximum iterations} to try before giving up
\end{itemize}
See \ttt{?lmerControl} for more.

Most of these parameters relate to convergence, while others relate to 
model validity (parameters of the fitted model taking on valid values), which in this case mostly means ensuring the model is not singular (see Section~\ref{sec:singular-ch10}).
%model validity---which mostly means whether the random effects contain a zero `dimension' (see below, XX), a.k.a `singular'.
%---how to decide when the model is sufficiently close to $\hat{M}$ and when to stop looking---(`convergence'), and when to stop looking.Others relate to how it is decided if the model is `valid'---in this case, this mostly means that the variance components are greater than zero.  
Both convergence and validity are non-trivial issues at play when one fits any sufficiently complex statistical model, because the likelihood function is complex and different datasets vary hugely.  In addition there is a trade-off between accuracy and speed: smaller tolerances or more iterations lead to better estimates of $\hat{M}$, at the expense of longer running time.

All this means that no one choice of optimizer and parameter values will work for all cases; they will sometimes need to be adjusted.
%for the case at hand. 
%In practice, users typically will use the default settings, so 
The {lme4} maintainers have put great care into choosing default settings which work in common cases, and balance accuracy and speed.

Another common reason for non-convergence or singular fits is that the model is too complex for the data, or \emph{overparametrized}. Intuitively, this makes it hard to find a {unique} optimal solution. We discuss further for singular fits, where overparametrization is a more common cause.  It is a common belief among users that non-convergence is a sign the model is overparametrized, especially in the random effects, but this is incorrect \citep[][32]{seedorff2019maybe}: models can fail to converge for other reasons (Box~\ref{box:convergence-model-quality}).
%and `false-positive' convergence warnings are not uncommon.   Per \ttt{?convergence}, ``[lme4] fits may produce convergence warnings; these do not necessarily mean the fit is incorrect''---or that there is any issue with your data or model.
%including that the model is too complex for the data. 
%They just mean that  further steps are needed to figure out if this is the case, and to determine how to resolve the warnings.
%is that the model is too complex for the data; intuitively this makes it hard to find a *unique* optimal solution.  Discussed more below for singular models, where this is a more common cause.

The upshot is that you should only report convergent models, but non-convergence is not a good basis for ruling a model out.
%(Box~\ref{box:convergence-model-quality}).


\begin{boxedtext}{Broader context: Convergence is not a model quality metric}
\label{box:convergence-model-quality}

It is worth emphasizing that \textbf{whether a model converges is not a good measure of model quality}, 
%This is worth emphasizing
even though 
model convergence has come to be seen as meaningful in practice in language sciences.  Papers (including my own work) routinely use model convergence to decide which terms to add to a model, which model to report, or to justify using alternative methods.  But whether a model converges with the {lme4} default settings is not in itself a good tool for these things, because non-convergence can result from many sources, and `false positive' convergence warnings are common.  Per \ttt{?convergence}, ``[lme4] fits may produce convergence warnings; these do not necessarily mean the fit is incorrect''---or that there is any issue with your data or model.
%(FUTURE: REF for this---Eager/Roy paper? BB somewhere?)
%and `false-positive' convergence warnings are not uncommon.   Per \ttt{?convergence}, ``[lme4] fits may produce convergence warnings; these do not necessarily mean the fit is incorrect''
%including that the model is too complex for the data. 
They just mean that  further steps are needed to figure out if this is the case, and to determine how to resolve the warnings.

For example, the default \ttt{lmer()} optimizer changed in 2/2019 from one version of the BOBYQA algorithm (\ttt{bobyqa}, used since 2013; from the {minqa} package) to \ttt{nloptwrap} (which uses a version from the {nloptr} package).   This led to faster model fits, but anecdotal reports (e.g.,\ {lme4} GitHub issue \#501) suggest more false-positive convergence warnings,  which may go away when the previous default optimizer is used---as in our example in Section~\ref{sec:changing-optimizer}. This kind of (very) technical change doesn't affect what the optimal model is, just how it is fitted. As a simpler example, it is often necessary for larger datasets or more complex models to increase the maximum number of iterations (Section~\ref{sec:changing-optimizer-parameters}); non-convergence is meaningless when the optimizer simply needs to run longer.

Note that convergence/singularity issues often reflect more general facts about the data, which are not particular to {lme4}. For example, \citet{bates2015parsimonious} show a case where the same fundamental issue, of the model being too complex for the data, applies regardless of whether an {lme4} or Bayesian model is used. 
%% FUTURE: consider for re-inclusion? just sounds whiny.
% For example, a common motivation for moving to Bayesian mixed-effects models cited in the literature is to avoid convergence issues that come up in selecting random-effect structure (REFS).  But the issues underlying non-convergence---model complexity, validity, and run time---do not go away using a Bayesian model, they just shift to different aspects of fitting and evaluating the model (choosing sensible priors, evaluating posterior distributions, choosing the number of iterations for each MCMC chain, etc.).  \citep{bates2015parsimonious}  show an example of how the same fundamental issue, of the model being too complex for the data, applies regardless of whether an lme4 or Bayesian model is used.

%% FUTURE: revise/reconsider all this... necessary?  Maybe should shift the Bayesian discussion somewhere into main text, briefer. Say, in summary, there are often practical issues to deal with when building an LME4 model, especially random effect structure.   We have shown solutoins while sticking with LME4 models.  Another option, which we do not discuss, is to move to a bayesian modeling framework, where these issues can be avoided---for example prior on ranef structure---at the cost of other complexites with fitting/interpreting models in this framework (e.g., Eager/Roy, Vasishth et al). See our Section XX for discussion.



\end{boxedtext}

% -- ``False-positive'' convergence warnings fairly common; often because of fixable issue with data/model/algorithm, or (harder)  because model is too complex for the data.


%\subsection{Fixes}

Two kinds of steps can be taken to resolve non-convergence or singular fits:
\begin{itemize}
\item \emph{Non-intrusive}: data/model checking, transformations, optimizer
\item \emph{Intrusive}: changing the model, typically by simplifying random-effect structure, or running several models
\end{itemize}
The same fixes often address other fitting issues: very slow fits or `false negative' models (convergent, but with major problems).

% 
% - To resolve convergence problems, two kinds of steps:
% 
% (1) `nonintrusive': data/model checking, transformations, optimizer
% (2) `intrusive': changing model structure, often by simplification of random effects, or running several models

We will discuss some fixes of each type, shown in Table~\ref{tab:conv-steps}, 
%under (1) and (2), shown in 
with examples for those we have found come up most in practice.
%\footnote{Note that this table does not include `collect more data' or `collect different data', which are often the best solution when possible (see \citealp{brauer2018linear}, Table 17).  In this book we assume data collection is finished.}
Our discussion draws a lot on \ttt{?convergence}, \citet{seedorff2019maybe}, and \citet{brauer2018linear}. We highly recommend reading more if you regularly encounter non-convergent or singular models (Box~\ref{box:convergence-more}).
%whose full discussion is recommended and on Brauer \& Curtin, whose full discussion is highly recommended, especially Table 17.


\begin{table}

\begin{enumerate}
\item Non-intrusive:
\begin{enumerate}
\item Check your data and model
\item Standardize predictors (center, possibly scale)
\item Increase number of iterations
\item Change the optimizer
\item Give the optimizer better start values
\end{enumerate}
\item Intrusive:
\begin{enumerate}
\item Remove random effects involving control predictors (must not be in interactions with critical predictors)
\item Selectively remove random-effect correlations: for control predictors, then correlations that are probably close to 0
\item Remove random intercept (leaving slope terms in)
\item Remove random slopes for critical predictors
%\item  Remove correlations in random effects: for control $>$ critical predictors
%\item Remove random intercepts
\end{enumerate}
\end{enumerate}

\caption{Possible fixes for non-convergent (non-intrusive + intrusive) and singular models (intrusive only), ordered by which to try first (adapted from \citealp{brauer2018linear}).  2(a) and 2(b) are tied.}
\label{tab:conv-steps}
\end{table}



\begin{boxedtext}{Broader context: Addressing non-convergence}
\label{box:convergence-more}

Given that dealing with convergent and singular models (along with model selection) is the most common problem users have in fitting mixed-effects models, there is surprisingly little truly practical guidance, showing worked examples and discussing which solutions to try before others. 
%---like Seedorf, Brauer \& Curtin. 
One reason is how difficult it is to give general advice; the steps to take can depend on the particular case and type of data.  Nonetheless, guidance is urgently needed \citep{seedorff2019maybe}, and this is an active research area. All work so far in the language sciences/psychology literature (see Section~\ref{sec:other-reading-ch10}) focuses on data from balanced experiments, with a low number of fixed and random-effect terms.  This makes sense (simple before complex cases), but much linguistic data does not fit this description. One goal of this chapter is to give practical advice on convergence/singularity and model selection for corpus data, with which I am most familiar.

\citet{brauer2018linear} give the most comprehensive discussion to date of practical steps to resolve convergence issues.  Their Table 17 lists 20 remedies, in rank order of which to try first and which are least likely to affect parameter estimates. All `non-intrusive' remedies we list come before `intrusive' remedies. 
%- Most comprehensive discsusion at this point of practical steps to resolve converence issues: Brauer \& Curtin. Their Table 17 lists *20* remedies in rank order, of try-first/less likely to affect parameter estimates; all (1) remedies before (2).  
Their exact ordering is not universally agreed-upon, but it forms a great starting point. 
%and in general I have followed their order. 

The list of convergence fixes we cover is partial: \ttt{?convergence} 
%and %Bolker (\url{rpubs.com/bbolker/lme4trouble1}, \url{rpubs.com/bbolker/lme4\_convergence})
discusses more `non-intrusive' steps; for `intrusive', we do not discuss solutions besides simplifying random-effect structure.  More drastic analysis changes which can be appropriate, such as fitting subset models or aggregating data, are discussed by \citet{brauer2018linear,seedorff2019maybe,barr2013random}. Most importantly, we do not discuss collecting more or different data, which are often the best option when possible.
%- The list of convergence fixes covered in the text is partial: for `non-intrusive' the convergenec help page (and Bolker REF) discusses more optimizer tricks; for `intrusive' we have only touched on simplifying random effect structure. More drastic analysis changes can be appropriate, such as fitting subset models or aggregating data, and are discussed by  BC, Barr et al., Seedorf.

\end{boxedtext}





%including by trying different optimizers and tolerances.  The \ttt{convergence} page offers practical steps to try, including using a differernt optimizer or decreasing tolerances; some are used in this chapter.



%-- Intro below, covering: optimizer, number of iterations, start parameters.

%-- Model non-convergence: not necessarily meaningful, meaning fit incorrect.


\subsection{Non-intrusive methods}

%- Use variants of three models we've discussed a lot: diatones, givenness, vot.

\subsubsection{Checking the data}

A model may not converge if the \textbf{structure of your data} makes the model impossible to fit: the distribution of a predictor or the response may be highly skewed, some combinations of predictor values may have few or no observations, or some participants/items may have little data in a cell. 
%Mixed-effects models are generally robust to these kinds of issues, but in extreme cases (e.g.,\ one predictor completely predictable from others, separable data) a unique solution does not exist, so the model doesn't converge for the same reasons as non-mixed-effects models (REF).

% - A model may not converge because the **structure of your data** may make model impossible to fit (BC 3).   predictor or response highly skewed; small number of observations for some pred values; or some subject/items have little data in a cell (BC 10).

For example, recall from Section~\ref{sec:sep-quasi-sep} that the \ttt{diatones} data has second-syllable coda originally coded as a factor (4 values: \ttt{syll2\_coda\_orig}), rather than the numeric version (\ttt{syll2\_coda}) which we have mostly used.  Fitting a regular logistic regression predicting \ttt{stress\_shifted} using \ttt{syll2\_coda\_orig} did not work because the data is separable: there are very few observations for \ttt{syll2\_coda\_orig} = \tsc{ccc}, all with \ttt{stress\_shifted} = 0. 
%with very few observations for  very little data for CCC.  
If we try to refit our final mixed-effects regression model for this data (\ttt{diatones\_melr}: Section~\ref{sec:diatones-melr}), using this predictor, the model doesn't converge:

% replays: diatones_melr_bad <- update(diatones_melr, . ~ . - syll2_coda + syll2_coda_orig)

<<echo=FALSE>>=
replay(diatones_melr_bad_eval)
@

We can see the problem in lines 1 and 7 of the fixed-effects table:

<<warning=FALSE>>=
tidy(diatones_melr_bad) %>% filter(effect == "fixed") %>%
  select(-effect, -group)
@

The issue is the same as for the non-mixed model: very high standard errors, because the effect of \ttt{syll2\_coda\_orig} = \tsc{ccc} can't be determined.
%because separabili to separability, because the can't determine the effect of  \ttt{syll2\_coda\_orig} = \tsc{ccc}.

More generally, convergence issues can result from {extreme} cases of most `problems' we have discussed for model validation (non-normal residuals, multicollinearity, etc.), and the solutions to address them will be similar. For example:
%the solutions to address them will depend on the case, for example:
\begin{itemize}
\item Combining some predictor values, or coding the predictor differently (as in this case: code as numeric \ttt{syll2\_coda}) 
\item Discarding or imputing values for a subset of the data (e.g., a single participant with unrealistic values of $y$, or extreme `outlier' observations)
\item Transforming a predictor or the response to meet distributional assumptions
\end{itemize}

In practice, mixed-effects models are much more robust to such issues than non-mixed models---for example, outlier participant/items will just be modeled by the random effects---and convergence issues reflect truly dire problems (i.e., complete separation).  

\subsubsection{Checking the model}

The structure of the model needs to make sense, given the data. The most common issues are:
\begin{itemize}
\item Including a random slope for a factor with uncorrelated random effects (shown in Section~\ref{sec:random-slopes-factors})
\item Including a by-unit random slope for a predictor that varies by unit (an `impossible' random effect: Section~\ref{sec:possible-random-slopes})
\item Including a predictor both as a grouping factor and a fixed effect
\end{itemize}

For example, consider  \ttt{vot\_mod3}, the final model of a subset of the VOT data  (Section~\ref{sec:ranef-corrs}), whose formula was:
<<>>=
formula(vot_mod3)
@
%% NOTE: vot_mod3 was refitted in the Online Appendix, and thus reloaded
%% in convergence_appendix_mods.RData .



Suppose that \ttt{place} were a critical predictor. We would then want to add all possible random slopes for \ttt{place} to the model. We might think that this means adding by-speaker and by-word random slopes: 
%of \ttt{place}, for this model:

% replays: vot_mod3_bad <- lmer(log_vot ~ speaking_rate_dev + foll_high_vowel + cons_cluster +  log_corpus_freq + place + gender + (1+speaking_rate_dev+place|speaker) + (1+place|word), data=vot_voiced_core)

<<echo=FALSE>>=
replay(vot_mod3_bad_eval)
@

The model does not converge because \ttt{place} is a word-level predictor, so a by-word random slope of \ttt{place} does not make sense.  When refitted without this term (after another fix), the model converges (Appendix Section 2.2).

Interestingly, there is nothing in the actual model output that suggests this is a nonsensical model (output shown in Appendix Section 2.2):

<<eval=FALSE>>=
summary(vot_mod3_bad)
@

This is because the model does not actually know that \ttt{place} is word-level:  from the random-effect structure, it assumes that \ttt{place} varies within word, but for each word you happen to have only observed one value of \ttt{place}.  This is technically a valid model, but it does not make sense given the structure of the data. 

In practice, this kind of misspecified random-effect structure, or the other two mentioned above, often do \textbf{not} lead to convergence problems (or singular models), and it is up to you to notice the nonsensical model structure.
%model misspecifications---like including the same variable as both a fixed effect and grouping factor), often do *not* lead to convergence problems, and it is up to you to notice the nonsensical model structure. 


% 
% \ttt{vot\_}
% - Also, structure of model: needs to make sense given the data
% -- example: by-unit random slope for predictor that varies within units
% (This one often won't lead to non-convergence, just terms with weird interpretations)


% 
% -- doesn't converge - Similar issue to sec:sep-quasi-sep  - ultimately this is due to very little data for CCC; model can't determine its effect vs. everythign else. 
% 
% - one solution, as in this case: discarding or imputing values.
% 
% - Other times:  combining variables,  or transforming predictors/response.


\subsubsection{Changing optimizer parameters}
\label{sec:changing-optimizer-parameters}
% **changing start values** (BC 8)


% - Details: Appendix Section 2

Recall that \ttt{vot\_mod3} was fitted for just a subset of voiced stops (\ttt{voicing} = \tsc{voiced}): only observations from certain speakers, and for monosyllabic words.  Let's refit the model for all voiced stops, without these restrictions:
% 
% - Consider the VOT model \ttt{vot\_mod3} again, of voiced stops (\ttt{voicing} = \tsc{voiced}).  Recall this was fitted for just a subset of voiced stop data: only some speakers, and words one syllable long. 
% 
% - Let's refit this model for all speakers, and all Now refit for *all* speakers, and without restricting to monosylalabic words:

<<>>=
## All observations from voiced stops
vot_voiced <- filter(vot, voicing == "voiced")
@
We have also added a \ttt{stress} fixed effect, which is relevant now that words can have $>$1 syllable.


%words:  for *all* speakers, all (voiced) words; now add \ttt{stress} predictor.  We just didn't do this originally to avoid convergence issue.  
 
 
 
% replays: vot_mod3_full <- update(vot_mod3, . ~ . + stress, data=filter(vot, voicing=='voiced'))
<<echo=FALSE>>=
replay(vot_mod3_full_eval)
@


\paragraph{Changing tolerances}

This model does not converge. \ttt{?convergence} suggests the first fix to try is decreasing the optimizer's stopping tolerances;  this will always lead to more accurate but slower optimization.
%\ttt{?convergence} suggests we should try:

<<eval=FALSE>>=
strict_tol <- lmerControl(optCtrl = list(xtol_abs = 1e-8, ftol_abs = 1e-8))
## Refit the model with these tolerances
vot_mod3_full_3 <- update(vot_mod3_full, control = strict_tol)
@
The model now converges.

\paragraph{Changing start values}
 
Another option is to simply restart the fit using as start values the estimates from the non-convergent model, or slightly perturbed versions (see \ttt{?convergence}):\footnote{Note that for a GLMM the syntax is different: \ttt{getME(mod, c("theta","fixef"))}.}

%- However, can just restart the fit from the original value, or a slightly perturbed value (see \ttt{?convergence}):

%% fitted in appendix
<<eval=FALSE>>=
## Refit starting from the final values
fittedVals <- getME(vot_mod3_full, "theta")
vot_mod3_full_1 <- update(vot_mod3_full, start = fittedVals)

## Refit starting from the final values + perturbaton
pars <- getME(vot_mod3_full, "theta")
fittedVals_perturbed <- runif(length(pars), 
  pars / 1.01, pars * 1.01) * sign(pars)
# sign(pars) makes this work when some fixed effects are negative
vot_mod3_full_2 <- update(vot_mod3_full, start = fittedVals_perturbed)
@

To be safe, we can verify that the models fitted with different start values are identical, 
%The models are identical, 
for example in their log-likelihoods and fixed effects (output shown in Appendix Section 3.1):
<<eval=FALSE>>=
logLik(vot_mod3_full) # start values = defaults
logLik(vot_mod3_full_1) # start values from vot_mod3_full
compareCoefs(vot_mod3_full_1, vot_mod3_full, zvals = TRUE)
@

\paragraph{Increasing number of iterations}

Another fix, which will always lead to more accurate but slower optimization, is to increase the maximum number of iterations.

For example, consider model \ttt{givenness\_m1} of the \ttt{givenness} data, which included all possible random slopes, uncorrelated. Refitting this model with `maximal' random-effect structure (correlations included):
%(all possible slopes, correlations):
% %% replay: giv_maximal <- glmer(stressshift ~ clabel.williams*voice.passive + npType.pronoun + 
%                               (1+clabel.williams+ npType.pronoun|item) +
%                               (1 +clabel.williams*voice.passive + npType.pronoun |participant),
%                             data=givenness, 
%                             family="binomial", 
%                             control=glmerControl(optimizer = "bobyqa"))
<<echo=FALSE>>=
replay(giv_maximal_bad_eval)
@

This model does not converge with the default number of maximum iterations (10000), but if we increase this parameter (to 100000), as suggested by the warning message, the model does converge:

%% fitted in appendix
<<eval=FALSE>>=
giv_maximal <- update(giv_maximal, 
  control = glmerControl(optimizer = "bobyqa", 
    optCtrl = list(maxfun = 100000)))
@

<<>>=
## Iterations to convergence
giv_maximal@optinfo$feval
@

It is common to need to increase the maximum number of observations, especially for larger datasets or models with complex random-effect structure.

%- Compare running time: 30 sec. min without convergence vs. 3.5 min.  (That is a huge difference once models take minutes-hours.)


\subsubsection{Standardizing predictors}
\label{sec:convergence-std}

It is useful to distinguish here between centering and scaling, the two aspects of standardizing predictors.

\textbf{Centering} predictors has the same effect, of reducing unnecessary collinearity and making coefficients more interpretable (for fixed effects), as for non-mixed-effects models---this includes using a centered coding scheme for any factors. Centering is more important in mixed-effects models because it also reduces collinearity in the random effects, which are more often a source of non-convergence than collinearity in the fixed effects.

It is also helpful to normalize predictors (dividing by 1 or 2 SDs). Mixed-effects models fit slower, and in extreme cases will not converge, when predictors are on very different scales.

Given these benefits, it makes sense to \textbf{always standardize predictors} in mixed-effect models by default, minimally by centering.
%- togehter with the other benefits of standardization for interpreting model, makes sense to just standardize predictors (fixed effects), or minimally center.

%- **standardize predictors** : BC 5, 9 

% -- centering especially important.  That means including factors. same logic as for non-mixed-effects models, incl. reduces unnecessary collinearity; now in *random effects* as well.

This matters especially for fitting models with complex random-effect structure.  For example, consider refitting the new VOT model from above (\ttt{vot\_mod3\_full}),  now  with maximal random-effect structure:

% fitted in appendix
<<eval=FALSE>>=
vot_mod_maximal <- update(vot_mod3_full, . ~ . - (1 | word) - 
    (1 + speaking_rate_dev | speaker) + 
    (1 + gender + speaking_rate_dev | word) +
    (1 + speaking_rate_dev + foll_high_vowel + cons_cluster + 
        log_corpus_freq + place + stress | speaker))
@

This model, where all predictors are standardized, does converge.


<<>>=
## speaking_rate_dev row of fixed effect table:
tidy(vot_mod_maximal) %>%
  filter(term == "speaking_rate_dev") %>%
  select(-group)
@

In contrast, the same model fitted using the non-standardized predictors does not converge (Appendix: Section 4).
%and takes about twice as long to run (Appendix: Section 4): 6 vs.\ 3 minutes on my laptop.
%(6 vs.\ 3 minutes, on my laptop).  
%This kind of time difference really matters once you are dealing with models that take a long time to fit.
%---you don't want to wait hours for an invalid model!

% 
% (Here I have changed the default optimizer and increased the maximum iterations, which turn out to be necessary for convergence.)   This model uses standardized predictors.
% 
% - Let's now refit the same model, but using unstandardized predictors (in this case, each factor is treatment-coded as 0/1, which is non-centered):
% 
% <<message=TRUE>>=
% contrasts(givenness$conditionLabel) <- contr.treatment(2)
% contrasts(givenness$voice) <- contr.treatment(2)
% contrasts(givenness$npType) <- contr.treatment(2)
% 
% system.time(giv_maximalimal_2 <- glmer(stressshift ~ conditionLabel*voice+ npType+ 
%                (1+conditionLabel+ npType |item) +
%         (1+conditionLabel*voice+ npType|participant),
%                           data=givenness, 
%                           family="binomial", control=giv_control))
% giv_maximalimal_2@optinfo$feval ## iterations to convergence
% @
% 
% Both models eventually converge, but the version with standardized predictors takes about twice as long to fit (measured in time, or iterations).   



% 
% - By contrast, convergences in  maxits if use standardized predictors:
% 
% <<message=TRUE>>=
% system.time(giv_maximalimal_2 <- glmer(stressshift ~ clabel.williams*voice.passive + npType.pronoun + 
%                               (1+clabel.williams+ npType.pronoun|item) +
%                               (1 +clabel.williams*voice.passive + npType.pronoun |participant),
%                             data=givenness, 
%                             family="binomial", 
%                             control=control=giv_control))
% @
% 
% - Now converges. 


\subsubsection{Changing the optimizer}
\label{sec:changing-optimizer}

Sometimes a model converges when the optimizer is changed. We saw this for MELR models, such as  \ttt{givenness\_m1}, where using the  \ttt{bobyqa} optimizer often helps (Section~\ref{sec:givenness-m1}). 

The assumption is that both models are very similar/identical, and the non-convergent model was a `false positive'.  If you want to be sure this is the case, the gold standard  (see \ttt{?convergence}) is to refit the model with all possible optimizers, using \ttt{allFit()} from {lme4} (Appendix Section 5):

%% NOTE: givenness_m1 was refitted in the Online Appendix, and thus reloaded
%% in convergence_appendix_mods.RData .
<<eval=FALSE>>=
giv_m1.all <- allFit(givenness_m1)
@

If the fits are very similar, it is safe to use the new fit---the non-convergent fit was a false positive. It is also usually OK to use the new fit if you find that all optimizers {except} the default give very similar fits, while the default fit looked nonsensical. (In this case, non-convergence was not a false positive, but we assume that the convergent models are OK.)

We can compare fits by their log-likelihoods and fixed effects (full output shown in Appendix Section 5):\footnote{We could also check random effects, as shown in \ttt{?allFit}, but it is less clear whether these need to be identical across models.}

<<output.lines=1:8>>=
summary(giv_m1.all)$llik
@
<<output.lines=1:8>>=
summary(giv_m1.all)$fixef
@

We see that the values are essentially identical across optimizers, including \ttt{nloptwrap} (the default) and \ttt{bobyqa} (which we used instead).
%including Nelder\_Mead (the default) and bobyqa (which we used instead), 



\subsection{Intrusive methods}
\label{sec:intrusive-methods}

Often, non-convergence is due to the model being too complex for the data. It is reasonable to assume this is the case if non-intrusive methods don't work.  Typically this is because the \textbf{random-effect structure is too complex}.  We will focus on this case, which comes up frequently in practice (see Box~\ref{box:convergence-more} for other cases).
%\citep{brauer2018linear,bates2015parsimonious} give a more comprehensive discussion 
%- Sometimes non-convergence is due to the model being too complex for the data---non-intrusive methods then will not work.  Typically this is because the *random effect structure* is too complex.  We'll focus on this case, which comes up frequently in practice; more comprehensive discussion: Brauer \& Curtin; Bates et al.

The possible steps to take are 2(a)--(d) in Table~\ref{tab:conv-steps}.
% 
% 
% - 1a. - remove random effects involving control predictors (must not be in ixns with RQ predictors)
% - 1b. selectively remove ranef correlations: for those not related to RQs, then those that are prob close to zero
% - 2. remove random intercept (leaving slope term in)
% - 3. remove random slopes for critical predictors
These are in priority order: 2(a) and 2(b) make sense in most cases, and there is not consensus on which one is preferable (removing terms related to control predictors vs.\ removing correlations: \citealp{brauer2018linear,seedorff2019maybe,bates2015parsimonious}).

2(c), removing a random intercept, occasionally makes sense, by the logic that the top priority is having random \textbf{slopes} for critical predictors in the model, to avoid Type I errors (\citealp[][16]{brauer2018linear}; \citealp{barr2013random}).  This is also why 2(d), removing random effects for critical predictors, almost never makes sense.
% 
% - 2 occasionally makes sense, by the logic that the top priority is having random *slopes* for critical predictors in the model, to avoid Type I errors.  This is also why 3 almost never makes sense. 


As an example, consider the \ttt{diatones\_melr} model, but refit with maximal random-effect structure:

% replays:diatones_melr_max <- update(diatones_melr, . ~ . - (1 | prefix) + (1+syll2_coda + syll2_td + frequency + syll2_td:frequency|prefix))

<<echo=FALSE>>=
replay(diatones_melr_max_eval)
@

<<output.lines=17:31>>=
summary(diatones_melr_max)
@

(We have also increased the maximum iterations; the model doesn't converge otherwise.)

As we can see just from the random-effect structure, this model is far too complex for the data: 15 variance components, corresponding to 75 random effects (5 terms $\times$ 13 \ttt{prefix} values), are being fitted to just 130 observations. Let's just assume this is why the model doesn't converge (technically we should first try `non-invasive methods'), and simplify the random-effect structure.

For this example, 2(a) is not an option, as there are no control predictors (all are of interest for the research questions).  We instead begin removing correlation terms (2(c)).
%we go with removing correlation terms. 
One can iteratively remove these, in an order respecting the hierarchy principle (Box~\ref{box:lower-order-terms}): correlations with the \ttt{syll2\_td:frequency} term, then correlations with  \ttt{frequency} or \ttt{syll2\_td} (if the model doesn't yet converge), and so on.  Appendix Section\ 5 shows this process, which results in a model with all correlation terms excluded:

%% replays: diatones_melr_max_2 <- update(diatones_melr, . ~ . - (1 | prefix) + (1+syll2_coda + syll2_td + frequency +syll2_td:frequency||prefix))

<<echo=FALSE>>=
replay(diatones_melr_max_uncorr_eval)
@

<<output.lines=17:25>>=
summary(diatones_melr_max_uncorr)
@

We may or may not choose to simplify the random-effect structure further, as the model is singular, but this model at least converges.

\section{Singular models}
\label{sec:singular-ch10}

Recall that a model is \emph{singular} if a `dimension' of the random effects has been estimated as 0 (Section~\ref{sec:singular-models}).  (See \ttt{?isSingular}, which we summarize here, for more details.)  Typically this means variances that are (near) 0, or correlation parameters near 1 or -1, in which case there is a linear combination of random effects which does not vary (e.g.,\ across participants).  Intuitively these values are unlikely to be correct: in reality there is probably some by-group variation in the intercept and every predictor, which will never be perfectly correlated.  
For example in \ttt{neut\_mod\_4} (Section~\ref{sec:singular-models}), both correlation term estimates are near 1/-1:

<<echo=FALSE>>=
# Refit model from Chapter 8, so code from this chapter (+ Appendix)
# can be run in isolation.
neut_mod_4 <- lmer(vowel_dur ~ voicing + prosodic_boundary +
  place + vowel + (1 + voicing | subject) +
  (1 + voicing | item_pair), data = neutralization)
@



<<output.lines=13:20>>=
summary(neut_mod_4)
@

And in our \ttt{diatones\_melr\_max\_uncorr} above, several random slope terms have variances near 0 (\ttt{syll2\_td}, \ttt{frequency}, \ttt{syll2\_td:frequency}).

% - model is *singular* -- correlation parameters near 1/-1 (or random effect estimated as 0). Unrealistic (text from 7.8.2?).

For correlations sufficiently near 1/-1 (or variances near 0) {lme4} detects the model as singular, and you get a warning message. It is also possible for the model to be singular without perfect correlations or zero variances (Section~\ref{sec:singular-examples} shows an example).

%\ttt{isSingular(neut\_mod\_4)} returns \ttt{TRUE} 
But sometimes a model is near-singular and no warning will be given, even though a random-effect term's estimate is unrealistic. For example, refitting \ttt{neut\_mod\_4} above without the by-item random-effect correlation gives a near-perfect correlation, but the fit is not flagged as singular:\footnote{\citet{bates2015parsimonious} suggest a more thorough way to see if the random effects are overparametrized, by examining principal components to gauge the dimensionality of the estimated random effects. The Appendix shows some examples, using the \ttt{rePCA()} function from {lme4}.} %to gauge dimensionality --- examining PCA of random effects to gauge dimensioanlity:}

<<output.lines=7:15>>=
neut_mod_4_noItemCorr <- update(neut_mod_4, . ~ .
- (1 + voicing | item_pair) + (1 + voicing || item_pair))
neut_mod_4_noItemCorr
@


% 
% 
% (footnote/box:
% - Bates et al. suggest a more thorough way to see if random effects are overparametrized --- examining PCA of random effects to gauge dimensioanlity:
% 
% <<eval=FALSE>>=
% rePCA(neut_mod_4)
% rePCA(neut_mod_4)$subject
% @
% 
% Very low SD for compoennt 2 suggest that there is only 1 dim here. 
% )

Singular or near-singular fits are well-defined models (they maximize (RE)ML), but there are several reasons they are suspect (copied from \ttt{?isSingular}):

\begin{enumerate}
\item Singular fits may correspond to overfitted models that may have poor power
\item Chances of numerical problems and mis-convergence are higher for singular models (e.g., when computing confidence intervals)
\item Standard inferential procedures such as Wald statistics and likelihood-ratio tests may be inappropriate
\end{enumerate}

% 
% On the last point, note that a likelihood-ratio test comparing \ttt{neut\_mod3} and \ttt{neut\_mod\_4} suggests the correlation terms do improve the model:
% <<>>=
% anova(neut_mod3, neut_mod_4)
% @
% 


%% FUTURE: this has now been said several places; cut somewhere...
(1) is  the  biggest worry for linguistic datasets in practice: \textbf{the random-effect structure is often too complex to be estimated given the data} (`overparametrized'). In other words, we often want to fit more complex models than the size and structure of our data allows.
%(e.g., \citealp[][p. 16]{meteyard2020best} for psycholinguistic data).

%% FUTURE: incorporate any of this box?
% \begin{boxedtext}{Broader context: What is overparametrization?}
% 
% What does it mean for a model to be too complex for the data?  Two possibilities, from the mixed-effects modeling literature.
% 
% First, there is a large literature on \textbf{robustness}: the validity of different inferential procedures (\#3 above) as a function of the size and structure of the data a mixed-effects model is fitted to.  To my knowledge, there has been no work addressing this kind of question for (the kinds of data examined in) language sciences in particular---intuitively, what model comlexity is `too much' for what data, e.g.,\ you are likely to just not be able to fit a sensible mixed-effects model, or $p$-values will be wildly off.  
% 
% For perspective, this  literature tends to develop inferential procedures for  data with more than $\sim$40--50 levels per grouping factor (e.g.,~\citep[][6.2]{snijders2011multilevel},  \citealp{maas2004robustness}, \citealp{luke2017evaluating}).  Smaller datasets than this are common in language sciences. If our data are often `small' by mixed-effects modeling standards, our models may often be somewhat overparametrized.  % ( contrasting opinion: Barr et al.)
% 
% Another consideration is \textbf{error}, specifically power: even if a model can be fit to the data, the more complex the model, the less power it has to detect effects of interest.
% 
% \end{boxedtext}



%\footnote{There have not been systematic studies quantifying what model complexity is `too much' for what data, but for perspective:  the mixed-effects modeling literature tends to assume at least $\sim$40--50 levels per grouping factor for valid inferential procedures (\#3) (e.g.,~\citep[][6.2]{snijders2011multilevel},  \citealp{maas2004robustness}, \citealp{luke2017evaluating}).  Smaller datasets than this are common in language sciences. If our data are often `small' by mixed-effects modeling standards, our models may often be somewhat overparametrized.}


% - See Maas and Hox 2004 ``Robustness issues'', Snijders/Bosker for a start on these Qs.
% 
% - The general issues seem to me important to be aware of:
% -- Most results for mixed-effects models (and much of this book) implicirtly assumes you have ``enough data'' -- like 30+ groups, the amount of data per group may be less important.


\subsection{Fixes}

%For steps to take to address singular models, it is useful to distinguish between (near)-perfect correlations and (near-)zero variances. 

Models which are singular due to (near-)\textbf{perfect correlations}, or \textbf{without a clear source} (no perfect correlations or zero variances), should not be used as a final model. You can take the same steps as above to simplify the random-effect structure (Table~\ref{tab:conv-steps}: 2(a)-(d)), until the model is non-singular. 

%*Perfect correlations* :
%- Final model should not have perfect correlations.  Steps you can take are the same as above, for simplifying random effect structure (1a-3).

If a model is singular due to (near-)\textbf{zero variances}: technically, these terms should be removed. But in practice, they should not affect model fit---including the values/significances of fixed effects---so it is fine to either remove these terms or leave them in, bearing in mind that 
%the true value may be `very small' rather than 0.
%that 0 is unlikely to really be the correct value;
there may be a small amount of variability that cannot be detected because the model is overparametrized \citep{brauer2018linear}.
It may especially make sense to leave zero-variance terms in if they are related to critical predictors (e.g.,\ the by-speaker random slope of \ttt{clabel.williams} in \ttt{givenness\_m1}).
%The interpretation in this case is that there is not enough variation to be If  bear in mind that 0 is unlikely to really be the correct value---there may be a small amount of variability that cannot be detected because the model is overparametrized.

% 
% *Zero variances*: 
% - Technically, these should be removed.  In practice, shouldn't affect the model's fit, including values/significances of fixed effects (REF on that, beside Brauer/Curtin), so it's optional---as long as we remember ``zero'' is likely really zero---and may make sense to leave in especially if relevant for RQs.  




% - Steps can take to address correlations near 1/-1:
% 
% 1a. - remove random effects involving control predictors (must not be in ixns with RQ predictors)
% 1b. selectively remove correlations: for those not related to RQs, then those that are prob close to zero, finally all
% 2. remove random-intercept (leaving slope term in)
% 3. remove random slopes for predictors related to RQs.
% 
% these are in priority order: 1a vs 1b make sense in most cases; 2 rarer, 3 almost never.

% In any case, the recommended practice for (near)-singular models is to remove the offending terms, one at a time, until arriving at a non-singular model \citep[e.g.,][8.8.1]{qmld}.  For the \ttt{neut\_mod\_4\*} models above this would result in the model with uncorrelated random effects (\ttt{neut\_mod3}).

% In practice, singular models are extremely common when modeling linguistic data (using correlated random effects), hand-adjusting random-effect structure is time-consuming, 
%time-consuming at best,\footnote{In some cases {lme4} syntax makes it impossible to remove only a single random-effect correlation from the random-effect structure.}
% and it seems odd to remove terms which significantly contribute to the model.
% removed: (even if the likelihood-ratio test is suspect in this case.)  
% What should one do by default?


% - Started discussion in chap. 8; could move some here.

% - Singularity can be indicated by 0/1/-1 values, or values very near these.  Copy a lot of example from chap. 8 here, including(?) neut\_mod\_4 example:
% 
% <<>>=
% neut_mod_4
% @






%-- remove slope, for non-covariate (BC 11)
%-- remove correlations: for those non-RQ-related or near-zero first (BC 15)
%-- remove zero slopes -- they shouldn't affect results anyway.

% (BC steps are for non-convergence, but seems reasonable to apply here as well)

% - The guiding principle is: when simplifying a singular model, whatever dimensions of variability exist, we want to be sure to detect *if* involves predictors of interest.

%% FUTURE: Michaela -- thinks could cut these examples if want to save space (this whole subsection).
\subsection{Examples}
\label{sec:singular-examples}

\subsubsection{Model \ttt{neut\_mod\_4}}

Since \ttt{voicing} is a critical predictor, the only possibility is to remove correlation terms (Table~\ref{tab:conv-steps}: 2(b)). Removing the by-item correlation gives the near-singular model \ttt{neut\_mod\_4\_noItemCorr} above. Removing the by-speaker correlation results in \ttt{neut\_mod3}, our model with uncorrelated random effects from Chapter~\ref{chap:lmm-1} (Section~\ref{sec:lmm-multiple-random-slopes}).

\subsubsection{Model \ttt{giv\_maximal}}
\label{sec:model-giv-maximal}

The maximal model of the \ttt{givenness} data fitted above is singular due to perfect correlations among the by-item random effects:

<<output.lines=1:15>>=
summary(giv_maximal)$varcor
@

We first remove the correlations with the random slope of \ttt{npType.pronoun}, a control predictor:

%% replays: giv_ch10_2 <- update(giv_maximal, . ~ . - (1 + clabel.williams + npType.pronoun | item) + (1 + clabel.williams|item) + (0+npType.pronoun|item))
<<echo=FALSE>>=
replay(giv_ch10_2_eval)
@

<<output.lines=1:15>>=
summary(giv_ch10_2)$varcor
@

The resulting model still has a perfect correlation among the by-item random effects, which we remove, along with the by-item random intercept with near-zero variance (output shown in Appendix Section\ 7.1):

%% replays: giv_ch10_3 <- update(giv_ch10_2, . ~ . - (1 + clabel.williams| item) + (1 + clabel.williams||item))
%% replays: giv_ch10_4 <- update(giv_ch10_3, . ~ . - (1 + clabel.williams|| item) + (0 + clabel.williams||item))

<<echo=FALSE>>=
replay(giv_ch10_3_eval)
replay(giv_ch10_4_eval)
@

The resulting model (\ttt{giv\_ch10\_4}) is still singular, despite having  no remaining zero variances or perfect correlations:

<<output.lines=1:14>>=
summary(giv_ch10_4)$varcor
@

% It is still singular, even after we remove the zero by-item random intercept (output shown in Appendix Section\ 6.1):

% <<echo=FALSE>>=
% replay(giv_ch10_4_eval)
% @

% 
% <<>>=
% summary(giv_ch10_4)$varcor
% @

%This is an interesting case, because the model is singular even though there there are no remaining zero variances or perfect correlations. 
This is because the by-participant random effects are overparametrized---which makes sense, as this is a small dataset---in some way which is not clear from the model table. It would be possible to proceed more incrementally (discussed below; Appendix Section\ 9.1), but let's just try removing all correlation terms:

%% replays: giv_ch10_5 <- update(giv_ch10_4, . ~ . - (1 + clabel.williams * voice.passive + npType.pronoun | participant)  + (1 + clabel.williams*voice.passive + npType.pronoun || participant))

<<echo=FALSE>>=
replay(giv_ch10_5_eval)
@

<<>>=
summary(giv_ch10_5)$varcor
@

This model is still singular, but only due to the zero \ttt{clabel.williams} by-participant slope---which we leave in, as it involves the effect of central interest. (This also explains the underlying reason that \ttt{giv\_ch10\_4} was overparametrized.)  

% If we wanted, we could add back in the by-item intercept, which would give our final model of the \ttt{givenness} data from last chapter (\ttt{givenness\_m1}): `maximal', with uncorrelated random effects.

% 
% - By-item: could iteratively remove by-item correlations:
% 
% <<eval=FALSE>>=
% update(giv_ch10_m1, . ~ . - (1 + clabel.williams + npType.pronoun | item) + (1 + clabel.williams| item) + (0+npType.pronoun||item)) -> k1
% update(giv_ch10_m1, . ~ . - (1 + clabel.williams + npType.pronoun | item) + (1 + clabel.williams+npType.pronoun||item)) -> k2
% @
% 
% - Now: 
% <<eval=FALSE>>=
% rePCA(k2)
% @
% 
% - Have two interpretable dimensions for by-item.  Optional whether to remove by-item intercept.
% 
% - By-subject: could leave as is, or attempt to simplify further.  We'd have to try removing (correlations with) npType.pronoun (not of interest)
% 
% <<eval=FALSE>>=
% update(k2,  . ~ . - (1 + clabel.williams * voice.passive + npType.pronoun | participant) + (1 + clabel.williams * voice.passive | participant) + (0+npType.pronoun|participant)) -> k3
% @
% 
% In practice, at this step I would just compare this `maximal' model to the `maximal with zero-correaltions' model, note qualitatively identical, proceed with either.
% 


%% FUTURE: Michaela: this section (before 10.5.1) can probably be said in two paragraphs
\section{Model selection}
\label{sec:lmm-model-selection}

Actually selecting a model---deciding which terms to include---is often the hardest part of mixed-effects modeling. This is backed up by a survey by \citet{meteyard2020best}, of 163 researchers in psycholinguistics and other language sciences, where ``selecting and specifying models'' was the most common worry about using mixed-effects models, along with a lack of standardized procedures.  Both get at the same basic fact: model selection is a hard topic, because there is no one way to choose a model.  On why, we paraphrase \citet[][\S6.2]{snijders2011multilevel}:

Model selection is difficult because there are two steering wheels: substantive (subject matter related) and statistical considerations. Is is harder still for mixed-effects models, where the second wheel involves specifying both fixed and random effects.

The goal of model selection is a model which describes the data well but without unnecessary complications,  which is of scientific interest, and which does not contain spurious effects that are interpreted as meaningful. Put simply: we want to arrive at a model that makes sense, given our data and research questions, via a sensible procedure that we can describe to a reader.  
%How to do this is an active research area, in particular for linguistic data.

This all means that there are no fixed rules to follow, to find the `best' model based on purely statistical considerations.  Instead, model selection is guided by \textbf{principles} (Section~\ref{sec:model-selection-principles}), which describe what makes a model better or worse.  To motivate these principles, we first show examples of models we do not want to end up at, by mechanically applying simple recipes which are not uncommon in published work (shown in Section~\ref{sec:bad-models}).
%which are commonly used.
%. All are common in published work.
%model selection that you should be aware of, based on research.
%to find the `best' model based purely on `objective' statistical considerations. 

%- ``The purpose of model [selection] is to arrive at a model that describes the observed data to a satisfactory extent but without unnecessary complications. A parallel purpose is to obtain a model that is of substantive interest without wringing from the data drops that are really based on chance but interpreted as substance.''

% - ``The complicated nature of the hierarchical linear model, combined with the two steering wheels for model specification, implies that there are no clear and fixed rules to follow. Model specification is a process guided by the following principles.''



% - as a general rule, and two `wheels' we can turn in selecting one: Snijders \& Bosker: subject matter, and statistical considerations.
% 
% - For mixed-effects models, `stat considerations' has two sub-wheels (fixed and random effects).  Active research area.
% 
%  -  As such, there is no perfect model, and no mechanistic algorithm to find the ``best'' model based purely on `objective' statistical considerations.  we want to arrive at one that makes sense, given our data and research questions, via a sensible procedure that we can describe to a reader. 

% - That said, there are important *principles* underlying model selection that you should be aware of, based on research. 

We then turn to \textbf{practical heuristics} 
%that are more or less agreed on in the literature, 
which you can use to guide your model building process (Section~\ref{sec:model-selection-heuristics}).  These motivate three \textbf{possible model selection approaches} (Section~\ref{sec:model-selection-possible-procedures}): `maximal', `data-driven', and `uncorrelated first', introduced in Chapter~\ref{chap:lmm-1}. These are demonstrated in \textbf{case studies} (Section~\ref{sec:ms-case-study-1}--\ref{sec:ms-case-study-3}). If you would prefer to focus on practical application, you can skip to Section~\ref{sec:model-selection-heuristics}.
 
% - To motivate the need for these principles, it is useful to motivate by showing examples of models we do *not* want to end up at, by mechanically applying a simple recipe. All are common in published work.

\subsection{Bad models}
\label{sec:bad-models}

\paragraph{Model 1: Random intercepts only}

For a number of years after mixed-effects models were introduced to language sciences ($\sim$2008), the default recipe was to fit a model with by-participant and by-item random intercepts (or by-speaker/word for corpus data, etc.), following introductions which emphasized the importance of these terms as improvements on other methods (e.g.,\ RM-ANOVA: Box~\ref{box:repeated-measures}).
%- For a number of years, the default recipe for ``fit a mixed model'' was ``use by-subject and by-item random intercepts'', following introductions to mixed-effects models which emphasized the role of these terms.

As we have now seen a number of times, mixed-effects models without random slope terms are usually problematic, because they can easily lead to `overconfident' conclusions, given the data \citep{schielzeth2009conclusions}.
%confidence intervals that are too narrow, and Type I errors.
%over-confident conclusions---whether Type I errors (concluding an effect is non-null when it is null), or simply 
For example, in Section~\ref{sec:vot-example-rate} we showed two models of a subset of the VOT data,
%(\ttt{vot\_voiced\_core}),  
to test the hypothesis that speaking rate affects VOT. The model with just by-speaker and by-word random intercepts (\ttt{vot\_mod\_1}) suggests strong support for the hypothesis ($|t|=3$, narrow confidence interval), while a model with a by-speaker random slope (\ttt{vot\_mod\_3}) does not ($|t|<2$, wide confidence interval).  

The conclusion from the intercepts-only model is incorrect, because it is overconfident.  In the data the models were fitted to (\ttt{vot\_voiced\_core}), speakers vary greatly in the \ttt{speaking\_rate\_dev} effect, and the results are very influenced by a couple of speakers. The correct conclusion is that we cannot rule out a \ttt{speaking\_rate\_dev} effect of 0.

Overconfidence is a serious problem, whether or not a Type I error has in reality been committed. In this case, the true \ttt{speaking\_rate\_dev} effect is probably not 0, as we saw in the model fitted to the full dataset  (Section~\ref{sec:convergence-std})---i.e.,
%as we saw when fitting a model to the *full* dataset (REF) 
%i.e., 
no Type I error.
But the degree of evidence the data gives against the null---the Fisherian definition of a $p$-value---is still drastically wrong.

\paragraph{Model 2: Too-maximal random effects}

\citet{barr2013random} pointed out this problem for linguistic data (independently of \citealp{schielzeth2009conclusions}, for ecological data), and recommended defaulting to `maximal' random-effect structure---all possible random slopes, and ideally all possible random-effect correlations---to avoid overconfident estimates.  The actual discussion in \citet{barr2013random} is  more nuanced and careful, but the lesson the field took away was the title: ``keep it maximal''.

%- Barr et al. REF pointed out this problem for linguistic data, and recommended defaulting to `maximal' random effect structure, especially all possible random slopes, to avoid overconfident estimates. 

% - Their discussion is more nuanced, but what the field took away from this was the title, ``keep it maximal''.

As we've now seen many times, the main problem with defaulting to `maximal' random-effect structure is practical: it is often too complex for the data, leading to convergence problems, as in our attempt to fit a maximal model for the \ttt{diatones} data above (Section~\ref{sec:intrusive-methods}).  But there is a more important conceptual problem, which we can see by returning to this model.

We can in fact get this model to converge by changing the optimizer, as we've often done for MELR models:

% - Consider again the ``maximal'' model for this data, which did not converge when we tried to fit above (REF).  We can in fact get the model to converge by changing the optimizer:

<<echo=FALSE>>=
replay(diatones_melr_max_bobyqa_eval)
@

The model is singular, but this is a less serious issue than non-convergence.
%and in 2013 it was not on many researchers' radar.  
The fixed effects for this model are:

<<output.lines=33:41>>=
summary(diatones_melr_max_bobyqa, correlation = FALSE)
@

Recall that for this data, the effect of word \ttt{frequency}, and its modulation by syllable structure (other predictors), are of interest (Section~\ref{sec:diatones-dataset}).  From this model, we would conclude that there are \textbf{no} significant predictors of whether a word shifts stress, and no evidence that \ttt{frequency} plays a role.
%- Consider again the diatones data, where the role of word frequency, and its modulation by syllable structure, are of interest.
% 
% 
% - From the fixed effects, we would conclude that there are *no* significant predictors of whether a word shifts stress; in particular there is no evidence that frequency plays a role.  

This conclusion is almost certainly wrong (a Type II error).  Based on prior work in this area, we expect at least some effect, in line with what we have found in more realistic models of this data in previous chapters.  The random-effect structure of the `maximal' model is far too complex for this data ($n = 130$), leading to greatly reduced power and \textbf{underconfident} estimates.

%- This conclusion is (probably) wrong.  Based on prior work in this area, we expect at least some effect, in line with what we see in simpler models of this data (REF). This model's random-effect structure is far too complex for the data, leading to greatly reduced power and *underconfident* estimates.

\paragraph{Model 3: Too-data-driven random effects}

\citet{bates2015parsimonious,matuschek2017balancing} pointed out that mechanical application of `keep it maximal' can easily lead to such overfitted models, and associated Type II errors and model fitting problems.  They recommended balancing Type I and Type II errors by using hypothesis tests (or another criterion, such as comparing AIC/BIC) to decide which random-effect terms to include. Again, their discussion is more subtle and careful, but this `data-driven' recipe is what the field took away, as an alternative to `keep it maximal'.

When applied mechanically, this approach has the same drawbacks as any stepwise approach to model selection (Section~\ref{sec:stepwise-variable-selection}), including that it ignores the `substantive' wheel. As an example, consider again the \ttt{neutralization} data, where the \ttt{voicing} effect is of primary interest; this includes by-participant and by-item variability in the effect.   

As fixed effects, we include those from \ttt{neut\_mod\_3}, as well as a \ttt{voicing:prosodic\_boundary} interaction, which is plausible for this data:\footnote{As in Section~\ref{sec:pred-vals-only}. This is theoretically plausible because (phonetic) contrasts are often enhanced in prosodically strong positions.}

<<>>=
neut_ch10_m1 <- lmer(vowel_dur ~ voicing * prosodic_boundary + place + 
    vowel + (1 | subject) + (1 | item_pair), data = neutralization)
@

(This is the same as is \ttt{neut\_mod\_1} from Section~\ref{sec:two-grouping-random-intercepts}.)
% 
% - Consider again the neutralization data, where the *voicing* effect is of primary interest; this includes subject and item variability in the effect.  Let's include all predictors used so far, as well as the prosodic boundary x voicing interaction, which is plausible.

One common `data-driven' method to build up random-effect structure is a forward stepwise algorithm: start with an intercepts-only model (as above), then add in the random-slope term $Z$ which gives the lowest $p$-value for a likelihood-ratio test (comparing models with and without $Z$). Repeat, until there is no $Z$ which can be added with $p<0.05$.

%- One common ``data-driven'' method is a forward stepwise algorith: the to start with an intercepts-only model, and add in the random-slope term $Z$ that has the lowest $p$ from a likelihood-ratio test (of adding $Z$). Repeat, until there is no term which can be added with $p<0.05$.

As shown in Appendix Section\ 8.2, applying this algorithm  leads to a model with only a by-participant random slope for \ttt{vowel}:

<<eval=FALSE>>=
neut_ch10_m2 <- lmer(vowel_dur ~ voicing * prosodic_boundary + place +
    vowel + (1 + vowel | subject) + (1 | item_pair), 
  data = neutralization)
@

In particular: although adding a by-speaker random slope of \ttt{voicing} improves \ttt{neut\_ch10\_m1} ($p<0.05$), the \ttt{vowel} random slope is added first because it has a lower $p$-value, and adding the \ttt{voicing} slope to this new model gives $p>0.05$.

There are no random slopes of \ttt{voicing} in this model, so we would conclude that neither participants nor items (significantly) differ in the effect.  This is implausible, given both the empirical data (Figure~\ref{fig:neut-var-emp}), which shows great by-participant variability,  and prior work, where such `incomplete neutralization' effects typically vary a lot by participant and item.

Here, we reached the wrong conclusion by not prioritizing terms related to \ttt{voicing}, the predictor of primary interest.

\paragraph{Models 4--5: Misspecified fixed effects}

%FUTURE: maybe just cut this?  Already had Ch 5 section on over/underfitting

Most discussion of model selection has focused on random-effect structure, but choosing appropriate fixed effects is also non-trivial.  The issues here are the same as in our discussion for linear regression (Section~\ref{sec:overfitting-underfitting}, \ref{sec:variable-selection})---there are dangers to either underfitting or overfitting the fixed effects.  This is a particular risk for corpus (or other observational) data, where the set of fixed effects is not usually clear a priori. 
Appendix Section\ 8.3 shows two examples using the \ttt{turkish\_if0} data. 

%- Underfitting and overfitting the *fixed effects* is similar to our discussion for linear regression (REF).  A couple examples will help illustrate how this is a particular risk for corpus data, where the set of fixed effects is not clear a priori.

For corpus data, it is not uncommon to see `minimal' models fitted, which account for a single fixed effect. For this data, \ttt{Voicing} is of primary interest; we have seen that random intercepts for word/consonant/speaker are appropriate; and minimally a by-speaker random slope for \ttt{Voicing} is needed.  This model gives a \ttt{Voicing} fixed effect with $p=0.03$: this is clearly underconfident, given how strong the effect is in a model where control predictors are included (e.g.,\ \ttt{if0\_m0}).  The minimal model is \textbf{underfitted}. 

% - Consider the Turkish IF0 data from above.  The effect of voicing is of primary interest. As we saw, should include nested random effect of consonant. So a minimal model could be:
% 
% <<>>=
% if0_bad_model <- lmer(mean_f0_smtn_50 ~ Voicing.vl + (1+Voicing.vl|speaker) + (1|word) + (1|consonant), data=turkishDf)
% summary(if0_bad_model, correlation=FALSE)
% @
% - we would conclude no significant IF0 effect, which is unlikely---Type II error.  Due to not modeling the control predictors.

At the other extreme, given our interest in \ttt{Voicing}, we might choose a `maximal' fixed-effect structure (in the sense introduced below:  Section~\ref{sec:model-selection-heuristics}), accounting for all possible interactions with control predictors, up to three-way interactions.   The resulting model has 30 fixed-effect terms, of which 15 involve \ttt{Voicing} and most are not significant. This model is \textbf{overfitted}, and too complex to give insight into the research question (``effect of \ttt{Voicing}'').

% At the other extreme, given our interest in \ttt{voicing}, we might ``play it safe'' by accounting for all its possible interactions with  control predictors, up to three-way interactions.  The resulting model (Appendix XX, not shown here) has 30 fixed-effect terms, of which 15 involve Voicing and most are not significant. This model is overfitted, and too complex to give insight into the research question (``effect of Voicing'').

% if0_bad_2 <- lmer(mean_f0_smtn_50 ~ Voicing.vl*(base_vowel+speaker_mean_smtn +  gender + local_f0_smtn)^2 + (1| consonant) + (1 | word) + (1 + base_vowel_AvIU + base_vowel_IvU + Voicing.vl|| speaker), data=turkish_if0)

% - It's also easy to overfit the fixed effects, as we showed by example for linear regression (model ou\_mod\_3): with too many terms, noise is over-modeled.

% But we typically don't know whether or not the null is true, so all we can ask of our statistical models is to  
% This does not change the fact that   Nonetheles
% The conclusion from the intercepts-only model is incorrect: given that speakers vary greatly in the \ttt{speaking\_rate\_dev} effect, and the effect is very influenced by a few speakers (REF), the conclusion from the intercepts-only model is incorrect.
% 
% 
% The first model may not lead us to a Type I error---as we saw above by modeling the *full* dataset (REF), there is probably a non-zero \ttt{speaking\_rate\_dev} effect for this data.

\subsection{General principles}
\label{sec:model-selection-principles}

Laying out principles for model selection lets us articulate what is wrong with such models, and how to arrive at better ones. This discussion follows \citet[][\S6.4]{snijders2011multilevel}, with some edits for my own perspective.\footnote{I have omitted their principles 4 and 5, on ``Doing justice to the multilevel nature of the problem'' and random-effect correlations, to simplify the discussion. Our principles are numbered differently from theirs.}  Principles 1--4 are most important, but all are violable in some situations.

%- Discussion follows Snidjers \& Bosker 6.4, with some edits for my own perspective.
% 
% - The first thing to acknowledge is that model selection is one of the most difficult parts of statistical analysis, because there are two `wheels': substantive and statistical considerations.  It's harder still for mixed-effects models, where the second wheel involves two parts---choosing fixed and random effect terms.
% 
% - ``The purpose of model [selection] is to arrive at a model that describes the observed data to a satisfactory extent but without unnecessary complications. A parallel purpose is to obtain a model that is of substantive interest without wringing from the data drops that are really based on chance but interpreted as substance.''
% 
% - ``The complicated nature of the hierarchical linear model, combined with the two steering wheels for model specification, implies that there are no clear and fixed rules to follow. Model specification is a process guided by the following principles.''



%- Most important principles (my perspective) are 1--4, listed first.

\paragraph{Principle 1: Prioritize subject matter considerations.}  Model selection must be guided by the research questions and study design, theoretical considerations, previous work, and common sense.

For example, in Bad Model 4, the effect of interest (\ttt{Voicing.vl}) is expected to be very small, relative to other factors affecting F0, so our model should control for these.  This principle often comes into play in deciding what terms to \textbf{not} add to a model: unless there are substantive reasons to consider additional predictors, it often does not make sense to add them, to keep the analysis confirmatory rather than exploratory---even if doing so would improve the model's fit \citep[][\S16.4]{winter2019statistics}.
%\footnote{\citet[][\S16.4]{winter2019statistics} gives a nice example describing the thought process behind not examining whether interactions between (theoretically-motivated) predictors improved a model's fit, as requested by a reviewer, because there were no predictions for these interactions.} 
%substantive ---a reviewer requested this step, but there was no expectation about this interaction it there was no substantive motivation,   It is always possible to try additional predictors (as fixed effects) to improve the model's fit,  but unless these are well-motivated, effects that predict another example, Winter (REF) describes a model containing only main effects, motivated by the research questions; a reviewer asked that interactions be considered as well.  There was no expectation from theory or previous work about what interactions would be expected, so the authors did *not* do this, to keep the model focused on the research questions, and not find spurious effects. This is a 
%Principle 1: subject matter considerations. Follow from research questions/study design, theoretical considerations, previous work, common sense.

%Example: M4. the effect of interest is *expected* to be very small, and many factors affect F0,  so we prob need to control for them.

\paragraph{Principle 2: Distinguish between terms on which the research is focused, and terms included for fit.} We call these `critical' and `control' terms. Often, the critical terms are a subset of the fixed effects; all other terms are controls. Our model is designed to estimate the critical terms as accurately as possible.


% Principle 2: distinction between terms on which the research is focused, and terms included for fit.  We call these `critical' and `control' terms.  Often `critical' = a subset of fixed effects; everything else is control.  Our model is designed to get better estimates of critical terms. 

This principle has a range of consequences.  Random-effect selection, whether `maximal' or `data-driven', should prioritize critical terms (as recommended by \citealp{barr2013random,bates2015parsimonious}).  For fixed effect selection: we should always be reluctant to drop critical predictors from the model, and if we need to select from many possible control predictors, we prioritize those which let us better estimate the critical terms (e.g., confounds).

% Range of consequences. If we do data-driven model selection, focus on control terms. Reluctance to drop critical terms from the model. If we need to select control predictors (e.g.,\ for a corpus study), guided by those which are confounds for stimating critical term.

For example, in Bad Model 3, interspeaker variation in the \ttt{voicing} effect is a research question, so we should prioritize by-speaker/item random slopes for \ttt{voicing}, even if this doesn't follow from a pure stepwise procedure.  If adding a random slope for (critical predictor) $x$ gives a singular model, with perfect random-effect correlations, we should try a model with uncorrelated random effects.\footnote{It is very common in current practice for by-$x$ random slopes to not be included, because the resulting model is singular, even when $x$ is a critical predictor. This results in overconfident fixed-effect coefficient estimates.}

% Example: In M3, a RQ is interspeaker variation, so we should prioritize by-speaker/item random slopes for voicing---even if this doesn't follow from the stepwise procedure we're using. If adding a random slope for X gives a singular model (perfect correlation), try an uncorrelated random slope.


\paragraph{Principle 3: Fixed effects need appropriate error terms.}

This primarily means appropriate random slopes, especially for critical predictors. For group-level predictors (e.g., gender, lexical class), this also means random intercepts (as discussed in Section~\ref{sec:controlling-nuisance}). Otherwise there is a serious risk of Type I (and Type II) errors, as in our Bad Model 1. This principle underlies the `keep it maximal' approach to selecting random effects.
%Principle 3: fixed effects need  appropriate error terms. This means *appropriate random slopes*, especially for critical predictors; for group-level predictors, this also means random intercepts 
%which account for non-independence of observations Type I errors on subject/item-level predictors 
%(e.g.,\ a fixed effect of speaker gender requires a by-subject random intercept): \citep{judd2017experiments,baayen2008mixed}.  
% Otherwise, serious risk of Type I (and to a lesser extent Type II) errors, as in our example M1. This underlies the ``maximal'' selection of random effects.

\paragraph{Principle 4: Larger models have lower power to detect individual effects.} This is the case, all else being equal.  In addition, our power is often low in  mixed-effects models, especially for group-level predictors or interactions.
%\citep{brysbaert2018power}. 
It is thus easy to build up models with low power. This principle underlies the `data-driven' approach to random-effect selection.

% Principle 4: awareness of the fact that larger models have lower power to detect indiv effects, all else being equal, and our power is often low in mixed-effects models, especially for group-level predictors (e.g., gender, lexical class).  It is thus easy to build up models with low power. This observation underlies ``data-driven'' selection of random effects.

\paragraph{Principle 5: The model's `hierarchical' structure should make sense.}  

The presence of an $x_1:x_2$ interaction implies main effects of $x_1$ and $x_2$ (in fixed effects or random slopes).  A by-$z$ random slope of $x$ implies a fixed effect of $x$ and a by-$z$ random intercept.

%Principle 5: the model's `hierarchical' structure should make sense. The presence of an $X_1*X2$ interaction implies main effects of $X_1$ and $X_2$; a random slope for $x$ implies a fixed effect of $x$; and by-$Z$ random slope(s) imply a by-$Z$ random intercept.
% 
% \paragraph{Principle 6: The multi-level nature of the problem should guide the selection process.}  For example, the random effect estimates often suggest what important fixed-effect terms could be missing.  A large by-speaker random intercept variance suggests looking for important speaker-level variables (as fixed effects); a large variance for a by-speaker random slope of $x$ suggests looking for interactions of $x$ with speaker-level variables (as fixed effects).
% 
% For example, in Model 4, where the 
% Example: In Model 4, where the result for the critical predictor is unexpected, we could be guided by the large by-speaker random intercept value to look for speaker-level predictors to include as fixed effects. Once these are included in the model (speaker mean F0 and gender), the \ttt{voicing} effect is clarified.

\paragraph{Principle 6: Be reluctant to overfit / include non-significant effects in the model.} 

For mixed-effects models, this can apply to either fixed effects (as in Bad Model 5) or  random effects (as in Bad Model 2).

Other principles can override \#6, most importantly subject matter/research question considerations (Principle 1).  For example, in modeling the \ttt{givenness} data, where the \ttt{clabel.williams} effect is of primary interest, it would not make sense to exclude fixed effects of \ttt{voice.passive} or \ttt{npType.pronoun}, regardless of significance. These predictors are part of the experimental design.  In our final model (\ttt{givenness\_m1}), there is a non-significant main effect of \ttt{voice.passive}, which cannot be excluded because it participates in an interaction with \ttt{clabel.williams} (Principle 5).
%Example: in modeling givenness data,  would not make sense to exclude voice or npType regardless of significance; these are part of experimental design (Principle 1).  In our final model (givenness\_m1), there is a non-significant main effect of voice.passive; this can't be excluded because it participates in an interaction (Principle 3).

\paragraph{Principle 7: Be reluctant to underfit.}

This usually just means, don't drop any significant terms.  Exceptions could include truly large datasets (where everything may be significant, regardless of importance), and the desire to keep models confirmatory and substantively motivated, rather than exploratory (see Principle 1).

% Principle 8: reluctance to underfit.  Usually means, don't drop anything significant (except in truly large datasets)



\subsection{Practical heuristics}
\label{sec:model-selection-heuristics}

% Principles are great, but these conflict...

Principles are great, but what should we actually do in practice? This section lays out a general procedure and practical heuristics to inform your model-building process. Each heuristic is violable in some settings: given how much model selection depends on the data and research questions, our goal is to give a set of general guidelines rather than strict rules.  We focus on points on which there seems to be broad agreement, informed by the sources in Section~\ref{sec:other-reading-ch10}, paragraph 1. 

The heuristics assume that you have hypotheses/research questions about fixed-effect terms (possibly some, not all), and that random-effect terms are to prevent errors on fixed-effect estimates.  This is the most common case, and some adjustment would be needed if, for example, individual differences (= by-participant random effects) were of primary interest.

% - Here, we're assuming you have hypotheses about some main effects and interactions, but not random effect structure; in other words, only fixed effects are of theoretical interest (possibly some, not all), and random effects are to prevent errors on fixed effect estimates.  This is the most common case; some adjustement would be needed if, for example, individual differneces (= by-subject random effects) were of primary interest.

The high-level procedure we assume is:\footnote{This roughly follows the `top down' method of \citet[][\S5.7]{zuur2009extensions}; \citet[][\S2.7]{west2014linear}. \citet{snijders2011multilevel} describe a `bottom-up' strategy that is more widely used in some fields, where the model is built up alternating between fixed and random-effect terms at different levels. All heuristics except \#4 would hold for the `bottom-up' strategy.}

\begin{itemize}
\item Step 1:
%Preliminary steps: 
exploratory analyses, visualization, selecting possible predictors, etc.
\item Step 2 Build `maximal' \textbf{fixed effects} structure (more on this below)
\item Step 3: Select \textbf{random effects} structure, using those fixed effects
\item Step 4: Do any pruning of fixed effects
% - General recommended procedure (e.g., Zuur):
% -- (0): exploratory analyses: plots, etc.
% -- (1) build `maximal' *fixed effect* structure (more on this below)
% -- (2): select *random effect* structure, using those fixed effects.
% -- (3): do any pruning of fixed effects
\end{itemize}


Three first heuristics inform this whole procedure:

\paragraph{Heuristic 1: Theoretical/subject matter considerations are most important.}

\paragraph{Heuristic 2: Non-convergent models are invalid. Singular models are dispreferred.}

\paragraph{Heuristic 3: Practical considerations can play a role.}

For example, it is fine to use different model selection procedures in settings where each model takes hours to fit versus seconds to fit (e.g.,\ large corpus datasets vs.\ small laboratory datasets).

%\subsubsection{General procedure}


%% - This is the `top-down' method: described with justification by West et al 2014 2.7 . 
%% - Alternative is `bottom-up' method of Snijders/Bosker -- roughly, start w/ random intercepts, then level 1 covariates + slopes; then level 2 covariates + slopes.
%% - What I haven't seen descirbed elsewhere in Ling stats literature is: 
%% -- There's more than one way to do this.
%% -- But most ways don't make sense -- including most see in literature.  Ex: starting with random intercepts only, then 


% In practice, especially when models take a long time to fit (the norm for my own work),  a simpler method is often used: choose the *final* fixed effect structure in (1);  erring on the side of including more terms; skip step (3).  This lazier procedure avoids having to refit models (in step (3)), avoids the temptation to boost $p$-values by dropping other terms (in step (3)), and simplifies random-effect selection (step (2)).  This is the method illustrated in Case Studies below.

We assume here that Step 1 is complete.  The next heuristics involve fixed-effect selection, Step 2.
%\subsubsection{Fixed effects}
%\label{sec:heuristics-fixed-effects}

\paragraph{Heuristic 4: Choose fixed effects before random effects}

The motivation for Step 2 coming before Step 3 is to get accurate residuals, because the random effects are modeling structure among the residuals---the `left over' variance after accounting for fixed effects \citep[][\S7.5]{fitzmaurice2008longitudinal}.  


In some cases, such as a classic factorial-design laboratory experiment, the fixed-effect structure is known in advance.  But usually, there are many possible predictors, and we have to choose what terms to include.  It is important that the fixed-effect structure err on the side of including more terms, in case there are (fixed) effects that only become clear once by-group variability is accounted for.   This \emph{maximal} fixed-effect structure is ``the most elaborate or complex model for the mean response that we would consider from a subject matter point of view''  \citep[][\S7.5]{fitzmaurice2008longitudinal}.  You can then always choose to prune terms (e.g., using model comparison) once the random effects have been selected (Step 3).
%The idea is that your fixed effect structure should err on the side of `maximal' -- include more terms -- in case there are effects that only become clear once by-group variability accounted for


% - This is to get accurate residuals, because the random effects are modeling structure among residuals (Fitzmaurice et al 7.5).

% - In some cases (classic experiment), fixed effects are known in advance. 
% 
% - Otherwise (usually), need to choose.  The idea is that your fixed effect structure should err on the side of `maximal' -- include more terms -- in case there are effects that only become clear once by-group variabiltiy accounted for.

In practice, choosing a `maximal model' is a subjective matter, with no automatic recipe.  The issues here are similar to model selection for (non-mixed-effects) linear regression, and different approaches are possible.  The maximal model could be ``all explanatory variables and as many interactions as possible'' \citep[][\S5.7]{zuur2009extensions}, or something more conservative, like a `maximum reasonable fixed-effect structure', chosen on theoretical grounds---similarly to the Gelman \& Hill method discussed for linear regression (Section~\ref{sec:gh-method-example}), but without dropping any terms.

% - In practice, ``maximal model'' is a subjective matter, with no automatic recipe, similar to chap. 5 discussion.  Could be  ``all explanatory variables and as many interactions as possible'' (Zuur), or something more conservative -- ``maximum reasonable fixed effects structure'', like the Gelman \& Hill method to choosing terms, without dropping any.

One common approach, which is strongly dispreferred, is selection of fixed effects by a purely stepwise procedure (e.g.,\ including all terms which lower the model's AIC); this has the same issues as stepwise regression (Section~\ref{sec:stepwise-variable-selection}).  However, it may be reasonable to do some model comparison in reaching your maximal model; this should be done using models with random intercepts only, as these control for basic non-independence of observations without assuming anything about fixed effects.

Since the issues here are similar to variable selection for non-mixed-effects models from Chapter~\ref{chap:linear-regression-2},  we will just assume  that a suitable maximal fixed-effect structure has been chosen in our case studies, where we will also not do the optional term-dropping Step 4. In summary:

\paragraph{Heuristic 5: Prefer a larger fixed-effect structure; do not choose by a purely automatic procedure.}

%\subsubsection{Random effects}

Selecting random-effect structure (Step 3) is harder,
%is harder,
and has been the focus of most work in the linguistics/psychology literature.   Different approaches are possible, which we describe after articulating broad points on which there is consensus.  These heuristics follow from Principles 1--4.



\paragraph{Heuristic 6: Random intercepts are almost always necessary.}
%5. *Random intercepts* are almost always necessary

%to account for non-independence of observations.  These also are necessary to avoid Type I errors on subject/item-level predictors (e.g.,\ a gender effect without a by-subject random intercept).

\paragraph{Heuristic 7: Random slopes must be considered at least for critical predictors, whether included a priori or via hypothesis testing.} 

%Heuristics 6 and 7 correspond to the `maximal' and `data-driven' approaches.


% 6. Must at least consider random slopes for all predictors of interest (`critical predictors'), whether included a priori (``maximal'') or via hypothesis testing (``data-driven'')
% (Principle 9)

\paragraph{Heuristic 8: Including random slopes is more important than including correlations between random effects.}


\paragraph{Heuristic 9: Random-effect terms (intercepts, slopes, correlations) related to critical predictors should be prioritized over those related to control predictors.}

\paragraph{Heuristic 10: Random-effect correlations for intrinsically-related predictors (factors, nonlinear effects) are more important than other correlation terms.}

\subsection{Possible procedures}
\label{sec:model-selection-possible-procedures}

% - General recommended procedure (e.g., Zuur):
% -- (0): exploratory analyses: plots, etc.
% -- (1) build `maximal' *fixed effect* structure (more on this below)
% -- (2): select *random effect* structure, using those fixed effects.
% -- (3): do any pruning of fixed effects

A high-level procedure is outlined as Steps 1--4 above. In practice, especially when models take a long time to fit, a simpler method is often used: choose the final fixed-effect structure in Step 1, erring on the side of including more terms, and skip Step 3.  This (lazier) procedure simplifies random-effect selection (Step 2), avoids having to refit models, and avoids the temptation to boost $p$-values by dropping terms (Step 3).  This is the method illustrated in case studies below.

% In practice, especially when models take a long time to fit (the norm for my own work),  a simpler method is often used: choose the *final* fixed effect structure in (1);  erring on the side of including more terms; skip step (3).  This lazier procedure avoids having to refit models (in step (3)), avoids the temptation to boost $p$-values by dropping other terms (in step (3)), and simplifies random-effect selection (step (2)).  This is the method illustrated in Case Studies below.

Within this (lazier) procedure, different model selection procedures essentially come down to \textbf{random-effect selection} (Step 3).  Heuristics 5--9 suggest fitting models with rich random-effect structure, at minimum including random intercepts and appropriate random slopes, to guard against (primarily) Type I errors in critical predictors.

% - These consensus points (Heuristics 5--9) suggest fitting models with rich random effect structure: at minimum, including random intercepts, and appropriate random slopes to guard against errors in critical predictors, especially Type I errors.

There are two common approaches, within these parameters, which we have seen:\footnote{\citet{seedorff2019maybe} argue based on simulations that a `model space' version of the data-driven approach, where all possible random-effect structures are compared using (for example) AIC, performs better than a hypothesis-testing approach. %Obviously 
This is 
%only 
feasible for models with a small number of fixed effects.}

\begin{enumerate}
\item \textbf{Maximal} approach: include random-effect terms based on the study design. Prioritizes Type I error. \citep{barr2013random}

\item \textbf{Data-driven} approach: include random-effect terms if they improve the model, using a liberal $\alpha$-value, e.g.,\ $\alpha=0.2$.  Tries to balance Type I error and power. \citep{bates2015parsimonious,matuschek2017balancing}

\end{enumerate}

% * `Maximal': include random-effect terms based on study design/stat considerations---prioritizes Type I error
% 
% * `Data-driven' approach: include based on whether they improve the model---*using a liberal alpha level, such as 0.2*.  trade-off between Type I error and power.

I often use a third approach, outlined in Chapter 8, given the difficulties of fitting either fully-maximal models or many models (required for the data-driven approach) for corpus data (many fixed effects and/or large datasets):

\begin{enumerate}
\item[3.] \textbf{Uncorrelated first} approach: fit a model with all possible random slopes, without any random-effect correlations.  Then consider iteratively adding in correlations, proceeding heuristically.
\end{enumerate}

`Proceed heuristically' could mean  prioritizing correlations involving critical predictors or intrinsically related predictors (Heuristics 9--10), or examining pairwise plots of random effects to see which correlations are largest.  Adding correlation terms can be justified using model comparison, or in another way.

The general ideas of this approach are to heavily prioritize random slopes over correlation terms in building up random-effect structure (Heuristic 8), and acknowledge practical considerations (Heuristic 3).


% FUTURE: Michaela: here it starts to feel a bit repetitive. I don't think that's a bad thing, but it could be trimmed potentially.
Whichever procedure you use, there are several choice points to arrive at an actual algorithm, including:
\begin{itemize}
\item `Backwards' (start with maximal, reduce) or `forwards' (start with minimal, build up)?
%`Maximal' and `Data-driven' are the most common.  Whichever procedure you use, there are several choice points within these, including:
\item How to do each step: assess every possible term which could be added/dropped, or consider terms sequentially (e.g.,\ add the random slope with lowest $p$-value vs. the first one which has $p<0.2$)?
\item How much to prioritize trying uncorrelated random-effect structure, as a practical matter.  (The `maximal' and `data-driven' papers do discuss dropping correlation terms, they just emphasize it less than the `uncorrelated first' approach.)
\end{itemize}

The literature describes a range of concrete algorithms  and assesses them using simulation studies (Box:~\ref{box:ranef-select-lit}).   We now give three case studies, each showing one approach.

\begin{boxedtext}{Broader context: Random-effect selection}
\label{box:ranef-select-lit}
% 
%  - how much to prioritize *uncorrelated* random-effect structures, as a practical matter.
%  - The literature---Barr et al; Bates et al; Seedorf---describe some concrete algorithms, for fairly simple datasets.
Random-effect selection is an active research area \citep{barr2013random,bates2015parsimonious,matuschek2017balancing,seedorff2019maybe}, and end users worry a lot about this issue when analyzing data \citep{brauer2018linear,meteyard2020best}.

But as \citet{seedorff2019maybe} emphasize, it is still unclear how much the exact procedure matters---in terms of Type I error and power---across different kinds of data (e.g.,\ corpus data) and types of models. For example, the datasets considered in this literature are fairly simple, from (psycholinguistic) laboratory experiments, and most work only considers linear mixed-effects models.
%(\citealp{seedorff2019maybe} consider logistic regression). 
Even once there is more work in this area, given the diversity of linguistic data, there will never be a one-size-fits-all recipe. I have in mind here the example of ecology, which has similarly diverse data, where a large literature exists on model selection approaches in different settings.
%especially given the diversity of linguistic data, and emphasize that there is no one recipe---
In my experience, (language) researchers are anxious to follow a recipe, and extract one from a paper cited above, or follow a procedure they or a colleague have used before.  It is better to \textbf{think about what method makes sense} for the kind of data you are dealing with and your research questions. However you do model selection, it is crucial to \textbf{report and justify your strategy}.

Although different model selection strategies are possible, it is important to choose your strategy in advance, and adjust as little as possible once you are analyzing the data.  It is too easy to choose a `strategy' that happens to support your hypotheses.  Reporting can be anything from verbal description of general principles to enumerating the models tried and posting full analysis scripts. \citet{meteyard2020best} give an excellent and reassuring discussion of these issues.

\end{boxedtext}


\subsection{Case study 1: `Maximal' approach}
\label{sec:ms-case-study-1}

This example uses the \ttt{givenness} data.  

\paragraph{Fixed effects}

As our `maximal' fixed-effect structure, we include the three predictors which are part of the experimental design (\ttt{clabel.williams}, \ttt{npType.pronoun}, \ttt{voice.passive}), as well as the one interaction with the predictor of primary interest (\ttt{clabel.williams:voicing.passive}) which looks promising from exploratory plots (Exercise~\ref{ex:givenness-eda}).  This is the fixed-effect structure of \ttt{givenness\_m1} (Section~\ref{sec:givenness-m1}).

In this case, another sensible `maximal' fixed-effect structure could be to include all two-way interactions with \ttt{clabel.williams}.

\paragraph{Random effects}

We use a backwards-selection version of the `maximal' approach:
\begin{enumerate}
\item Fit the model with maximal random effects
\item Deal with any convergence or singular model issues
\item If this requires simplifying random-effect structure, remove as few terms as possible
\end{enumerate}

% 
% - An example of maximal (no hypothesis testing), backwards selection of random effect structure, for givenness data:
% -- Fit maximal model
% -- Deal with any convergence, singularity
% -- Keep ranef structure as maximal as possible, following principles 1-3

We already did (1) in Section~\ref{sec:changing-optimizer-parameters}, resulting in the singular model \ttt{giv\_maximal}. Appendix Section\ 9.1 shows the full process of (3), iteratively removing random-effect terms following Steps 2(a)--(d) from Table~\ref{tab:conv-steps}.  We eventually end up either  at the model with uncorrelated `maximal' random effects shown in Section~\ref{sec:model-giv-maximal} (\ttt{giv\_ch10\_5}), or at that model with a single correlation term (not shown here).
% 
% - Already did (1), above. Ended up with model \ttt{giv]\_ch10\_4}, still singular.  Appendix 8.1 shows the full process of (3), iteratively removing random-effect terms, following principles (1)-(3).
% 
% We eventually arrive either at the model with uncorrelated `maximal' random effects shown above (\ttt{giv\_ch10\_5}), or at that model with a single correlation term left in (not shown here).

<<output.lines=20:37>>=
summary(giv_ch10_5)
@

This model is still singular, but only due to the zero \ttt{clabel.williams} by-participant slope---which we leave in, as it involves the effect of central interest, by Heuristic 1.  If we wanted, we could add back in the by-item intercept, which would give our final model of the \ttt{givenness} data from last chapter (\ttt{givenness\_m1}): `maximal', with uncorrelated random effects. Since this term has zero variance, the fit will be identical.  

In sum, for this case, the `maximal' model has all random slopes, uncorrelated.
%and no correlations.
%uncorrelated random-effect model is %about  as maximal as we can go.

% (\ttt{givenness\_m1}), or that model with almost all correlations removed:
% `
% - Already done (a)-(b) above (singularity example); ended up with givenness\_m1, the model with uncorrelated random effects.
% 
% - To do (c), could check what correlation terms might be added back in:
% % pairscor.fnc(ranef(givenness_m1)$participant) ## npType/voice, clabel.williams
% % pairscor.fnc(ranef(givenness_m1)$item) ## clabel/npType
% 
% - Adding any of those in gives a singular model (perfect correlations: Appendix XX), suggesting that uncorrelated random effect is as `maximal' as we can go.

\subsection{Case study 2: Data-driven}
\label{sec:ms-case-study-2}

This example uses the \ttt{turkish\_if0} data.  We start with the intercepts-only model \ttt{if0\_m0} from Section~\ref{sec:controlling-nuisance}:

<<>>=
formula(if0_m0)
@

\paragraph{Fixed effects}

The fixed effects here are chosen based on the research question (\ttt{Voicing.vl+ Voicing.vl:base\_vowel}), with most possible control predictors from the dataframe added.
%\footnote{\ttt{speaker\_mean\_f0}, the speaker's average f0 for the corpus, could also be added, but we leave it out for simplicity---the by-speaker random intercept accounts for speaker differences.}
The `maximal' fixed-effect structure could in principle contain further interactions with \ttt{Voicing.vl}. As we don't have any clear expectations for other interactions, we omit these for simplicity.

\paragraph{Random effects}

We use a `forward' version of the data-driven approach, prioritizing critical predictors (Heuristic 9). 

We start with the intercepts-only model \ttt{if0\_m0}, and test whether each possible random slope {related to the critical predictors} improves the model, defined as a  likelihood-ratio test (comparing REML fits) with $p<0.2$. If a slope doesn't improve the model when added with correlation terms, we try adding it without correlation terms. The possible random slopes are:
\begin{enumerate}
\item By-speaker random slopes of \ttt{Voicing.vl}, \ttt{base\_vowel}
\item By-speaker random slope for their interaction
\item By-consonant random slope of \ttt{base\_vowel}
\end{enumerate}

We can then consider random slopes involving control predictors.  The rationale here, rather than a pure data-driven method (just consider all possible random slopes, add the one with the lowest $p$), is to avoid the kind of problem we saw in Bad Model 3, by prioritizing random effects for critical predictors.

Both the \ttt{Voicing.vl} and \ttt{base\_vowel} slopes improve the model with $p<0.2$:

<<output.lines=5:7>>=
## Test by-speaker random slope of Voicing
newMod1 <- update(if0_m0, . ~ . - (1 | speaker) + (1 + Voicing.vl | speaker))
anova(if0_m0, newMod1, refit = FALSE)
@


<<output.lines=5:7>>=
## Test by-speaker random slope of base_vowel
newMod2 <- update(if0_m0, . ~ . - (1 | speaker) + (1 + base_vowel | speaker))
anova(if0_m0, newMod2, refit = FALSE)
@


<<output.lines=15:35>>=
## Model with both random slopes added
if0_m1 <- update(if0_m0, . ~ . - (1 | speaker) +
    (1 + Voicing.vl + base_vowel | speaker))
@

Further model comparisons (in Appendix Section\ 9.2) show that adding term (2) from the list above improves the model ($p<0.2$), but is only possible with uncorrelated random effects, and adding term (3) does not improve the model ($p>0.2$).  The interim model is thus:

% fitted in appendix
<<eval=FALSE>>=
if0_m2 <- update(if0_m1, . ~ . +
  (1 + Voicing.vl + base_vowel_AvIU + base_vowel_IvU | speaker) +
  (0 + Voicing.num:base_vowel_AvIU + Voicing.vl:base_vowel_IvU || speaker))
@

<<output.lines=15:49>>=
summary(if0_m2, correlation = FALSE)
@

For the remaining fixed effects (control predictors), there are 8 possible random slopes:
\begin{itemize}
\item  \ttt{gender}: by-word, by-consonant
\item  \ttt{utterance\_num\_sylls}: by-speaker, by-word, by-consonant
\item  \ttt{local\_f0}: by-speaker, by-word, by-consonant
\end{itemize}

Appendix Section\ 9.2 shows the full process of considering each one for addition to the model.   Only a by-word random slope of \ttt{utterance\_num\_sylls} improves the model with $p<0.2$, so our final model includes this term:

% fitted in appendix
<<eval=FALSE>>=
if0_m3 <- update(if0_m2, . ~ . + (0 + utterance_num_sylls | word))
@


<<output.lines=16:52>>=
summary(if0_m3, correlation = FALSE)
@

% 
% - Data-driven model selection is very time-consuming. In this case, random effects for control predictors required fitting 12 additional models, each of which takes a few minutes.  In other settings, fitting these extra models could take days. For example, in more realistic corpus data, there may be closer to 10-15 control predictors and each model could take hours.  Even for our case, fitting and evaluating the extra models took me hours---and the final model is almost identical to the one without random slopes for control predictors.
% 
% - It can be reasonable to triage which random slopes are considered. The simplest option is to just not consider random slopes for control predictors, and simply stop after having considered all possible random effects for critical predictors.


\paragraph{Results}

Figure~\ref{fig:if0-mod-preds} shows the model-predicted \ttt{f0} as a function of \ttt{Voicing.vl} and \ttt{base\_vowel}, which address the main research question (effect of voicing on F0). The model suggests that there is a significant consonant voicing effect on F0 in Turkish, of about 0.75 semitones (averaging across vowels); it is larger for high vowels (\tsc{i}/\tsc{u}) than for low vowels (\tsc{a}).  We could alternatively report the results in terms of the \tsc{voiceless}$>$\tsc{voiced} difference for each vowel, using post-hoc tests (Exercise~\ref{ex:if0-posthoc}).
 % exercise here would be: refit the model using factor version of Voicing.vl. Now do post hoc tests for all claims want to make: voicing effect for A, I, U (is it there for each vowel?), and A < I, U.


<<if0-mod-preds, echo=FALSE, message=FALSE, warning=FALSE, fig.asp=0.5, out.width='55%', fig.width=default_fig.width*.55/default_out.width,  fig.cap='Interaction plot of \\ttt{Voicing.vl} and \\ttt{base\\_vowel} for model \\ttt{if0\\_m2}, with 95\\% CIs, marginalizing over other predictors.'>>=
ggeffect(if0_m2, terms = c("Voicing.vl", "base_vowel")) %>%
  data.frame() %>%
  mutate(Voicing = factor(x, labels = c("voiced", "voiceless"))) %>%
  rename(base_vowel = group) -> if0_m2_df

if0_m2_df %>% ggplot(aes(x = Voicing, y = predicted)) +
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, shape = base_vowel), position = position_dodge(width = 0.5)) + 
  geom_line(aes(group = base_vowel, group = base_vowel), position = position_dodge(width = 0.5)) + 
  ylab("Predicted F0")

@


There is also substantial speaker variability in the \ttt{Voicing.vl} effect, as discussed later (Section~\ref{sec:by-sub-uncert}).
%- There is also subsatntial speaker variability, as discussed later (REF).

\begin{boxedtext}{Practical advice: Common sense in random effect selection}
\label{box:ranef-common-sense}

Whatever method you use for random effect selection must be tempered by common sense: what is possible given the 
%---`maximal', `data-driven', etc.---using common sense is crucial.  our procedure must be partially based on practical considerations: the 
structure of the data and how long models take to fit.  This is especially true for models with many predictors (e.g.,\ corpus data), or fitted to particularly large (or small) datasets. 

For example, for the Turkish IF0 data, truly `maximal' random-effect structure---with all possible slopes, including correlations---would be impossible (try it).  This model contains more by-word random-effect terms than observations, making it literally impossible to fit. Common sense would dictate excluding all by-word random slopes, as there are only about 3 observations per word.
%
More generally, when you have many predictors relative to sample size, fitting models with `maximal' random-effect structure (including correlations) will often result in non-convergent models or massively overfitting. Since most predictors are controls, it isn't worth the effort to spend hours trying to fit over-complex models.

On the other hand, as the Turkish IF0 example in Appendix Section\ 9.2 illustrates, fully `data-driven' methods are extremely time-consuming, even for a modest ($\sim$5--10) number of predictors. The sheer number of possible model comparisons is huge, there are not currently good tools to automate the process, and pure stepwise methods are dangerous in any case.  For models that take an hour to fit---which is not uncommon for corpus data---a pure `data-driven' method is impossible.   More common is the situation in the Turkish IF0 example: fitting and evaluating models to evaluate random effects unlikely to be relevant for the research questions (random slopes for control predictors) took me hours, and the final model (\ttt{if0\_m3}) is almost identical to the model before these steps (\ttt{if0\_m2}).

The upshot is that practical considerations (our Heuristic 3) are an important reason to not just mechanically apply a recipe.  It helps to use the \textbf{goals of your analysis} and the structure of the data central to choose between different possibilities; typically this means prioritizing terms related to the research questions. There is no shame in this; you just have to \textbf{report when writing up your analysis what you did, and why} (``more complex random-effect structure was not considered due to...'').

In the Turkish IF0 example, it would have been reasonable to not consider random slopes for control predictors, and simply stop after having considered all possible random effects for critical predictors.  Our justification could be that we are taking a mild risk of underfitting the data for convenience, but with good confidence about our estimates for critical effects. It would also be reasonable to only try adding by-speaker random slopes for control predictors, with the justification that by-word random slopes are unreasonable (by the logic above), and the only purpose of by-consonant random effects is to get a more accurate estimate for \ttt{Voicing.vl}, which requires a random intercept, but no random slopes.

\end{boxedtext}

% - Have to discuss general model selection procedure: fixed then random effects, or v.v. (We'll assume fixed $>$ random; Zuur does the latter.)    (This is referenced in previous chapter)
% 
% - Fixed effect selection similar to lin reg.  There are different ways to do this, and depends on goals of your study.
% 
% 
% Once we get into random effects selection...
% 
% Examples:
% 
% - 8.8.1.1, from QMLD
% 
% - Example of ``backwards'' random effect selection. Of primary interest here is clabel.williams.
% 
% - Worked example w/ gievnness data, gory detail: in melr scratch.  (Replaces example from QMLD, which also uses givenness data.)
% 

\subsection{Case study 3: Uncorrelated first}
\label{sec:ms-case-study-3}

This example returns to the \ttt{vot\_voiced} data from above (Section~\ref{sec:changing-optimizer-parameters}: model \ttt{vot\_mod3\_full}).  The research question remains ``is there an effect of speaking rate?'' In models fitted to this data so far, we have operationalized speaking rate as within-speaker deviation (\ttt{speaking\_rate\_dev}). To make things more interesting, we now consider the speaker's mean rate (\ttt{speaking\_rate\_mean}), which we saw briefly in Section~\ref{sec:uncorr-ranef-preds}. So speaking rate could affect VOT via either faster speech (within-speaker), or faster speakers.


We first standardize this variable  and extract numeric contrasts for \ttt{place}, which we will need for uncorrelated random effects:

%- Details: Appendix 8.3

% - VOT voiced data (\ttt{vot\_voiced}) from above. RQ remains, effect of speaking rate. To make things more interesting, in addition to our usual predictor, speaking rate deviation, let's add in speaker's mean rate (\ttt{speaking\_rate\_mean}). So speaking rate could affect VOT via either faster speech (within-speaker), or faster speakers. 

% - Standardize this variable, and extract numeric contrasts for \ttt{place}, which we'll need for uncorrelated random effects:

<<>>=
vot_voiced <- mutate(vot_voiced, 
  speaking_rate_mean_std=rescale(speaking_rate_mean))
mm <- model.matrix(~place, vot_voiced)
vot_voiced$place1 <- mm[, 2] ## contrast 1
vot_voiced$place2 <- mm[, 3] ## contrast 2
@

\paragraph{Fixed effects}

As fixed effects we will use \ttt{speaking\_rate\_dev}, \ttt{speaking\_rate\_mean}, and the same set of 6 control predictors as above.  The base (intercepts-only) model is:

<<>>=
vot_voiced_m0 <- lmer(log_vot ~ speaking_rate_dev + foll_high_vowel +
  cons_cluster + stress + log_corpus_freq + place + gender +
  speaking_rate_mean_std + (1 | word) + (1 | speaker), 
  data = vot_voiced)
@

At this point we could consider adding interaction terms for a larger `maximal' model, expanding `the effect of speaking rate' to include modulation by other predictors. Exercise~\ref{ex:vot-voiced-extended} asks you to consider this case, which gives interesting results, but for the current example we use the fixed effects in \ttt{vot\_voiced\_m0} as `maximal'.

%% - This is actually an interesting extended exercise -- how would we use a more maximal fixed effect structure?  Corresponds to Project 2, sort of.
%% - speaking rate example: could consider interactions
%%   with both speaking rate predictors.  We don't have any hypotheses,
%%   So we could do this in a couple ways -- just include all terms, and
%% prune later, or try to somehow be more selective.
%%  - Let's follow GH advice, and consider interactions with predictors
%%    with large effects.  From Anova(vot_voiced_m0), these are
%%    place, cons_cluster.  So, add these interactions to the original model
%%  - update(vot_voiced_m0, . ~ . + (speaking_rate_dev + speaking_rate_mean_std)*(place+cons_cluster)) -> vot_voiced_m0_2 
%%  - Add all random slopes, uncorrelated:
%%  - vot_voiced_m1_2 <- update(vot_voiced_m0_2, . ~ . -(1|speaker) - (1|word) +(1+gender + speaking_rate_mean_std+speaking_rate_dev||word) + 
%%  (1+speaking_rate_dev*(place1+place2+cons_cluster) + foll_high_vowel + stress + log_corpus_freq||speaker),  data=vot_voiced)
% 
%% effects for those which come out signif:
% ggeffect(vot_voiced_m1_2, c('cons_cluster', 'speaking_rate_dev [-2:2]')) %>% data.frame() %>% ggplot(aes(x=group, y=predicted)) + geom_line(aes(color=factor(x), group=factor(x))) + xlab("speaking rate dev")
% ggeffect(vot_voiced_m1_2, c('place', 'speaking_rate_mean_std [-2:2]')) %>% data.frame() %>% ggplot(aes(x=group, y=predicted)) + geom_ribbon(aes(color=x, group=x, ymin=conf.low, ymax=conf.high, fill=x), alpha=0.5) + xlab("speaking rate mean")

%% suggests: *within-speaker*, smaller contextual effect (CCluster) for faster speech
%% - faster *speakers* have a smaller POA contextual effect

%% Overall, suggests *within-speaker* rate affects VOT for voiced stops; also small contextual effect on both within and across-speaker. mysterious...


Note that this model contains several non-significant fixed effects ($|t|<2$):

<<>>=
tidy(vot_voiced_m0) %>%
  select(-effect, -group, t = statistic) %>%
  filter(abs(t) < 2)
@

If we wanted to consider removing these terms, that would come after building up the random effects.

\paragraph{Random effects}

We start with the model with all possible random slopes, uncorrelated:

% fitted in appendix
<<eval=FALSE>>=
vot_voiced_m1 <- update(vot_voiced_m0, . ~ . - (1 | speaker) - 
    (1 | word) +
    (1 + gender + speaking_rate_mean_std + speaking_rate_dev || word) +
    (1 + speaking_rate_dev + foll_high_vowel + cons_cluster + stress +
        log_corpus_freq + place1 + place2 || speaker), 
  data = vot_voiced)
@

The model's output is shown in Appendix Section 9.3:
<<eval=FALSE>>=
summary(vot_voiced_m1, correlation = FALSE)
@


We now try adding in correlation terms holistically.  One way to do this, following Heuristics 9--10, is to consider terms in the following order:
\begin{itemize}
\item[(a)] Critical predictors: correlations with each random slope 
\item[(b)] Intrinsically-related predictors (here, \ttt{place}): correlations between related predictors
\item[(c)] Control predictors: a subset of correlations, suggested by pairwise
plots of estimated random effects
\end{itemize}

At each step, we use model comparison (likelihood-ratio test of REML fits, $p<0.2$) to decide whether to keep the correlation terms in the (convergent/non-singular) model.    We could alternatively not do hypothesis testing, and just keep the terms in the model at each step as long as the model converges and is non-singular. 

% - At each step, use LR test with $p<0.2$ to decide if keep in model, provided the model is non-singular and converges (Could alternatively just keep as long as converges/non-signular.)

Steps (a) and (b) are shown in Appendix Section 9.3.  Both improve the model, though for (b) the optimizer needs to be changed for convergence. The resulting model is:

%- Try adding correlations with intercept of speaking rate predictors:

<<eval=FALSE>>=
vot_voiced_m3 <- update(vot_voiced_m0, . ~ . - (1 | speaker) - 
    (1 | word) +
    (1 + speaking_rate_mean_std + speaking_rate_dev | word) +
    (0 + gender || word) + 
    (1 + speaking_rate_dev + place1 + place2 | speaker) +
    (0 + foll_high_vowel + cons_cluster + stress +
        log_corpus_freq || speaker),
  control = lmerControl(optimizer = "bobyqa"))
@

The model's output is:
<<eval=FALSE>>=
summary(vot_voiced_m3, correlation = FALSE)
@

% 
% 
% <<output.lines=18:23>>=
% anova(vot_voiced_m3, vot_voiced_m1, refit=FALSE)
% @

We could now do step (c): examining pairwise plots (e.g., \\ \ttt{pairscor.fnc(ranef(vot\_voiced\_m3)\$speaker)}) to see which correlations have the largest magnitudes, which we could consider adding.
%be added. The process is shown for this case in Appendix 9.3, resulting 
In this case, shown in Appendix Section 9.3, we end up trying correlations with the by-speaker random slopes for \ttt{cons\_cluster} and \ttt{log\_corpus\_freq} ($r$ around 0.5--0.6), but both give singular models.  So \ttt{vot\_voiced\_m3} is our final model.
%and for 
%but only the latter gives a valid model which improves significantly on M3:
% 
% <<eval=FALSE>>=
% ## vot_voiced_m3 with correlations with by-speaker 
% ## random slope of cons_cluster added in:
% vot_voiced_m5 <- update(vot_voiced_m0,  . ~ . -(1|speaker) - (1|word) +
%          (1+speaking_rate_mean_std + speaking_rate_dev|word) +
%          (0+gender||word) +
%          (1+speaking_rate_dev+place1+place2+cons_cluster|speaker)  + 
%            (0+foll_high_vowel + log_corpus_freq + stress||speaker), 
%          control=lmerControl(optimizer = 'bobyqa'))
% @
% 
% This model's output is:
% <<>>=
% summary(vot_voiced_m5, correlation=FALSE)
% @
% 
% It would also be fine to skip step (3), which is time-consuming, since with the model after steps (1)--(2)
% %the 
% %time-consuming step, since with the 
% %which is time-consuming, and  refitting takes forever, and with the
% %current model
% we are confident in estimates for our critical predictors.  Thus, our final model will be either \ttt{vot\_voiced\_m3} or \ttt{vot\_voiced\_m5}.
\paragraph{Results}

It is worth taking a step back to compare the two models above: the `maximal' no-correlations model, and the final  model with some correlations added.  Adding the correlation terms significantly improves the model:

<<output.lines=5:7>>=
anova(vot_voiced_m3, vot_voiced_m1, refit = FALSE)
@

However, the fixed-effect estimates for the  models, shown in Figure~\ref{fig:vot-coef-comp}, are very similar---all terms are qualitatively identical.

<<vot-coef-comp, echo=FALSE,out.width='75%', fig.width=default_fig.width*.75/default_out.width,  fig.cap='Fixed-effect coefficient estimates and 95\\% Wald confidence intervals  (omitting the intercept), from models \\ttt{vot\\_voiced\\_m1} and \\ttt{vot\\_voiced\\_m3} of the \\ttt{vot\\_voiced} data with all possible random slopes, differing in which random-effect correlations are included: no correlations in \\ttt{m1}, all correlations in \\ttt{m3}.'>>=
bind_rows(
  tidy(vot_voiced_m1, conf.int = TRUE) %>% filter(effect == "fixed") %>% select(-effect, -group) %>% add_column(model = "M1"),
  tidy(vot_voiced_m3, conf.int = TRUE) %>% filter(effect == "fixed") %>% select(-effect, -group) %>% add_column(model = "M3")
) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(x = term, y = estimate,  shape = model)) +
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high), position = position_dodge(width = 0.5), size=0.65) +
  geom_hline(aes(yintercept = 0), lty = 2) +
  coord_flip() + theme(axis.title.y = element_blank()) +
    labs(y="Coefficient estimate") +
  theme(legend.position = c(0.85, 0.85),
    legend.background = element_rect(fill="white", 
      size=0.4, linetype="solid")) 
@

This reiterates the point that we saw for simpler models in Section~\ref{sec:corr-vs-uncorr}, in a more realistic case: in practice, whether random-effect correlations are added rarely affects the qualitative conclusions with respect to fixed effects (assuming that predictors are all centered, etc.: Box~\ref{box:uncorr-caveat}).  This is a reasonable, if purely empirical, argument for using maximal uncorrelated random effects as your default.

In terms of the actual research question: the model suggests that VOT decreases with faster speech, within-speaker (\ttt{speaking\_rate\_dev}), but does not  
%effect (in contrast to $|t|$ near 2 for models of the \ttt{vot\_voiced\_core} subset).  VOT does not 
significantly decrease for faster speakers (\ttt{speaking\_rate\_mean\_std}).





%% FUTURE: anything from here to be incorporated into Section~\ref{sec:lmm-information-criteria}?  
% 
% 
% Continuation of Section~\ref{sec:lmm-information-criteria}:
% 
% - Introduce *cAIC*
% 
% - Details in Box, but: marginal AIC (mAIC) is what's calculated by default (AIC method), presumably since fixed effects of primary interest.  Computing cAIC is trickier; implementation in cAIC4 package. In general cAIC will select smaller
% 
% 
% Box:
% 
% - mAIC uses "marginal likelihood" = likelihood for an average cluster, which is default. df = number of fixed effects + variance components.   AFAIK this would also be what to use if *variance components* are of interest (not just fixed effects). Doesn't assume these particular clusters (=subjects, items) are of interest.
% 
% - cAIC uses conditional likelihood: evaluated for these particular cluster values (using BLUPs).  df for each ranef term between 1 and $k$ (\# levels)---similar to ABOVE.
% 
% - Discussion: Vaida \& Blanchard 2005, Greven \& Kneib 2010, Saefkin et al 2018.  This distinction matters if these measures are used for model selection.
% 


<<echo=FALSE, cache=FALSE>>=
opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE)
@

\section{Predictions and uncertainty for individual levels} %uncertainty in by-subject/item predictions}

\label{sec:by-sub-uncert}

% Instead of using a pre-existing method to calculate model predictions and uncertainties, we can do this ourselves, for any input dataframe. The advantage is that this method is very general and can be used to predict/get uncertainty for *any* quantity predicted by the model---not just those allowed by the pre-existing method.

In the remaining sections, we turn from model selection to additional `advanced' topics for mixed-effects models.

We saw in Section~\ref{sec:lmm-model-predictions}
how to predict different quantities from a fitted mixed-effects model, including predictions for each level (e.g.,\ by-participant/item), assuming we didn't want to include a measure of uncertainty (`confidence interval' or `prediction interval').
%-subject/item predictions -- assuming we didn't want to quantify uncertainty.

The more general case is complex, for the reason discussed there:  how to quantify uncertainty depends on  what sources of variability to capture, which depends on exactly what we are trying to predict (Section~\ref{sec:pred-val-uncert-lmm}).  Quantifying and communicating uncertainty from fitted models is an area where best practices and software are evolving.
%see XX for more.
%(: it depends what we're trying to predict, which dictates what  because we have to decide what sources of variability to include () it depends exactly what we're trying to predict -- depends exactly what we're trying to predict, which helps dictate what sources of variance to include. 

In this section we show some examples illustrating different ways to address the same research question: robustness of the \ttt{Voicing.vl} effect across speakers in the \ttt{turkish\_if0} data.  These are:
\begin{itemize}
\item Uncertainties on \textbf{predicted random effects} (each speaker's deviation)
\item Uncertainties on \textbf{model predictions} (each speaker's \tsc{voiceless}, \tsc{voiced} F0)
\item Uncertainties on \textbf{effects} (each speaker's \ttt{Voicing.vl} effect)
\end{itemize}

These examples introduce some general methods for quantifying uncertainty, which are progressively more general/accurate/slower:
\begin{itemize}
\item Using information from the fitted model (very fast)
%(Example 1)
\item Simulating from the fitted model (fast)
%(Examples 2--3)
\item Parametric bootstrapping from the fitted model (very slow)
\end{itemize}
%(Example 4)arametric bootstrap from fitted model to get "" 
 %(Ex 4)

\subsection{Example 1: Predicted random effects and uncertainties}

Section~\ref{sec:pred-rand-eff} introduced predicted random effects, a.k.a. BLUPs -- the deviations of each speaker (etc.) from the population effect.  These could be of interest for studying `individual differences', for example, where of interest is which speakers have higher and lower values.   The fitted model also contains a variance-covariance matrix for BLUPs (see \ttt{?ranef}); these can be used to define SEs, and hence define Wald 95\% confidence intervals.  BLUPs can be extracted using \ttt{tidy()} from {broom.mixed} (or \ttt{ranef()} from {lme4}):

<<output.lines=1:5>>=
##  Each speaker's BLUP for voicing term
tidy(if0_m2, effects = "ran_vals", conf.int = TRUE) %>%
  filter(group == "speaker" & term == "Voicing.vl") %>%
  rename(speaker = level) %>%
  select(-effect)
@

% 
% 
% - Box: box:blups: Fitted model also contains a var-cov matrix for BLUPs (see ?ranef); these can be used to define SEs, and hence define Wald 95\% CIs.  Done autoamtically by \ttt{tidy} in broom.mixed:

These are shown in Fig~\ref{fig:if0-blup-plots} (left). Note how large the CIs are: for most speakers, we cannot even be sure whether they are above or below the mean.

<<if0-blup-plots, echo=FALSE, message=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width,  fig.cap='BLUPs (with 95\\% CIs) for random intercept for each speaker (left) and consonant (right) for model \\ttt{if0\\_m2}.'>>=
spkrBlupDf <- tidy(if0_m2, effects = "ran_vals", conf.int = TRUE) %>%
  filter(group == "speaker" & term == "Voicing.vl") %>%
  rename(speaker = level) %>%
  select(-effect) 

spkrBlupDf %>% ggplot(aes(x = reorder(speaker, estimate))) +
  geom_pointrange(aes(y = estimate, ymin = conf.low, ymax = conf.high)) +
  xlab("Speaker") +
  ylab("Est. speaker Voicing\nrandom effect") +
  theme(
    axis.text.x = element_blank()
  )

tidy(if0_m2, effects = "ran_vals", conf.int = TRUE) %>%
  filter(group == "consonant") %>%
  ggplot(aes(x = reorder(level, estimate))) +
  geom_pointrange(aes(y = estimate, ymin = conf.low, ymax = conf.high)) +
  xlab("Consonant") +
  ylab("Est. consonant\nrandom intercept")
@

This is not unusual---errors in estimated random effects tend to be large, and should be accounted for in any subsequent analyses, such as predicting `individual differences' using other variables.\footnote{For example, for the \ttt{turkish\_if0} data, we could check whether the size of a speaker's \ttt{Voicing} effect is predicted by their `macro' prosody: their mean F0, pitch range, etc.  We would need to use a form of linear regression that allows for $y$ to have errors (e.g.,\ the \ttt{weights} argument of \ttt{lm}).}  

% The simplest option is to assume these errors are uncorrelated and use the \ttt{weights} their pitch range, speakers with larger effects a   \footnote{For example, for the \ttt{turkish\_if0} data, a separate analysis was done of vowel height effects on F0 (measured over the entire vowel: REF). We could check whether the size of speakers' vowel (height) and consonant (\ttt{Voicing}) effects on F0 are correlated, using a method where error in both $x$ and $y$ can be taken into account (Deming regression?)}
%be predicted by other variables.  

In that plot, each speaker shows similar uncertainty (CI width).  
%similar uncertainty for each speaker. 
This is only because speakers have similar sample sizes; in general, uncertainty will be proportional to the amount of data per level.  For example, consider the estimated random intercepts for each \ttt{consonant} (Fig~\ref{fig:if0-blup-plots} right), which is very unbalanced:

<<>>=
xtabs(~consonant, data = turkish_if0)
@

The confidence intervals are widest for level \tsc{tS}, with the fewest observations, and narrowest for level \tsc{b}, with the most observations.

\subsection{Example 2: Predicted values via simulation from fitted model}
\label{sec:ex-2-pred-val}

We saw in Section~\ref{sec:pred-vals-only} how to obtain predictions from a fitted model for any new observation, including for each level of a grouping factor.  For example, using the \ttt{neut\_mod\_2} model of the \ttt{neutralization} data, we predicted \ttt{vowel\_dur} for each \ttt{voicing} value for each participant, averaging across items:

<<eval=1>>=
neut_mod_2 <- lmer(vowel_dur ~ voicing + (1 + voicing || subject) +
    (1 | item_pair), data = neutralization)
ggpredict(neut_mod_2, terms = c("voicing", "subject"), type = "re")
@

These correspond to adding together fixed and random-effect estimates.
When predicting for a new \textbf{observation}, we generally want to incorporate uncertainty beyond just in the fixed effects,  to give `prediction intervals'. There are different ways to do this, which incorporate different sources of uncertainty, as discussed in Section~\ref{sec:pred-val-uncert-lmm}.

The simplest way was to simulate from the fitted model (using \ttt{simulate()})---this incorporates uncertainties in the fixed effects and the residual variance.  We first set up a prediction dataframe corresponding to both \ttt{voicing} values for every \ttt{subject}:
%now using a prediction dataframe- Simplest way: simulate from the model using \ttt{simulate}, introduced in sec:conf-pred-int, now setting up the prediction dataframe to get values for each subject.


<<>>=
pred_df <- tidyr::expand(neutralization, voicing, subject)
## Makes predictions using population-level pred for item_pair
pred_df$item_pair <- "XX"
@

The simulation is otherwise similar to the example in Section~\ref{sec:pred-val-uncert-lmm}:

% FUTURE: can refactor this code, used at least twice
<<>>=
sims <- simulate(neut_mod_2,
  newdata = pred_df,
  re.form = ~ ((1 | subject) + (0 + voicing | subject)),
  allow.new.levels = TRUE, nsim = 1000
)

quantile_fun <- function(x) {quantile(x, c(0.025, 0.5, 0.975))}
## get prediction (= median) + 95% PIs
simPredsDf <- t(apply(sims, 1, quantile_fun))
colnames(simPredsDf) <- c("lower", "pred", "upper")

pred_df <- cbind(pred_df, simPredsDf) %>%
  mutate(voicing = factor(voicing, labels = c("voiced", "voiceless")))
@

The arguments to \ttt{simulate()} specify that $n=1000$ new values should be simulated from the model conditional on the current random-effect estimates (BLUPs) for by-speaker terms, but assuming an average item (\ttt{item\_pair = XX}, \ttt{allow.new.levels = TRUE}).

These predictions and 95\% PIs are shown in Figure~\ref{fig:neut-mod2-preds}. The uncertainty on each prediction is large, reflecting (primarily) the large residual variance.  Comparing to Figure~\ref{fig:neut-3-preds}, the version without uncertainties, we get a different picture: by-participant variability in both overall \ttt{vowel\_dur} and the \ttt{voicing} effect are tiny compared to the overall degree of variability in the data.  Speakers are internally rather consistent.
%the PI takes into account uncertainty in fixed effects and the residual variance (but not the uncertainty in the BLUP itself).
% FUTURE: I think that's right?

<<neut-mod2-preds, echo=FALSE,  fig.asp=0.7, out.width='75%', fig.width=default_fig.width*.75/default_out.width, fig.cap='By-participant predictions for \\ttt{vowel\\_dur} as a function of \\ttt{voicing}, for \\ttt{neut\\_mod2}, with 95\\% prediction intervals.'>>=
pred_df %>% ggplot(aes(x = reorder(factor(subject), pred), y = pred)) +
  geom_errorbar(aes(group = voicing, ymin = lower, ymax = upper, color = voicing), position = position_dodge2(width = 1)) +
  geom_point(aes(group = voicing, shape = voicing, color=voicing), size=2,  position = position_dodge2(width = 1)) +
  xlab("Participant") +
  ylab("Pred. vowel duration") + 
  theme(legend.position = 'bottom')
@

% - See that the PI on each prediction is large.  Reflects it taking into account uncertainty in fixed effects *+ residual variance*.  (*Not* uncertainty in BLUP itself, I think.)

\subsection{Example 3: Predicted effects via simulation from the fitted model}

Often, what we want to extract from the model are not individual predictions, but \textbf{effects}, such as ``the \ttt{Voicing.vl} effect for each speaker.''
%- Often, what we want to extract from the model is not a single prediction, but effects, such as ``the Voicing.vl effect for each speaker''.
%
This can be done using \ttt{coefficients()}:

<<eval=FALSE>>=
coefficients(if0_m1)$speaker$Voicing.vl
@

Figure~\ref{fig:if0-voicing-effects} (top left) shows a histogram of these values, which is one way to address the research question of by-speaker robustness. Most speakers have a positive \ttt{Voicing.effect}, which we can quantify as this percentage:

<<>>=
# Percentage of speakers with positive Voicing.vl effects
sum(coefficients(if0_m1)$speaker$Voicing.vl > 0) / 
  nrow(coefficients(if0_m1)$speaker)
@

We would also like a way to take uncertainties in the effects into account here. One way to do this, which works very generally, is to state `effects' in terms of model predictions and use these to calculate estimates and uncertainties.  For example, the output of \ttt{simulate()} above could be used to calculate $n$ \tsc{voiceless}-\tsc{voiced} values for each speaker, and used to define a 95\% PI.
% 
% - But in this kind of setting, we really should take *uncertainties* into account: we saw that each speaker has huge errorbars. How should we do this?
% 
% - It's always possible to state things in terms of predictions, and calculate ``effects'' + errors that way. For example, could use sim results above to compute the effect of `voicing' for each speaker, iteration, get 95\% PIs.

It is often simpler to state `effects' in terms of model parameters, which have already been defined to be quantities of interest. For example, in the Turkish IF0 model, we can address the research question by estimating each speaker's \ttt{Voicing.vl} effect---which is already defined to average across different \ttt{base\_vowel} levels, etc.---with associated uncertainties.

% - But often simpler to state effects in terms of *model parameters*, which have already been defined to be quanties of interest. For example, in the Turkish IF0 model, to address the RQ about individual differences, we would like to estimate each speaker's Voicing.vl effect -- which is already defined to average across diff basevowel levels, etc -- with associated uncertainty. 

The easiest way to do this is to simulate new parameter values from the fitted model, giving $n$ simulated values of the fixed-effect coefficient value for \ttt{Voiced.vl}, $n$ values of Speaker 1's \ttt{Voiced.vl} random slope (which are added to give $n$ values of Speaker 1's \ttt{Voiced.vl} effect), etc.\footnote{See \citet{glmmfaq} (`Confidence intervals on conditional means/BLUPs/random effects') for an even simpler method, which does not require simulation---just adding together the uncertainties for the fixed effect and BLUPs. This requires assuming their estimates are independent, which is not in general true.}   The \ttt{sim()} function in the {arm} package is one implementation of this, which takes into account uncertainty in (a) the fixed-effect coefficients, (b) BLUPs (as in Example 1), and (c) the residual variance; the random-effect variance components are held at their estimated values.\footnote{Technically, \ttt{sim()} does this by treating the fitted model as quasi-Bayesian: it uses the model's likelihood for (a)--(c), assumes  a flat prior for each model parameter, and samples from the resulting posterior.  This means that the fixed effects, random effects, and residual variance are sampled from distributions centered at their estimates in the fitted model.  See \citet[][\S7.2, 18.3--18.4]{gelman2007data} and \ttt{simmer.R} in the {arm} source code.}%\footnote{At least, I think this is what it does. The implementation is \ttt{simmer.R} in the \ttt{arm} source code, following \citet{gelman2007data}.}

% - The simplest method here is to simulate new *parameter values* from the fitted model:   Intuitively: uses uncertainty in fixed-effect coeffs, BLUPs (as in Ex 1 above), and the residual variance; holding random-effect *variances* at estimated values.   Get 1000 simulated values of fixed effect coeff for Voiced.vl, 1000 for spekaer 1's ranef for Voiced.vl, etc.


% - Technically: this is done by thinking of the fitted model as quasi-Bayesian: likelihood for the fixed-effect coeffs, random effects, residual variance; flat prior for each parameter. Simualte from the posterior, assuming independence (I think)---simmer.R, from arm.

% - Implementation: sim in arm.

\ttt{sim()} does not work easily for models with uncorrelated random-effect terms.  Instead we use model \ttt{if0\_m1}, where the random effects for each grouping factor are correlated.  We first draw $n=1000$ values of the \ttt{Voicing.vl} BLUP for each speaker, and of the \ttt{Voicing.vl} fixed-effect coefficient:

<<>>=
# Simulate all model parameters
if0_m1_sim <- sim(if0_m1, n.sims = 1000)
## Extract just those we need
voicingRanefDraws <- ranef(if0_m1_sim)$speaker[, , "Voicing.vl"]
voicingFixefDraws <- fixef(if0_m1_sim)[, "Voicing.vl"]
@

We add these together for each speaker to get $n$ values of the \ttt{Voicing.vl} coefficient, which are then used to define by-speaker estimates (medians) and 95\% PIs:
<<echo=1:4>>=
voicingCoefDraws <- apply(voicingRanefDraws, 2, function(x) {
  x + voicingFixefDraws
})

voicingCoefPreds <- t(apply(voicingCoefDraws, 2, quantile_fun))
colnames(voicingCoefPreds) <- c("lower", "pred", "upper")

voicingCoefPreds <- data.frame(voicingCoefPreds) %>% 
  rownames_to_column("speaker")
@


These are shown in Figure~\ref{fig:if0-voicing-pred-intervals} (left).  The PIs are large, reflecting the several sources of uncertainty taken into account; note that the PIs overlap zero for most speakers.  We can now calculate the same percentage measure as above (of speakers with positive \ttt{Voicing.vl} effects), but {with a prediction interval}:

<<echo=FALSE>>=
library(parallel)
nc <- detectCores()
fName <- "if0_m1_boot.rds"
fPath <- paste("objects/", fName, sep = "")

do_comp <- !file.exists(fPath)

if (do_comp) {
  ## bootMer requires X as a function to be computed using fitted model
  mySumm <- function(mod) {coefficients(mod)$speaker$Voicing.vl}
  
  if0_m1_boot <- bootMer(if0_m1, mySumm, nsim = 100, use.u = TRUE, parallel = "multicore", ncpus = nc)
  
  saveRDS(if0_m1_boot, file = fPath)
} else {
  if0_m1_boot <- readRDS(fPath)
}

bootPredDf <- data.frame(
  speaker = rownames(coefficients(if0_m1)$speaker),
  pred = apply(if0_m1_boot$t, 2, function(x) {
    quantile(x, 0.5)
  }),
  lower = apply(if0_m1_boot$t, 2, function(x) {
    quantile(x, 0.025)
  }),
  upper = apply(if0_m1_boot$t, 2, function(x) {
    quantile(x, 0.975)
  })
)
@

<<if0-voicing-effects, echo=FALSE, message=FALSE, fig.asp=0.8, out.width='90%', fig.width=default_fig.width*.90/default_out.width,  fig.cap='Top: histograms of by-speaker \\ttt{Voicing.vl} effects estimated by three methods, for model \\ttt{if0\\_m1}: fitted model values (BLUPs: top left), simulation from the fitted model (top center), and parametric bootstrapping (top right). Bottom: by-speaker \\ttt{Voicing.vl} effects with 95\\% PIs, estimated by two methods.'>>=
p1 <- data.frame(diff = coefficients(if0_m1)$speaker$Voicing.vl) %>% ggplot(aes(x = diff)) +
  geom_histogram() +
  xlab("Pred. Voicing effect (st)") +
  ggtitle("BLUPs") +
  xlim(-0.5, 1.6)

p2 <- voicingCoefPreds %>% ggplot(aes(x = pred)) +
  geom_histogram() +
  xlab("Pred. Voicing effect (st)") +
  ggtitle("Simulation") +
  xlim(-0.5, 1.6)

p3 <- bootPredDf %>% ggplot(aes(x = pred)) +
  geom_histogram() +
  xlab("Pred. Voicing effect (st)") +
  ggtitle("Param. bootstrap") +
  xlim(-0.5, 1.6)

p4 <- voicingCoefPreds %>% ggplot(aes(x = reorder(speaker, pred), y = pred)) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  xlab("Speaker") + 
  ylab("Pred. Voicing effect (st)") +
  ggtitle("Simulation") +
  theme(axis.text.x = element_blank())

p5 <- bootPredDf %>% ggplot(aes(x = reorder(speaker, pred), y = pred)) +
  geom_pointrange(aes(ymin = lower, ymax = upper)) +
  xlab("Speaker") +
  ylab("Pred. Voicing effect (st)") +
  ggtitle("Param. bootstrap") +
  theme(
    axis.text.x = element_blank()
  )

(p1 | p2 | p3) / (p4 | p5)
@


% <<if0-voicing-pred-intervals, echo=FALSE, message=FALSE,   out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='By-speaker \\ttt{Voicing.vl} effects with 95\\% PIs, estimated by two methods, for model \\ttt{if0\\_m1}.'>>=
% voicingCoefPreds %>% ggplot(aes(x = reorder(speaker, pred), y = pred)) +
%   geom_pointrange(aes(ymin = lower, ymax = upper)) +
%   xlab("Speaker") + 
%   ylab("Pred. Voicing effect (st)") +
%   ggtitle("Simulation from fitted model") +
%   theme(axis.text.x = element_blank())
% 
% bootPredDf %>% ggplot(aes(x = reorder(speaker, pred), y = pred)) +
%   geom_pointrange(aes(ymin = lower, ymax = upper)) +
%   xlab("Speaker") +
%   ylab("Pred. Voicing effect (st)") +
%   ggtitle("Parametric bootstrap") +
%   theme(
%     axis.text.x = element_blank()
%   )
% @
% 

<<>>=
# Calculate the % speakers with pos. Voicing.vl effect for each draw
posVlPercVec <- apply(voicingCoefDraws, 1, function(x) {
  sum(x > 0) / length(x)
})

## Calculate median + 95% PI of percentages
quantile(posVlPercVec, c(0.025, 0.5, 0.975))
@

% The by-speaker estimates are plotted as a histogram in  Figure~\ref{fig:if0-voicing-effects} (center), which looks similar to the by-speaker estimates extracted from the fitted model (left).

A histogram of the by-speaker estimates using the simulation method (Figure~\ref{fig:if0-voicing-effects}: top center) looks very similar to the histogram of by-speaker estimates using BLUPs (top left), but with the simulation method we have a better sense of what the model can and cannot tell us about by-speaker variability in the \ttt{Voicing.vl} effect (the research question). For most \textbf{individual} speakers, we can't be confident the effect is positive, but we can be confident that a large majority of speakers have a positive effect (95\% PI: \Sexpr{quantile(posVlPercVec, 0.025)}--\Sexpr{quantile(posVlPercVec, 0.975)}).

%But the model is confident that 
% 
% Histogram of predicted effects looks v similar to BLUPs: but now we get a better sense of what the model can and can't tell us about indiv variabiltiy.  For most *individual* speakers, can't be sure effect is larger than zero.  But model is sure that a majority of speakers have a positive effect: 77--97\%.


\subsection{Example 4: Predicted effects via parametric bootstrap}
\label{sec:parametric-bootstrap-example}

The gold standard for estimating any quantity $X$ from a fitted model, with uncertainty, is parametric bootstrapping. The \ttt{bootMer()} implementation repeatedly ($n$ times) does the following: simulates a new dataset from the model (using \ttt{simulate.merMod()}), refits the model, and calculates $X$. The resulting distribution of $X$ ($n$ values) can be used to calculate an estimate and prediction interval.  This method takes all sources of uncertainty into account, and is more accurate than other methods, but is very computationally intensive.
%(because the model is refitted $n$ times).

As an example, we use parametric bootstrapping to recalculate an estimate and 95\% PI for the \ttt{Voicing.vl} effect for each speaker. This is now $X$, calculated by this function:

<<>>=
## bootMer requires X as a function to be computed using fitted model
mySumm <- function(mod) {coefficients(mod)$speaker$Voicing.vl}
@

To perform the parametric bootstrap, with $n=100$:\footnote{This $n$ is low if you are actually reporting these results;  at least 500--1000 would be better.}

<<eval=FALSE>>=
## This takes ~10 min on my laptop.  Change ncpus for your computer.
## If you run this code, make sure to save the output --
## See chap. 9 ``Saving/Loading models'' box
if0_m1_boot <- bootMer(if0_m1, mySumm, nsim = 100, use.u = TRUE,
  parallel = "multicore", ncpus = 8)
@

The \ttt{use.u = TRUE} option makes the simulated datasets conditional on  the BLUPs for the fitted model---for example, the \ttt{Voicing.vl} effect for speaker 1 in the simulated datasets will be a distribution centered around the value from \ttt{coefficients(if0\_m1)}, rather than simulating a new `speaker 1' value centered on the fixed effect of \ttt{Voicing.vl} for each dataset.  This is necessary for us to obtain uncertainties for the \ttt{Voicing.vl} effects for \textbf{these speakers}, and parallels what \ttt{sim()} does.
%\footnote{Alt we wanted to  make the simulated datasets conditional on \ttt{speaker} random effects, but not \ttt{word} or \ttt{consonant} random effects (hence increasing the uncertainty for each speaker's predicted \ttt{Voicing.vl} effect), we would use \ttt{re.form =[speaker random effects only]}, as in the \ttt{simulate} example above. There are a lot of choice points.}

The raw values are stored in \ttt{t}:

<<output.lines=1:3>>=
## Voicing.vl coeffs for speakers 1-8 for iterations 1-2
if0_m1_boot$t
@

These can be used to calculate estimates and 95\% PIs for each speaker, as above:

% FUTURE: redo this refactoring code... already did something similar
% previously but with diff code. Note that this bit also in a hidden
% chunk
<<output.lines=1:3>>=
bootPredDf <- data.frame(
  speaker = rownames(coefficients(if0_m1)$speaker),
  pred = apply(if0_m1_boot$t, 2, function(x) {
    quantile(x, 0.5)
  }),
  lower = apply(if0_m1_boot$t, 2, function(x) {
    quantile(x, 0.025)
  }),
  upper = apply(if0_m1_boot$t, 2, function(x) {
    quantile(x, 0.975)
  })
)
@

The by-speaker estimates are shown in a histogram in Figure~\ref{fig:if0-voicing-effects} (top right), and with 95\% PIs in the bottom right panel.  We can again calculate the percentage of speakers with a positive \ttt{Voicing.vl} effect, with 95\% PI:
%- Plot by-speaker, as above: Figure~\ref{fig:if0-voicing-effects} (right)

% <<>>=
% # bootPredDf %>% ggplot(aes(x=reorder(speaker,pred), y=pred)) + geom_pointrange(aes(ymin=lower, ymax=upper)) + xlab("Speaker") + ylab("Predicted Voicing effect (semitones)")
% #bootPredDf %>% ggplot(aes(x=pred)) + geom_histogram()
% @

% Calculate the \% speakers with a positive effect, with PI:

<<>>=
posVlPercVec2 <- apply(if0_m1_boot$t, 1, function(x) {
  sum(x > 0) / length(x)
})

quantile(posVlPercVec2, c(0.025, 0.5, 0.975))
@

With respect to the research question: we would conclude that almost all speakers have a positive \ttt{Voicing.vl} effect (PI: \Sexpr{quantile(posVlPercVec2, 0.025)}--\Sexpr{quantile(posVlPercVec2, 0.975)}), and most \textbf{individual} speakers have a clearly-positive effect (Figure~\ref{fig:if0-voicing-pred-intervals}: right).  As seen in the histogram, the basic picture is that the \ttt{Voicing.vl} effect is robust across speakers, but varies greatly in magnitude. This is a stronger conclusion than we made from the simulation method.

Thus, in this case, a more accurate method of quantifying uncertainty (parametric bootstrapping) gives a stronger conclusion with respect to the research question: robustness of the \ttt{Voicing.vl} effect across speakers.\footnote{The simulation method seems to be overestimating the amount of uncertainty, and the degree of difference between speakers. This may be because there are correlations between different kinds of uncertainty which \ttt{sim} doesn't account for in simulating from the posterior.}
% 
% 
% 
% Comparing what this method says about the (model's predictions w.r.t. the) research question to the simulation-based method is interesting. Using parametric bootstrapping: 
% 
% - Comparison to simulation-based method is interesting: from PB, model is more sure that almost all speakers have a positive Voicing effect (CI: 90-100\%), and most *individual* speakers have a clearly-positive Voicing effect.  As seen in histogram, the effect is robust across speakers, but varies greatly in magnitude.  

% In this case, using a more accurate method (PB) gives a stronge conclusion w.r.t the RQ, about robustness of the effect.

% (Footnote: The simulation method seems to be overestimating the amount of uncertainty, and the degree of difference between speakers. This may be because there are correlations between different kids of uncertainty whcih it doesn't account for.)


% 
% Example: getting prediction intervals for by-subject predictions.
% 
% - Simplest method is simulation from the fitted model using \ttt{simulate} (\ttt{simualte.merMod} from {lme4})
% 
% - predictions made for the first neutralization model, for new data \ttt{pred\_df} (repeated for convenience):
% 
% <<>>=
% pred_df <- expand(neutralization_2, voicing_fact, item_pair)
% @
% 
% - To add predictions and prediction intervals from simulation (adapt 7.12.1 text):
% 
% <<>>=
% sims <- simulate(n_sub_mod1, newdata=pred_df, nsim=10000, re.form=NULL)
% 
% pred_df$lower <- apply(sims, 1, function(x){quantile(x,0.025)})
% pred_df$upper <- apply(sims, 1, function(x){quantile(x,0.975)})
% pred_df$pred <- apply(sims, 1, function(x){quantile(x,0.5)})
% @
% 
% - The \ttt{re.form} argument makes predictions conditional on the current random-effect estimates (BLUPs) -- rather than generating new values, which is what we'd want if doing predictions just using fixed effects.
% 
% - Could use these to add CIs to Fig XX, or to visualize by-item variability another way:
% 
% <<>>=
% ggplot(aes(x=item_pair, y=pred, group=voicing_fact), data=pred_df) + geom_errorbar(aes(ymin=lower, ymax=upper, color=voicing_fact), position='dodge2')
% @
% 
% - See that CI on each item is large.
% 
% - To instead add *confidence* intervals would need to use bootMer.  (see ciTools implementation).  So this example can show both simulation-based methods: simulate, bootMer (slower).




 % NOTE TO SELF (1/29/21): got into huge rabbit hole here on all the different ways to calculate uncertainties, choices to make when using bootMer, predictInterval, simulate (= simulate.merMod).  I think the upshot is: for a non-standard case (not just prediction based on fixed effects), uncertainty to compute will depend on what makes sense.
%  
% Examples: 
% - showing estimated intercept for each language, with language = random effect: the actual levels are of interest. woudl want to show *confidence* intervals; could compute with bootMer.
% 
% - visualizing by-subject variability in slope of X predicted by the model: simulate new subjects, effects of X
% 
% 
%% FUTURE: 
% - REturn to that rabbit hole on *simulation* to understand the fitted model: see note near the end of chap. 9.  Can discuss predictive simulation, merMod functionality (or just allude to it).  The core idea of all this is -- simulation using the fitted model is the ultimate, most general way to do everything -- also hardest, must make some choices.
%
% - somehow, should say that for this kind of simulation-based inference, it quickly makes sense to  go bayesian.


%% FUTURE: go into PPD / model checking via predictive simulation, at all? GH chap. 24
% Extra diagnostic/model evaluation stuff:
% 
% - Posterior predictive distribution?
% 
% (like quantile regression, bootmer, sim, this gets at using the *probability distribution* implied by the fitted model)

%% FUTURE: just put this (missing data/imputation) in the stuff-we-didn't cover chapter. I tried for a day to make a good example here, but am too uncertain about the results, and what is better than the `complete cases' approach (discard all non-complete data). Got weird results for an LMER model of CDI data.
% 
% \section{Missing data / imputation}
% 
% - Very brief intro here to a topic which is important, and complex, but is barely used in linguistic data analysis currently.
% 
% - Source: Van Buuren 2018 book.
% 
% - Often, data is incomplete: measures of DV or predictors missing for some participants, or values nonsensical.  Have to decide what to do in these instances---all standard stat methods we've covered assume *complete* data.  Have to choose some way to fit model to data, anyway; will run the usual risks (Type I/II/M/S, error), depending on what *true* values are.  (Give intutive example)
% 
% - Most common practice is to simply discard incomplete observations: ``listwise deletion''.   This is one possible method, default in many R functions.  It turns out to usually not be a good one. In the best case, where data is missing completely at random, get unbiased estimates, etc., but power is reduced.  (But if the percentage of missing data is small, then it may be OK.)
% 
% - If the data is *not* missing at random, can severely bias estimates of means, regression coefficients, etc. 
% 
% - Better to somehow replace missing values---`imputation'. Many possible algorithms implemented in \ttt{mice} package. The risks depend on how random the missing data can be assumed to be, but in general, using a good, fairly standard imputational algorithm is a better idea than pairwise deletion.   %missing values, based on information from observed values: `multiple imputation'
% 
% %If data is missing at random, you lose power, relative to very simple alternatives, such as just predicting missing values from non-missing ones (at simplest: just use averages!). Such methods are called ``imputation'' for missing values.  If the data is *not* missing at random, obviously the inference from the remaining data will be biased.
% 
% %- In short, it is usually better to use *some* imputation method.  Various methods available in \ttt{mice} package.
% 
% - Simplest method is to make an educated guess by averaging over similar cases: `mean'.
% 
% - For example, for turkish if0 data, 123 rows missing `localf0'.  Could just guess the average of that speakers `localf0 (Exercise FUTURE).  
% 
% 
% - As a more realistic example, consider the French CDI model from the last chapter.  Almost half the data was discarded as missing some predictor values: frequency, MLU, word length.
% 
% 
% <<>>=
% library(mice)
% 
% french_cdi_24 <- read.csv("data/wordbank_fr.csv")
% french_cdi_24 <- french_cdi_24 %>%
%   filter(lexical_class!='other') %>%
%   mutate(child = as.factor(data_id)) %>%
%   droplevels() %>% tibble()
% 
% ## set lexical_class to intuitive order
% french_cdi_24 <- french_cdi_24  %>% mutate(lexical_class=fct_relevel(lexical_class, 'function_words',
%                                    'verbs', 'adjectives', 'nouns'))
% 
% french_cdi_24_sub <- french_cdi_24 %>% select(produces, num_phons, MLU, frequency, lexical_class, sex, definition, child)
% 
% ## just impute from means: the dumbest possible method
% ## see mice vignette
% ## just using this because I don't understand more 
% ## complex methods
% imp <- mice(french_cdi_24_sub, method = "mean", m = 1, maxit = 1)
% 
% french_cdi_24_complete <- complete(imp)
% 
% french_cdi_24_complete <- french_cdi_24_complete %>% 
%   mutate(freq.std = arm::rescale(frequency), 
%          log_nphones.std = arm::rescale(log(num_phons)),
%          MLU.std = arm::rescale(MLU))
% contrasts(french_cdi_24_complete$lexical_class) <- contr.helmert(4)
% french_cdi_24_complete$sex <- as.factor(french_cdi_24_complete$sex)
% contrasts(french_cdi_24_complete$sex)<- contr.helmert(2)
% 
% # cdi_mod_mice <- update(cdi_mod, data=french_cdi_24_complete)
% @
% 


\section{Nonlinear effects}

\label{sec:nonlin-mems}

 In Section~\ref{sec:melr-nonlin} we introduced nonlinear effects in mixed models, in an example of a  model of the \ttt{french\_cdi} data with a nonlinear \textbf{fixed} effect. Here, we show a more realistic case: fitting a mixed-effects model with random slopes for nonlinear terms, and making model predictions. Both are complicated by similar issues to random slopes for factors. 

%- Saw mixed-effects model with nonlinear fixed effect in XX (CDI example)
%
%- Example of mixed-effects model with nonlinear effects in random *slopes*, making model predictions. Both complicated by similar issues to factors.

First, let's refit the random intercepts-only model for the \ttt{vot\_voicing} data from Section~\ref{sec:ms-case-study-3}, but now with a nonlinear effect of \ttt{speaking\_rate\_dev}:

<<>>=
vot_voiced_m0_ns2 <- lmer(log_vot ~ ns(speaking_rate_dev, 2) +
    foll_high_vowel + cons_cluster + stress + log_corpus_freq +
    place + gender + speaking_rate_mean_std + (1 | word) + 
    (1 | speaker), data = vot_voiced)
@

This could be a reasonable `maximal' fixed-effect structure to consider,  as the speaking rate effect is of primary interest, if there is some reason to suspect a nonlinear effect. In this case, there isn't really---the model doesn't improve on \ttt{vot\_voiced\_m0} by AIC, and the empirical effect of speaking rate looks roughly linear---but we use this model anyway as an example.

Minimally, this model would need to include random slopes for the speaking rate terms of primary interest:

<<warning=TRUE, message=TRUE>>=
## Doesn't converge unless optimizer changed
vot_voiced_m1_ns2 <- update(vot_voiced_m0_ns2, . ~ .- (1 | speaker) +
    (1 + ns(speaking_rate_dev, 2) | speaker) -
    (1 | word) + (1 + speaking_rate_mean_std | word),
  control = lmerControl(optimizer = "bobyqa")
)
@

This model is singular, from a perfect correlation in the random effects:
<<>>=
summary(vot_voiced_m1_ns2)$varcor
@

This often happens with nonlinear terms in the random slopes, especially since the \ttt{ns} spline basis functions (= the two \ttt{speaking\_rate\_dev} predictors are not centered), but there is no simple way to standardize the predictors in this case.

To fit a model with \textbf{uncorrelated} random effects, we would need to  extract individual components of the \ttt{ns} term as numeric variables,  analogously to extracting the contrasts as numeric variables for a factor to use uncorrelated random effects:

<<>>=
mm <- model.matrix(~ ns(speaking_rate_dev, 2), vot_voiced)
vot_voiced$speaking_rate_dev1 <- mm[, 2]
vot_voiced$speaking_rate_dev2 <- mm[, 3]
@

We can then fit the model without random-effect correlations:

<<>>=
vot_voiced_m1_ns2_uncorr <- update(vot_voiced_m1_ns2, . ~ . -
  (1 + ns(speaking_rate_dev, 2) | speaker) +
  (1 + speaking_rate_dev1 + speaking_rate_dev2 || speaker),
data = vot_voiced)
@

It is not clear to me how legitimate this is, given that spline components are not in general centered and orthogonal, and it doesnt make conceptual sense to use uncorrelated random effects for predictors which are {known} to be correlated. But at least in this case, the models give extremely similar results, so we will examine the results of each model:

<<>>=
## Correlation between fitted values from both models
cor(predict(vot_voiced_m1_ns2_uncorr), predict(vot_voiced_m1_ns2))
@

Understanding the nonlinear effect requires model predictions, as the coefficients are uninterpretable (Section~\ref{sec:nonlin-interp-reporting}).  We would like predictions as speaker and \ttt{speaking\_rate\_dev} are varied, for an average word. We also vary the factor \ttt{place}, which we'll marginalize over later. First, set up the dataframe:

<<>>=
## Varying predictors + speaker
pred_df <- expand.grid(
  speaker = unique(vot_voiced$speaker),
  speaking_rate_dev = seq(-2, 2, 0.1),
  voicing = unique(vot_voiced$voicing),
  place = unique(vot_voiced$place)
)

## Set other predictors to mean values
pred_df <- mutate(pred_df,
  cons_cluster = 0, foll_high_vowel = 0, stress = 0,
  log_corpus_freq = 0, gender = 0, speaking_rate_mean_std = 0
)
@

We obtain predicted values with \ttt{predict()}, using the fixed effects and only the by-speaker random effects, and marginalize over \ttt{place}:
<<>>=
pred_df$pred <- predict(vot_voiced_m1_ns2,
  newdata = pred_df,
  re.form = ~ (1 + ns(speaking_rate_dev, 2) | speaker)
)

## Average across levels of place
pred_df_corr <- pred_df %>%
  group_by(speaker, speaking_rate_dev) %>%
  summarise(pred_marg = mean(pred))
@
These predictions are shown in  Figure~\ref{fig:vot-nonlin-preds} (left).  

To do the same for the model with uncorrelated random effects, we need to manually fill in the values of the spline components in the prediction dataframe:
<<>>=
## Make splines for the *full* dataset:
sr_splines <- ns(vot_voiced$speaking_rate_dev, 2)

## Get values of two components for pred_df values of speaking_rate_dev:
sr_splines_pred <- predict(sr_splines, pred_df$speaking_rate_dev)
pred_df$speaking_rate_dev1 <- sr_splines_pred[,1]
pred_df$speaking_rate_dev2 <- sr_splines_pred[,2]
@

We then obtain predictions (and marginalize) as before; these are shown in Figure~\ref{fig:vot-nonlin-preds} (right).

<<>>=
pred_df$pred <- predict(vot_voiced_m1_ns2_uncorr,
  newdata = pred_df,
  re.form = ~ (1 + speaking_rate_dev1 + speaking_rate_dev2 || speaker)
)

pred_df_uncorr <- pred_df %>%
  group_by(speaker, speaking_rate_dev) %>%
  summarise(pred_marg = mean(pred))
@


<<vot-nonlin-preds, echo=FALSE,message=FALSE, fig.asp=0.6, out.width='65%', fig.width=default_fig.width*.65/default_out.width, fig.cap='Predicted effect of \\ttt{speaking\\_rate\\_dev}, by-speaker, for models \\ttt{vot\\_voiced\\_m1\\_ns2} and \\ttt{vot\\_voiced\\_m1\\_ns2\\_uncorr}, which have correlated and uncorrelated random effects.'>>=
bind_rows(
  data.frame(pred_df_corr, correlation='correlated REs'),
  data.frame(pred_df_uncorr, correlation='uncorrelated REs')
) %>% ggplot(aes(x = speaking_rate_dev, y = pred_marg)) +
  geom_line(aes(group = speaker, color = speaker)) +
  facet_wrap(~correlation) + 
  xlab("Speaking rate deviation") +
  ylab("Predicted log VOT") +
  scale_color_grey() +
  theme(legend.position = "none")
  
# 
# pred_df_corr %>% ggplot(aes(x = speaking_rate_dev, y = pred_marg)) +
#   geom_line(aes(group = speaker, color = speaker)) +
#   xlab("Speaking rate deviation") +
#   ylab("log VOT") +
#   ggtitle("Correlated random effects") +
#   theme(legend.position = "none") +
#   ylim(1.9, 3)
# 
# pred_df_uncorr %>% ggplot(aes(x = speaking_rate_dev, y = pred_marg)) +
#   geom_line(aes(group = speaker, color = speaker)) +
#   xlab("Speaking rate deviation") +
#   ylab("log VOT") +
#   ggtitle("Uncorrelated random effects") +
#   theme(legend.position = "none") +
#   ylim(2, 3)
@

%- Preds from both models: Figure~\ref{fig:vot-nonlin-preds} 

The predictions from the two models are qualitatively similar, but with more curvature in the model without random-effect correlations.  Presumably if we added 95\% PIs, which could be done using \ttt{simulate} rather than \ttt{predict} (as in Section~\ref{sec:ex-2-pred-val}), we would find that differences between the two models lie within the PI.
%- Qualitatively similar, but more curvature in model without ranef correlations. Presumably if we added PIs (which could be done as in XX above), we'd find that diff between two models lies within PI.

% - Will already have done one example in Ch 9; do another here showing adding random slopes, and by-speaker predictions (for a nonlinear effect) : see week 12-13 extra, already worked it out.

\section{Power for mixed-effects models}
\label{sec:power-mixed-effects}

How to calculate and think about power is a very important topic for mixed-effects models, but enough of an active research area that it makes more sense to refer you to recent articles than to offer a treatment which will be dated by the time you read this.\footnote{I agree with a reviewer that power ``for most of our work in linguistics is rather central (or should be) and ought to be presented as such''---this is just not the place for that presentation.} 
 My thoughts as of a couple years ago are in \citet{kirby2018mixed}, a case study illustrating design analysis for the \ttt{neutralization} data, using power (and effect size) calculations for mixed-effects models of this data.

%- Things that are important in practice, but out of our scope, or we don't have space.
%
%1. Power
The high-level considerations are similar to non-mixed-effects models, introduced in Chapter~\ref{chap:es-power}: power is a function of sample size, degree of variability, true effect size, and analysis method (including significance level); and is relevant for both study design and for interpreting fitted models (`design analysis').  Power calculations are more complex for mixed-effects models, because there are now several `sample sizes' and `degrees of variability' (the number of observations, participants, etc.; the residual variance, by-participant random intercept variance, etc.), each of which affects power. 
% 
%  \item the sample size
%  \item the true effect size
%  \item the amount of variability in the data
%  \item the significance level ($\alpha$).  
% 
% - Similar considerations to chap. 3 -- but more complex for mixed-effects models, because there are now several ``sample sizes'' and ``variances'' which can affect power.
% 
% - Can calculate either for study planning, or design analysis.

For simple cases it is possible to calculate power using online calculators, such as those by Jake Westfall (\ttt{http://jakewestfall.org/}), which include calculators for the crossed random-effects case.  In general, power needs to be calculated via simulation; the two best current packages for this are {simr} and {mixedpower} \citep{green2016simr,kumle2020estimating}.

\citet{kumle2020estimating} is an excellent recent tutorial, showing practical examples in common situations, using these two packages.  Their summary (as of 2020) is that ``Power analysis for mixed-effects models is still a largely uncharted terrain containing many open and unresolved issues.''  By the time you read this, more options should be available.

Other recent discussions or tutorials of power for mixed-effects models, in the context of different kinds of linguistic data, include
\citet{brysbaert2018power};
%(psycholinguistics), 
\citet{brysbaert2020power};
%(bilingualism), 
\citet{sprouse2017design}. 
%(experimental syntax), \citet{kirby2018mixed} (phonetics).


\section{Other readings}
\label{sec:other-reading-ch10}

There is an influential  literature in statistics for language scientists/psychology discussing model selection and fitting (convergence/singularity) for {lme4} models, primarily linear mixed-effects models \citep{barr2013random,bates2015parsimonious,matuschek2017balancing};   \citet{seedorff2019maybe} also consider logistic regression.  This literature tends to focus on selecting random-effect structure. Several papers discuss these issues as motivation for using Bayesian models 
%the perspective of recommending with fitting lme4 models, esp. random effect structure, and recommend using Bayesian mixed-effects models as alternative 
\citep[e.g.,][]{kimball2019confronting,eager2017mixed,vasishth2018bayesian}.
There is a large literature on these issues in other fields, which tends to focus less narrowly on random-effect structure. I have found \citet{snijders2011multilevel,zuur2009extensions,gelman2007data,mcelreath2015statistical} helpful, as well as {lme4} help pages (\ttt{?convergence}, \ttt{?isSingular}).

Random effects can be used to capture many additional kinds of non-independence not discussed here, such as temporal/spatial autocorrelation, and multi-membership models (each observation can come from 1+ levels)  \citep[e.g.,][]{zuur2009extensions,baayen2018autocorrelated,pinheiro2000mixed,bolker2019estimating}. 

In addition to power, another important topic for mixed-effects models we do not have space to cover is %include: 
dealing with missing/incomplete data \citep{van2018flexible}.

Bayesian regression models are a natural next step after you are familiar with mixed-effects models.  `Hierarchical Bayesian models', which can be seen as a generalization of frequentist GLMMs, are the most widely-used type of Bayesian model.    Bayesian models allow for much more general model structure than GLMMs, greatly expanding the set of research questions you can address. They also have the practical advantage, discussed by the references above, of avoiding the convergence/singularity issues involving random-effect structure which have been a major theme of this chapter, by using prior distributions over random effects.  These advantages come at the cost of a modeling framework and software packages which are decidedly more complex than (G)LMMs using lme4, and require serious study (including a good understanding of GLMMs!) before you can confidently fit and interpret models.  This is why we have not touched on Bayesian models in this book even briefly, although they are increasingly used to analyze linguistic data, and will become more widely used in coming years.
Some places to start for linguists and cognitive scientists are \citet[][]{vasishth2018bayesian,nicenboim2021introduction}.  \citet[][]{mcelreath2020statistical}, \citet[][]{kruschke2014doing}, are comprehensive but approachable general introductions.
%to Bayesian regression modeling.

% - In addition, barr, Vasishth, Bates papers --- model selection (but, not GLMMs, just LMMs)
%
% - indepnedent simulation studies showing increased Type I error from not including random effect: \citet{barr2013random}
%

%% TODO:
% major theme of this chapter -- there are often practical issues to deal with when building an LME4 model, especially random effect structure.  We have shown solutoins while sticking with LME4 models.  Another option, which we do not discuss, is to move to a bayesian modeling framework, where these issues can be avoided---for example prior on ranef structure---at the cost of other complexites with fitting/interpreting models in this framework (e.g., Eager/Roy, Vasishth et al). See our Section XX for discussion.


% TODO: add back in somewhere footnote about Bayesian mixed-effects models, or in next chapter? see 7.8.2 QMLD.  At this point it doesn't make sense to not at least refer to this direction for next steps.


\section{Exercises}
\exer{\label{ex:neutralization-vc} Fit the model to the \ttt{neutralization} data described in Box~\ref{box:fixed-vs-random} (``We could alternatively...''), as an alternative to model \ttt{neut\_mod3}.  Compare these two models.  Do they give the same qualitative conclusions for terms of interest (= involving \ttt{voicing})?  Is there any advantage you see to one over the other?}

%% also, better conceptual interpretation: generalizes to all german VCs, not just ``random items from this population''
% neutralization$VC <- interaction(neutralization$place, neutralization$vowel)
% update(neut_mod3, . ~ . - place - vowel + (1|VC)) -> k

\exer{\label{ex:givenness-eda} Make exploratory plots of all possible (two-way) interactions for the \ttt{givenness} data.  If we were deciding what interactions to include in a `maximal' fixed-effect structure of a model predicting \ttt{stressshift} for this data, based purely on exploratory plots, which interactions look most promising?}

\exer{\label{ex:if0-posthoc} In Section~\ref{sec:ms-case-study-2}, the fitted model \ttt{if0\_m2} is interpreted with respect to the research question---whether there is a \ttt{Voicing.vl} effect, and if it is modulated by \ttt{base\_vowel}---using contrast coding (model coefficients).  Reinterpret the model using post-hoc tests which address four questions: whether there is a \ttt{Voicing.vl} effect for each vowel (\tsc{a}, \tsc{i}, \tsc{u}), and whether it is larger for high than for low vowels (\tsc{i}/\tsc{u} vs.\ \tsc{a}).  (It may be helpful to first refit the model with \ttt{Voicing} coded as a factor.)  Can you think of reasons to prefer one way of reporting the results over the other?}
 % exercise here would be: refit the model using factor version of Voicing.vl. Now do post hoc tests for all claims want to make: voicing effect for A, I, U (is it there for each vowel?), and A < I, U.

\exer{\label{ex:vot-voiced-extended} The case studies for model selection in the text use fairly restricted fixed-effect structures, for simplicity.  This exercise asks you to redo Case Study 3 (Section~\ref{sec:ms-case-study-3}) using a larger `maximal' fixed-effect structure, and examine how the results change.}

\subexer{Given the research question, it would be reasonable to consider some interactions with \ttt{speaking\_rate\_dev} and \ttt{speaking\_rate\_mean\_std} in the maximal fixed effects.  We have no a priori hypotheses about these interactions.  We could try adding all such interactions, and prune later, or be more selective.  Let's follow Gelman \& Hill's strategy (Section~\ref{sec:gh-model-selection}), and consider just interactions with predictors whose effects are largest.   Determine which 2--3 predictors have the largest effects (e.g.,\ using \ttt{Anova()} from the car package).  Add interactions of these with \ttt{speaking\_rate\_dev} and \ttt{speaking\_rate\_mean\_std} to the maximal fixed effects, and fit a random-intercepts-only model (analogous to \ttt{vot\_voiced\_m0}).}

\subexer{Build up the random-effect structure for this model, using the `uncorrelated first' strategy.  You can choose to consider correlation terms or not, for this exercise.  The resulting model is analogous to \ttt{vot\_voiced\_m3}.}

\subexer{Which effects involving \ttt{speaking\_rate\_dev} and \ttt{speaking\_rate\_mean\_std} are important in this model---by some criterion?  Make model prediction plots illustrating these effects.}

\subexer{You should find that both \ttt{speaking\_rate\_dev} and \ttt{speaking\_rate\_mean\_std} participate in significant interactions, while at least one speaking rate predictor has a significant main effect.  Discuss what the model predicts about {how} speaking rate affects VOT.  How does this conclusion differ from \ttt{vot\_voiced\_m3}?}



%% effects for those which come out signif:
% ggeffect(vot_voiced_m1_2, c('cons_cluster', 'speaking_rate_dev [-2:2]')) %>% data.frame() %>% ggplot(aes(x=group, y=predicted)) + geom_line(aes(color=factor(x), group=factor(x))) + xlab("speaking rate dev")
% ggeffect(vot_voiced_m1_2, c('place', 'speaking_rate_mean_std [-2:2]')) %>% data.frame() %>% ggplot(aes(x=group, y=predicted)) + geom_ribbon(aes(color=x, group=x, ymin=conf.low, ymax=conf.high, fill=x), alpha=0.5) + xlab("speaking rate mean")

%% suggests: *within-speaker*, smaller contextual effect (CCluster) for faster speech
%% - faster *speakers* have a smaller POA contextual effect

%% Overall, suggests *within-speaker* rate affects VOT for voiced stops; also small contextual effect on both within and across-speaker. mysterious...
