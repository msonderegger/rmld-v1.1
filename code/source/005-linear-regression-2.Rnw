% !Rnw root = master.Rnw

<<cache=FALSE, echo=FALSE>>=
## use num signif digits (which is what
## tidy print does by default) instead of decimal
options(knitr.digits.signif = TRUE)
options(digits = 2)
@

\chapter{Linear regression 2}
\label{chap:linear-regression-2}

In the previous chapter we fit linear regressions assuming that all assumptions of linear regression were met, and the set of variables to include in the model was fixed.  In this chapter we first discuss validation of a fitted model: the assumptions of linear regression,  how to detect and address when they are violated (Section~\ref{sec:lr-assumptions}--\ref{sec:lr-problems-with-observations}),  and assessing overfitting or underfitting to the data (Section~\ref{sec:overfitting}). 
%
%and  \emph{model criticism}, then the notion of `overfitting' and testing whether the model overfits the data, or \emph{model validation} 
We then turn to methods for comparing different models (Section~\ref{sec:lm-model-comparison}), including selecting which predictors to include (Section~\ref{sec:variable-selection}),
%\emph{variable selection}, 
culminating in different possible methods for building up a model.


\section{Preliminaries}

\subsection{Packages}

This chapter assumes that you have loaded packages  from previous chapters, 
as well as the arm, car, performance, and MuMIn packages \citep{armPackage,fox2019r,performance,MuMIn}, which contain useful functionality for working with regression models and assessing their performance.
%e\ttt{arm}, \ttt{car}, and \ttt{performance} packages 
%:\footnote{\ttt{arm} and \ttt{car} are the companions to texts on applied regression modeling \citep{gelman2007data,fox2019r}. \ttt{performance} is introduced in the next chapter.}
%hence the `ar' in their names.}
%a text on \textbf{a}pplied \textbf{r}egression \textbf{m}odeling which we will often refer to.}

<<message=F, error=F, warning=F, cache=FALSE>>=
library(tidyverse)
library(broom)
library(languageR)
library(car)
library(arm)
library(performance)
library(MuMIn)
library(patchwork)

## Ensures that `rescale', a function we will use, 
## is not the version from the `scales' package.
rescale <- arm::rescale
@

\subsection{Data}
\label{sec:vot-dataset}

\subsection*{The \ttt{vot} dataset}

We also assume that you have loaded the \ttt{vot} dataset:
<<>>=
vot <- read.csv("data/vot_rmld.csv")
@

This dataset, analyzed in \citet{sonderegger2017medium} and described in more detail in Appendix~\ref{sec:vot-data}, contains positive voice onset times (VOTs: column \tsc{vot}, in msec) measured for word-initial stop consonants (p/t/k/b/d/g) in spontaneous speech from a corpus of speakers of different British English dialects. VOT is the primary acoustic cue in English signaling whether a stop is phonologically \tsc{voiced} (p/t/k) or \tsc{voiceless} (b/d/g) (column \tsc{voicing}), but also varies as a function of many other properties of the word, context, or speaker, including:
%\footnote{If you know about VOT, you have learned that it can be positive or negative.  VOT in this dataset is always positive, because `prevoicing' almost never occurs in spontaneous speech (see \citealp{sonderegger2017medium}, Glasgow-VOT refs).} 
\begin{itemize}
\item \ttt{speaking\_rate} (in syllables/second) of the surrounding phrase
\item \ttt{place} (levels \tsc{alveolar}, \tsc{labial}, \tsc{velar}): the consonant's place of articulation
\item \ttt{cons\_cluster} (levels \tsc{no}, \tsc{yes}): whether the stop is in a consonant cluster (e.g.,\ `tick', `trick')
\item \ttt{foll\_high\_vowel} (levels \tsc{no}, \tsc{yes}): whether the following vowel is high or non-high (e.g.,\ `tease', `tack')
 \item \ttt{gender} of the speaker (levels \tsc{f}, \tsc{m})
 \item \ttt{log\_celex\_freq}: the word's frequency (log-transformed)
\end{itemize}
We will consider different research questions in several analyses of this data.
% Of interest is which/how different predictors affect VOT, and their relative strengths.  

Observations from the same speaker (\ttt{speaker}) or word (\ttt{word}) are not independent.
%but independent observations are assumed by methods used in this chapter. 
To have a plausibly-independent dataset we can analyze using linear regression (as discussed in Section~\ref{sec:type-ii-error}), we take a subset \ttt{vot\_michael} of data consisting of observations from a single speaker (\tsc{michael}) and one observation per word:

<<>>=
vot_michael <- filter(vot, speaker == "michael") %>%
  ## First use of each unique word
  filter(!duplicated(word)) %>% 
  droplevels()
@

%We assume that you have run these commands, so the dataframe 
This dataframe contains $n = \Sexpr{nrow(vot_michael)}$ observations.
In addition, we simplify the \ttt{place} column to have just two levels (\tsc{labial}, \tsc{non\_labial}), since we have not covered multi-level factors yet:

<<>>=
vot_michael <- mutate(vot_michael,
  place = recode_factor(place,
    alveolar = "non_labial", velar = "non_labial"
  )
)
@


\section{Linear regression assumptions}
\label{sec:lr-assumptions}

Any regression model makes a range of assumptions---about the data, the form of the relationship between predictors and the response, and so on.
%and 
%the model relating predictors to the response.
Following \citet[][chap.~6--7]{faraway2015linear}, it is useful to think of these in terms of concrete `problems' which could apply to a given model:
\begin{itemize}
\item Problems with the \textbf{errors}  (e.g.,\ non-normality: Section~\ref{sec:lr-problems-with-errors})
\item Problems with the \textbf{model}   (e.g.,\ nonlinearity: Section~\ref{sec:lr-model-problems})
\item Problems with \textbf{predictors}  (e.g.,\ collinearity: Section~\ref{sec:lr-predictor-problems})
\item Problems with \textbf{observations} (e.g.,\ outliers: Section~\ref{sec:lr-problems-with-observations})
\end{itemize}

%Diagnosing and addressing these problems is sometimes called \emph{model criticism} \citep[][1.4.7]{chatterjee2012regression}.  

In each case it is important to know how to check for the problem, and if the problem exists, how serious is it and what can be done to address it?  

Closely related topics are assessing how well the model fits the data (goodness of fit: Section~\ref{sec:goodness-of-fit}), and how well it generalizes to unseen data. These can be thought of as \textbf{problems with fit} (Section~\ref{sec:overfitting}): either not fitting the dataset well,
%(\emph{underfitting}), 
or not generalizing well to unseen data.
%(\emph{overfitting}).  
Fitting observed and unseen data well are not  assumptions of linear regression models, but are obvious desiderata for a model.
%A closely related topic is determining how likely the model is to have problems generalizing to unseen data, or \emph{overfitting}.  (However, fitting unseen data well is not actually a regression assumption, so it's best to keep the terms separate.)

All these topics are part of \emph{model validation}---the
%, along with assesing goodness of fit (covered previously)
%are part of `validation'  or `criticism' of a model.
%Sources differ on what `criticism' versus `diagnosis' versus `validation' mean, so we just use a single term for simplicity, following Wikipedia.}
%The purpose of \emph{model validation}---the 
iterative process of diagnosing and (possibly) addressing problems, and (possibly) changing the data and model as a result---to arrive at a statistical model that we can trust.\footnote{Some authors \citep[e.g.,][]{baayen2008analyzing} distinguish between `model criticism' (checking and correcting regression assumptions) and `model validation' (assessing goodness of fit and overfitting). In this book we just use a single term (`validation') for simplicity.} Only then should we interpret our model to address research questions, or use it to predict for new data.  This process can feel overwhelming because there are so many things that could be wrong with a model,
%(Box~\ref{box:model-reassurance}), 
but the point is not to give a `correct' model. As \citet[][14]{faraway2016extending} puts it:
\begin{quote}
``It is virtually impossible to verify that a given model is exactly correct. The purpose of the diagnostics is more to \textbf{check whether the model is not grossly wrong}.'' (emphasis mine)
\end{quote}
%Indeed, a successful data analyst should pay more attention to avoiding big mistakes than optimizing the fit.

There is an important additional assumption 
%In addition to the assumptions discussed below, there is an important high-level assumption 
for every regression model, ``that the data at hand are actually relevant for the question of interest'' \citep[][97]{faraway2016extending}.  There are no diagnostics to check this.


%% FUTURE: consider adding box back in
% \begin{boxedtext}{Broader context: model validation reassurance}
% \label{box:model-reassurance}
% 
% Model validation, though often omitted in current practice (by language scientists, including me), is critical before interpreting the model---to answer research questions, or  to predict for new data. 
% %For linguistic data, \citet{baayen2008analyzing} emphasizes this point and shows detailed examples.  
% This includes all the steps discussed in the following sections, 
% %`Criticism' includes the problems listed above, 
% but also basic sanity checks like seeing if your model's key results are consistent with empirical plots, and just thinking about whether it makes reasonable assumptions (e.g.,\ `can it predict values that are physically impossible?': Section~\ref{sec:bounded-discrete-y}).  It is very tempting to skip these steps once you have a fitted model---especially if it confirms a result you hoped to find.  But the less a model has been validated/criticized, the more likely your conclusions about scientific questions will be wrong. 
% 
% That said, even if you are being conscientious about model validation, it can feel overwhelming that there are so many ways for a regression model to \textbf{not} validate.    And it is never possible to do exhaustive model validation---there is always another diagnostic plot or statistic you could calculate---so it is up to the analyst which ones are most important given her data/model/research questions.  Paradoxically, the better you understand regression (or any statistical technique), the easier it is to become paralyzed with fear by such issues.  
% 
% It is important to take a step back, and remember:
% when doing data analysis, you should validate your data and model as best you can, and then \textbf{report your results anyway}. You should not fit regression models blindly, and it's important to be aware of the relevant issues and have intuitions about the practical consequences (our goal in this chapter). 
% But you should also keep
% %But the more you learn about regression (or any statistical technique), the easier it is to become paralyzed with fear by everything that could be wrong with your data. Keep 
% in mind the dictum of George Box: ``Essentially, all models are wrong, but some are useful.'' That is, every model is just that: a \textbf{model} of reality, that is only as useful as the insight it can offer about the questions of interest. Every step of model validation you perform brings you closer to this goal.  But often, minor violations of model assumptions (or modest overfitting) don't affect the qualitative conclusions you can draw from the model anyway.
% \end{boxedtext}

% 
% This tendency is not helped by the fact that model criticism and model validation are complex subjects which can seem overwhelming---a full treatment is beyond the scope of this book (see `Other reading'). Our goal is to give a basic introduction, and some intuitions about the relevant issues, in order to illustrate the practical consequences of model criticism and validation.  An important theme is that model criticism/validation lead to \textbf{better} models---sometimes this means a result which supported your research hypothesis `goes away', but often it means \textbf{stronger} results, by addressing problems which obscured a pattern.  Either outcome is desirable.   So spending time on model criticism is not just the right thing to do (though it is)---it is also a good investment of time.
% 
% 
% It may seem a bit daunting that there are so many ways for a regression model to \textbf{not} be appropriate.
% It is important to take a step back, and remember:
% when doing data analysis, you should just satisfy the assumptions of the statistical tool being used as best you can, and then \textbf{fit and report your model anyway}. You should not fit regression models blindly, and it's important to be aware of the assumptions being made by these models, and their limitations. But the more you learn about regression (or any statistical technique), the easier it is to become paralyzed with fear by everything that could be wrong with your data. Keep in mind the dictum of George Box: ``Essentially, all models are wrong, but some are useful.'' That is, every model is just that: a \textbf{model} of reality, that is only as useful as the insight it can offer about the questions of interest. Often, minor violations of model assumptions don't affect the qualitative conclusions you can draw from the model.
% \end{boxedtext}
% 
% 
% FUTURE: anything like this?
% \begin{boxedtext}{Broader context: ???}
% 
% Every introduction to regression models is clear about the importance of model criticism and (to a lesser extent) validation before drawing any conclusions from a model. For linguistic data, \citet{baayen2008analyzing} emphasizes this point and shows detailed examples.  `Criticism' includes the problems listed above, but also basic sanity checks like seeing if your model's key results are consistent with empirical plots, and just thinking about whether it makes reasonable assumptions (e.g.,\ Section\ XX).  The less a model has been criticized (and validated), the more likely your conclusions about scientific questions will be wrong. 
% 
% Nonetheless, I find that these steps are often skimped on when analyzing linguistic data (including by me!), and one suspects reading between the lines of statistics texts for non-linguists that the same is true for practitioners in many fields.  It is very tempting to just interpret a fitted model without (much) criticism or validation, especially because these steps are `due dilligence' that are not usually detailed in a write-up, they feel like a lot of work, and they are subconsciously avoided because they feel like they may `break' result(s) from your current model.
% 
% This tendency is not helped by the fact that model criticism and model validation are complex subjects which can seem overwhelming---a full treatment is beyond the scope of this book (see `Other reading'). Our goal is to give a basic introduction, and some intuitions about the relevant issues, in order to illustrate the practical consequences of model criticism and validation.  An important theme is that model criticism/validation lead to \textbf{better} models---sometimes this means a result which supported your research hypothesis `goes away', but often it means \textbf{stronger} results, by addressing problems which obscured a pattern.  Either outcome is desirable.   So spending time on model criticism is not just the right thing to do (though it is)---it is also a good investment of time.
% 
% \end{boxedtext}

 
% Any regression model makes a range of assumptions about the data and the model, which can be thought of as  following REF-Faraways, 
% 
% For a linear regression, these include normally-distributed errors, that the  For each assumption, it is important to know how to determine whether it is  
% 
% \begin{itemize}
% \item
%   What are the assumptions (of linear regression)?
% \item
%   For each assumption, how do we determine whether it's valid?
% \item
%   How much of a problem is it, and what can be done, if the assumption is not met?
% \end{itemize}
% 
% Note that R almost never checks whether your data and model meet regression analysis assumptions, unlike other software (e.g.,~SPSS, sometimes).
% 
% The purpose of model criticism and validation---the iterative process of diagnosing and (possibly) addressing such problems, then (possibly) changing the data and model---is to arrive at a statistical model that we can `trust' to make conclusions

\paragraph{Visual methods}
%\label{sec:visual-methods}


<<echo=FALSE>>=
anscombe_r <- cor(anscombe$x1, anscombe$y1)
temp_mod <- lm(y1 ~ x1, data = anscombe)
anscombe_a <- coefficients(temp_mod)[1]
anscombe_b <- coefficients(temp_mod)[2]
@


Visualization is crucial for checking model assumptions, and for data analysis in general. A famous example illustrating this is \emph{Anscombe's quartet} (\citealp{anscombe1973graphs}; \ttt{anscombe} in R), a set of four small datasets ($n = \Sexpr{nrow(anscombe)}$) of \((x,y)\) pairs which seem similar just from numerical summary statistics. The (Sample) mean and standard deviation for $x$ and $y$, as well as the least-squares regression line (and thus the correlation between $x$ and $y$: $r_{xy} = \Sexpr{anscombe_r}$), are the same for each dataset.
%have the same values.
% which all have the same values: 
% \begin{itemize}
% \item (Sample) mean and standard deviation for $x$ and $y$ 
% %($\bar{x} = \Sexpr{mean(anscombe$x1)}$, $\hat{\sigma}_x = \Sexpr{sd(anscombe$x1)}$, $\bar{y} = \Sexpr{mean(anscombe$y1)}$, $\hat{\sigma}_y = \Sexpr{sd(anscombe$y1)}$)
% \item Correlation between $x$ and $y$, and least-squares regression line
% %($r_{xy} = \Sexpr{anscombe_r}$)
% %\item least-squares regression line ($y = \Sexpr{anscombe_a} + \Sexpr{anscombe_b} \cdot x$)
% \end{itemize}


<<anscombe, echo=FALSE, fig.asp=.4, fig.cap="Anscombe's quartet.">>=
## NB: 'anscombe' is loaded automatically in R
anscombe_m <- data.frame()

for (i in 1:4) {
  anscombe_m <- rbind(
    anscombe_m, 
    data.frame(set = i, x = anscombe[, i], y = anscombe[, i + 4])
  )
}

ggplot(anscombe_m, aes(x, y)) +
  geom_point(size = 1) +
  geom_smooth(color='darkgrey', method = "lm", fill = NA, fullrange = TRUE) +
  facet_wrap(~set, ncol = 4)
@

Yet it is immediately clear from plotting the datasets (Figure~\ref{fig:anscombe}) that they show qualitatively different patterns.  Dataset 1 is what we instinctively imagine given the summary statistics: a line of best fit, with each observation about evenly well-predicted, and no one observation having much more influence than any other. But these assumptions are violated for Datasets 2, 3, and 4 (respectively), each of which corresponds to a different problem we'd like to detect through model validation: non-linearity, outliers, and overly influential observations.

With more than one predictor it is not possible to check regression assumptions by just plotting the data, but other visual methods (presented below, such as residual plots) are available.  Although there are usually numeric `tests' available as well to check regression assumptions, in general visual methods are more effective (e.g.,\ \citealp[][75]{faraway2015linear}; \citealp[][85]{chatterjee2012regression}; \citealp{zuur2010protocol}).
% ``A formal diagnostic test may have a reassuring aura of exactitude about it, but one needs to understand that any such test may be powerless to detect problems of an unsuspected nature. Graphical techniques are usually more effective at revealing structure that you may not have suspected.''  Also  \citet[][85]{chatterjee2012regression}, other REF?

% 
% quantiative measures available  In general such graphical   presented below are crucial: residual plots


% 
% With more than one predictor it becomes difficult to check regression assumptions by just plotting the data, and visual methods such as \emph{residual plots} (presented below) are crucial.

\section{Problems with the errors}
\label{sec:lr-problems-with-errors}

Any linear regression model, 
%Linear regression equations, 
such as equation (\ref{eq:mlr-1}), contains a normally-distributed \emph{error} term (\(\epsilon_i\) for the \(i^{\text{th}}\) observation). There are several associated assumptions made by the model.
%linear regression model. 
%associated of a particular form ($\epsilon_i \sim N(0, \sigma)$). The 

\subsection{Independence of errors}
\label{sec:lin-reg-indep}
A crucial assumption is that these \(\epsilon_i\) are {independent}: knowing the error for one observation shouldn't tell you anything about the error for another observation.

Unfortunately, violations of this assumption are endemic in linguistic data, and ignoring this assumption in data analysis (`pseudoreplication') can cause serious problems---as we discussed with respect to $t$-tests in Section~\ref{sec:t-test-indep-assumptions}.  Violations of the independence assumption are often anti-conservative: CIs will be too narrow and \(p\)-values too small if the lack of independence of errors is not taken into account by the model.  The examples in Section~\ref{sec:t-test-indep-assumptions}, which carry over directly to linear regression models,\footnote{Since they are analyzed with $t$-tests (Section~\ref{sec:multiple-linear-regression}).}  show the effects of the most common kind of non-independence in linguistic data: grouping structure (e.g.,\ by subject and item).  This is arguably the most important assumption underlying statistical analysis of linguistic data more generally, and a primary  motivation for using regression models which can take grouping structure into account, which we turn to in 
%we discuss further when we turn to mixed models in 
Chapter~\ref{chap:lmm-1}.  Non-independent errors can also arise due to \emph{autocorrelation} among observations which are somehow `near' each other, usually in time or space (Box~\ref{box:autocorrelation}).
%; we do not cover methods for modeling autocorrelated errors.

\begin{boxedtext}{Broader context: Autocorrelation}
\label{box:autocorrelation}
%Non-independent errors also commonly arise due to \emph{autocorrelation} among observations which are somehow `near' each other, usually in time or space.  
%also arise due to temporal structure or spatial structure, where observations which are `near' each other have correlated errors (\emph{autocorrelation}). 
Autocorrelation is fairly common in `raw' linguistic data.   For example, nearby observations in time will be  correlated in acoustic measurements of speech (pitch or formant tracks) or time series from behavioral experiments (e.g.,\  eye tracking data, self-paced reading).
%, or even adjacent sites, or priming experiments).  Less obviously, adjacent trials in a behavioral experiment can be correlated (subjects may adapt to the experiment).  
In data from dialectology or neurological studies
%or some neurological data 
(where the output is a map of brain activity), observations spatially near each other may be correlated.  

For data where each observation is a time series (like a pitch or eye-tracker track), it is common in linguistics to deal with temporal autocorrelation by collapsing to a single number, by averaging over a `region of interest' (e.g.,\ 100--200 msec from stimulus onset) or taking a single point (e.g.,\ 33\% through a vowel).  But unlike grouping
% JPK flipped out at "e.g., a time term in...", but turns out that was because he thought this implied linear time.
structure, it is sometimes possible to account for temporal or spatial structure by simply including predictors for time/space (e.g., a linear or polynomial effect of \ttt{time}) in the regression model.
%in a multiple linear regression model).  

If correlated errors still exist, you can use regression models which take autocorrelation into account by explicitly modeling the errors, like `generalized least squares' or `time series' models (most commonly autoregressive moving average: ARMA).
Modeling autocorrelated errors is beyond the scope of this book, but some places to start %with R examples
are \citet[][chap.~8]{faraway2015linear} and \citet[][chap.~16--19]{zuur2007analyzing}.  
%The upside is it is actually fairly simple to account for limited autocorrelation in common regression models (LMs, GLMs, LMEs), though not commonly done in current practice for linguistic data.
% FUTURE (another Zuur book better?). 
For linguistic data, recent work deals with autocorrelated errors using more complex methods
%methods
%complex methods
%spatial statistics or 
%more complex methods, such as 
%generalized additive mixed models
%,`generalized additive mixed models'
%, which can handle both clustered data and autocorrelated errors 
(e.g.,\ \citealp{baayen2018autocorrelated,wieling2018analyzing,grieve2017spatial} for psycholinguistics, phonetics, dialectology). 
%% FUTURE: any more refs in last parens, and after Zuur book? just took these out so it reads well. isn't there anything from dialectology? it would be nice to cite an example of dealing with autocorrelated errors in the context of a less fancy model than GAMMs.
\end{boxedtext}

% correlated errors still exist is an empirical question. and so....  
% (for temporal data sometimes just including time as a factor helps, but this is isn't the right place for discussion..)
% \begin{itemize}
% \item Time, such as pitch measurements taken every 10 msec on a speech signal, or raw eye-tracker data.
% \item Space, such as in typological data
% \begin{itemize}
%The simplest example is in time series, or longitudinal data---such as pitch measurements taken every 10 msec in a speech signal.
% 
% \begin{quote}
% \textbf{Questions}:
% 
% \begin{itemize}
% \tightlist
% \item
%   Can you think of why this might be the case?
% \end{itemize}
% \end{quote}
% 
% In linguistics and psycholinguistics, violations of the independence assuption are common because most datasets include multiple observations per participant or per item (or per word, etc.). Crucially, violations of the independence assumption are often \emph{anti-conservative}: CIs will be too narrow and \(p\)-values too small if the lack of independence of errors is not taken into account by the model.
%
%Some solutions to these issues:

In practice it is usually hard to detect problems with independence via diagnostic plots. You have to think about the structure of your data to know whether the independence assumptions hold, and how to transform or subset it to satisfy them (see discussion in Section~\ref{sec:type-ii-error}).
%For example, the \ttt{english} dataset results from averaging a larger dataset across responses from all participants (Sec.~\ref{sec:english-dataset}) Lexicon Project data across subjects 


% for data grouped by subject but consisting of pairs of non-independent observations where the difference is of primary interest (like a single \tsc{voiced}/\tsc{voiceless} pair in the \ttt{neutralization} data), you could compute pairwise differences and analyze these with a linear regression model. 
% For data where each observation is a time series (like a pitch or eye-tracker track), it is common in linguistics to deal with temporal autocorrelation by collapsing to a single number, whether by averaging over a `region of interest' (e.g.,\ 100--200 msec from stimulus onset) or taking a single point (e.g.,\ 33\% through a vowel).

% may be able to analyze pairwise differences with a regression model.\footnote{However for the \ttt{neutralization} data this would only work }  You have to think about your data the independence assumptions; you have to think about your data to know whether they hold, and whether your data can be transformed to satisfy them.  For example, for the  the paired $t$-test introduced above (XX) deals with a particular kind of violation of the independence assumption---pairs of observations are not independent---by computing pairwise differences, then carrying out a one-sample $t$-test on these values.
% 
% 
% \begin{itemize}
% \item
%   \protect\hyperlink{paired-t-test}{\textbf{Paired-t-tests}}, where applicable (binary predictor; two measures per participant)
% \item
%   \textbf{Mixed-effects regression} (more general solution, major focus later this term)
% \end{itemize}


\subsection{Normality of errors}\label{sec:normality-of-errors-assumption}

The next major assumption is that the errors \(\epsilon_i\) are normally distributed, with mean 0 and a fixed variance. This assumption is impossible to check directly, because we never observe the true errors \(\epsilon_i\), only the residuals $\hat{\epsilon}_i$.
%\emph{residuals} \(e_i\).  
In general, we {check assumptions about errors by examining the distribution of the residuals}. This is because {if} the normality of errors assumption holds, {then} the  residuals will be normally distributed with mean 0 and fixed variance. So if they are not, we know the assumption does not hold. (If the results are normally distributed, it's not a guarantee that the assumption holds, but we hope for the best.)

\begin{boxedtext}{Broader context: Standardized/studentized residuals}
\label{box:standardized-residuals}

% One barrier to thinking about resiudals and examining resiudal plots is confusing terminology.  Plots/R output sometimes mention `residuals' and sometimes `standardized' or `studentized' residuals, or apply a $\sqrt{|\text{residual}|}$ transform. For example, the diagnostic plots for a linear regression model (e.g.,\ \ttt{plot(mlr\_mod\_4)}) use three different variants on the $y$-axis.  Usually these distinctions do not matter: it matters much more that

Technically there is a flaw in the reasoning above: the residuals of a linear regression are not normally distributed (even if the errors are), because some observations will be more influential than others in determining the fitted responses \(\hat{y}_i\) when fitting the least-squares estimates of the regression coefficients. This can be seen
%easiest to see
in a simple linear regression line-of-best-fit (e.g.,\ Figure~\ref{fig:young-subset-1}): the confidence interval is wider for points further  from the means of $x$ and $y$, because these points are more influential, causing the variance of their residuals to increase.  Thus the variance of the residuals is not constant, so they don't follow a normal distribution.  
It is possible to transform the residuals in a way which accounts for the different influence of different observations  to `studentized' or `standardized' residuals, then use these transformed residuals in diagnostic plots.  This is recommended by some regression texts \citep[e.g.,][\S4.3]{chatterjee2012regression}, while others \citep[e.g.,][\S6.1]{faraway2015linear} think it is sufficient to just use plots based on untransformed residuals.  We usually use untransformed residuals, but sometimes use standardized residuals when this is the default of an R function, or standardizing is particularly important (defining outliers: Sec.~\ref{sec:outliers}).

\end{boxedtext}

% \footnote{The studentized and standardized residuals, or ``externally studentized'' and ``internally studentized'' residuals (in \citet{chatterjee2012regression}), differ slightly in how they estimate the error variance: a leave-one-out estimate versus an estimate using all observations. This difference shouldn't matter much except when certain observations are highly influential or in small datasets.} (In R, by applying \texttt{rstudent} or \texttt{rstandard} to a fitted model.)


% 
% 
% \hypertarget{example-11}{%
% \subsubsection*{Example}\label{example-11}}
% \addcontentsline{toc}{subsubsection}{Example}
% 
% <<fig.align='center', fig.height=3, fig.width=7>>=
% young <- filter(english, AgeSubject=='young')
% 
% set.seed(2903)
% young_sample <- young %>% sample_n(100)
% 
% ggplot(young_sample, aes(WrittenFrequency, RTlexdec)) + 
%   geom_point(size=0.5) +
%   geom_smooth(color=default_line_color, method="lm") + 
%   ggtitle("Linear regression line and 95% CI")
% 
% @
% 
% The width of the confidence interval increases for points further from (average of \texttt{WrittenFrequency}, average of \texttt{RTlexdec}), because these points are more influential, causing the variance of the residuals to increase---thus, the variance is not constant.


\subsubsection{Example}
\label{sec:vot-ex-1}

Consider a first model of the \ttt{vot\_michael} data, where we predict VOT as a function of three predictors expected to have strong effects (Section~\ref{sec:vot-dataset}): \ttt{voicing}, \ttt{place}, \ttt{cons\_cluster}, \ttt{speaking\_rate}.

<<>>=
vot_mod_1 <- lm(vot ~ voicing + place + cons_cluster + speaking_rate,
  data = vot_michael)
@

We can extract the residuals using \ttt{residuals(vot\_mod\_1)}, or the tidy \ttt{augment()} function (from \ttt{broom}), which adds several useful columns of information for diagnostic plots to the original dataframe, including the fitted values ($\hat{y}_i$) and residuals ($\hat{\epsilon}_i$):

<<>>=
augment(vot_mod_1) %>% head(n=3)
@

<<resid_plot_1, echo=FALSE, out.width='30%', fig.asp=.75, fig.width=default_fig.width*.3/default_out.width, fig.cap='Three residual diagnostic plots for \\ttt{vot\\_mod\\_1}: Q-Q plot of residuals (left), fitted value-residual plot (middle), scale-location plot (right)', echo=1>>=
augment(vot_mod_1) %>% ggplot(aes(sample = .resid)) +
  geom_qq(size=0.5, color='darkgrey') +
  geom_qq_line(color='black', size=1) +
  xlab("Theoretical quantiles") +
  ylab("Residuals")
augment(vot_mod_1) %>% ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(size=0.5, color='darkgrey') +
  geom_smooth(color='black', se = F) +
  geom_hline(aes(yintercept = 0), lty = 2) +
  xlab("Fitted values") +
  ylab("Residuals")
augment(vot_mod_1) %>% ggplot(aes(x = .fitted, y = sqrt(abs(.resid)))) +
  geom_point(size=0.5, color='darkgrey') +
  geom_smooth(color='black', se = F) +
  xlab("Fitted values") +
  ylab(expression(sqrt(abs("residual"))))
@
The augmented data frame can then be used to make diagnostic plots, such as a \emph{Q-Q plot} of residuals to check normality (Figure~\ref{fig:resid_plot_1} left). 
The residuals are somewhat non-normally distributed (histogram in Figure~\ref{fig:resid_plot_2} left).
%longer right tail than left tail). 
The Q-Q plot suggests that this is primarily due to observations with the very highest and lowest residuals.  
In the \emph{fitted values-residuals plot} (Figure~\ref{fig:resid_plot_1} left), we see that the points responsible for non-normality tend to be those with the highest and lowest values of the response (\ttt{vot})---which are also responsible for the non-normal distribution of the response (Figure~\ref{fig:resid_plot_2} right).  This illustrates %probably
the most common source of non-normal residuals: a highly non-normal distribution of the predictor or response.



<<resid_plot_2, echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Distributions of residuals of \\ttt{vot\\_mod\\_1} (left), and of VOT in the \\ttt{vot\\_michael} dataset (right) for \\tsc{voiced} and \\tsc{voiceless} consonants.'>>=
augment(vot_mod_1) %>% ggplot(aes(x = .resid)) +
  geom_histogram() +
  xlab("Residuals")
vot_michael %>% ggplot(aes(x = vot)) +
  geom_density(aes(fill = voicing, color = voicing), alpha = 0.5) +
  xlab("VOT") + theme(legend.position=c(0.8,0.8)) + 
  scale_fill_grey() + scale_color_grey()
@

The italicized diagnostic plots
%in Figure~\ref{fig:resid_plot_1} 
are among those made automatically by \ttt{plot} applied to a linear regression model in R (e.g., \ttt{plot(vot\_mod\_1)}).
%, as shown in Figure~\ref{fig:resid_plot_4} for a later model.
%(e.g.,\ \ttt{plot(vot\_mod\_1)}.



%Figure~\ref{}  by \texttt{plot(mod)} in R (where \ttt{mod} is a linear regression).

% \hypertarget{c2ex2}{%
% \subsubsection*{Example}\label{c2ex2}}
% \addcontentsline{toc}{subsubsection}{Example}
% 
% This exercises uses the \texttt{halfrhyme} data, briefly described \protect\hyperlink{halfdata}{here}. Let's abstract away from what the variables actually mean, and just think of them as \(Y\) and \(X\):
% 
% <<warning=FALSE>>=
% ggplot(aes(x=cohortSize, y=rhymeRating),  data=filter(halfrhyme, conditionLabel=='bad')) +
%   geom_point() + geom_smooth(method='lm') + 
%   geom_jitter() + xlab("X") + ylab("Y")
% @
% 
% The distribution of the standarized residuals for the regression of \(Y\) as a function of \(X\) is:
% 
% <<message=FALSE>>=
% halfrhyme.sub <- filter(halfrhyme, conditionLabel=='bad' & !is.na(cohortSize))
% mod <- lm(rhymeRating ~ cohortSize, data=halfrhyme.sub)
% halfrhyme.sub$resid <- rstandard(mod)
% ggplot(aes(x=resid), data=halfrhyme.sub) + geom_histogram() + xlab("Residual (standardized)")
% @
% 
% \begin{quote}
% \textbf{Questions}:
% 
% \begin{itemize}
% \tightlist
% \item
%   Why do the residuals have this distribution?
% \end{itemize}
% \end{quote}
% 
% This example illustrates probably the most common source of non-normal residuals: a highly non-normal distirbution of the predictor or response.

%\subsubsection{Effect and solution}

Non-normality of residuals is a common violation of regression assumptions. In practice it often doesn't matter much, at least if our goal is estimation of the regression coefficients (as opposed to prediction)---especially for large samples, where the central limit theorem kicks in (\citealp[][\S6.4]{faraway2015linear}; \citealp[][46]{gelman2007data}).
%matter much How much does it actually matter? \citet{gelman2007data} (p.~46) argue ``not much'', at least in terms of the least-squares estimates of the regression line (i.e., the regression coefficient values), which is often what you are interested in.

However, it is always a good idea to examine the distribution of residuals because
%(serious) 
non-normality of residuals
%(especially when severe) 
can signal other issues with the data, such as the presence of outliers, or the predictor or response being on the wrong scale.
%(e.g.,\ using non-log-transformed word frequency: see Section~\ref{sec:transforming-to-normality}).
Non-normality of residuals can often be dealt with by transforming the predictor or response to have a more normal distribution. Section \ref{sec:transformations-problems} discusses these points further.

Non-normality of residuals can also signal other errors, such as an important predictor missing.

\subsubsection{Example}
\label{sec:non-normal-resid-ex}

Consider two linear regressions of reaction time (\ttt{RTlexdec}) for the \ttt{english} data: as a function of word frequency (\ttt{WrittenFrequency}) alone, and as a function of word frequency and subject age (\ttt{AgeSubject}).

<<>>=
english_mod_1 <- lm(RTlexdec ~ WrittenFrequency, data = english)
## Add one predictor to model formula
english_mod_2 <- update(english_mod_1, . ~ . + AgeSubject)
@

The residuals of the first model are not normal (Figure~\ref{fig:resid_plot_3} left), primarily because they are bimodal---the two modes correspond to values of \ttt{AgeSubject} (left, right---\tsc{old}, \tsc{young}: Exercise \ref{ex:non-normal-resid}).  The residuals of the second model (Figure~\ref{fig:resid_plot_3} right) look closer to normality; they are no longer bimodal.

<<resid_plot_3, echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Distributions of residuals of \\ttt{english\\_mod\\_1} (left) and \\ttt{english\\_mod\\_2} (right).'>>=
english_mod_1 %>%
  augment() %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 30) +
  xlab("Residuals")
english_mod_2 %>%
  augment() %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 30) +
  xlab("Residuals")
@

This example shows one reason that examining the residual distribution is useful. If we didn't already know what the missing predictor was, the non-normality of the residual distribution gives us a way to look for an explanatory variable. (Look at observations in each mode of the distribution, see what they have in common.)


% 
% <<fig.align='center', fig.height=4, fig.width=5>>=
% ggplot(english, aes(x = WrittenFrequency, y = RTlexdec)) + 
%   geom_point(size = 0.5) + 
%   geom_smooth(method = "lm", se=F)
% @
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \setcounter{enumi}{1}
% \item
%   Do you think the residuals of this model are normally distributed? Why/why not?
% \item
%   Now plot a histogram of the standardized residuals of the mode. Does the plot confirm your first impressions?
% \end{enumerate}
% % 
% <<eval=F>>=
% m8 <- lm(RTlexdec ~ _______, english)
% m8.resid.std <- rstandard(______)
% hist(______, breaks = 50)
% @
% 
% <<fig.align='center', out.width='45%', fig.width=default_fig.width*.45/default_out.width>>=
% ggplot(english, aes(WrittenFrequency, RTlexdec)) + 
%   geom_point(size=0.5) + 
%   geom_smooth(method="lm", se=F)
% 
% m8 <- lm(RTlexdec~WrittenFrequency, english)
% m8.resid.std <- rstandard(m8)
% 
% ggplot(data.frame(m8.resid.std), aes(x = m8.resid.std)) + 
%   geom_histogram(bins = 50)
% @
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \setcounter{enumi}{3}
% \tightlist
% \item
%   Now add \texttt{AgeSubject} to the model, and plot a histogram of its standardized residuals. What has changed? Why so?
% \end{enumerate}
% 
% <<echo=F, out.width='45%', fig.width=default_fig.width*.45/default_out.width>>=
% m9 <- lm(RTlexdec ~ WrittenFrequency + AgeSubject, english)
% m9.resid.std <- rstandard(m9)
% ggplot(data.frame(m9.resid.std), aes(x = m9.resid.std)) + 
%   geom_histogram(bins = 50)
% 
% ggplot(english, aes(WrittenFrequency, RTlexdec, color = AgeSubject)) + 
%   geom_point(size=0.5) + 
%   geom_smooth(method="lm", se=F)
% 
% #grid.arrange(day9_plt4, day9_plt3, ncol = 2)
% @


\subsection{Constancy of variance}
\label{sec:constancy-variance-assumption}

% \emph{Homoscedasticity} is one of the trickier regression assumptions to think about: 

Linear regressions assume not just that the errors  \(\epsilon_i\) are normally distributed, but that the variance of the normal distribution is the same across all values of the predictor.
%\footnote{This is also called the \emph{homogeneity} or \emph{homoskedaisticity} assumption, which you may be familiar with from ANOVAs.}% (a.k.a.\ `homoskedasticity').

For example, in our model of VOT as a function of  three predictors (\ttt{vot\_mod\_1}: Section~\ref{sec:vot-ex-1}), it is assumed that the amount of (remaining) variability in VOT is similar for voiced and voiceless stops, for pre-vocalic voiced stops and pre-consonant voiceless stops (\ttt{voicing}=\tsc{voiced} \& \ttt{cons\_cluster}=\tsc{no} vs.\ \ttt{voicing}=\tsc{voiceless} \& \ttt{cons\_cluster}=\tsc{yes}), and so on.
% a following consonant   reaction time as a function of subject age and word frequency, it is assumed that the amount of variability in reaction time is similar for old speakers and young speakers, for high frequency words and young speakers, for observations of high frequency words for old speakers, and so on.

Constancy of variance can be examined with plots of the fitted values or a predictor ($x$-axis) versus the residuals ($y$-axis). %(That is, $\hat{y}$ or one of the $x_i$ on the $x$-axis, and  $\epsilon$ on the $y$-axis.) 
If constancy of variance holds, then the residuals are uncorrelated with predictor values and with the fitted values, and there should be a constant spread (variance) of residual values ($y$-axis) for each fitted or predictor value ($x$-axis). Thus, it is common to make diagnostic plots of residuals versus fitted values and versus predictors. 
%as dia (The fitted values-residuals plot is one of the diagnostic plots that shows up if you \texttt{plot(mod)} in R, where \texttt{mod} is a fitted model.) Th
The desired pattern is a flat line, with the same variance for different $x$-axis values.

In the VOT example, a fitted value-residual plot (Figure~\ref{fig:resid_plot_1} middle) shows non-constant variance: higher values of \(\hat{y}\) show higher variance in the residuals. Somtimes the relationship is instead examined with a `scale-location plot', where fitted values are plotted against plotted against $\sqrt{|\epsilon_i|}$
%$y$-axis is the square root of residuals 
%The constant-variance assumption can also be examined more directly with a \textbf{scale-location} plot 
(Figure~\ref{fig:resid_plot_1} right).
%where fitted values $\hat{y}_i$ are plotted against $\sqrt{|\epsilon_i|}$. 
For normally-distributed errors, how vertically spread-out the points are (`scale', or `spread') should be the same for all values of $\hat{y}$ or any predictor, and the average spread should not change---so we want to see a horizontal line \citep[][\S6.1.1]{faraway2015linear}.
% seems like too much
%\footnote{The technical justification is: if the errors $e_i$ were normally distributed, the $|e_i|$ would follow a half-normal distribution (for any $\hat{y}$ or $x_i$ value). It's hard to judge a half-normal distribution by eye because it's very right-skewed, so $\sqrt{|\epsilon_i|}$ is examined to reduce the skew \citep[][\S6.1.1]{faraway2015linear}.}

Non-constant variances arise frequently in some kinds of linguistic data, such as count data (e.g.,\ from lexical statistics, typology, dialectology), which is one motivation for analyzing them using more complex models than linear regression (e.g.,\ Poisson regression: \citealp[][35]{baayen2008analyzing}; \citealp[][ch.\ 13]{winter2019statistics}; \citealp[][ch.~5]{faraway2016extending}).
%% REMOVED 12/21 because actually non-constant variance is discussed in Winter, Gries, Eddington...
%% Notably Winter uses it to motivate Poisson reression for count data.
%% FUTURE: say something more intelligent here -- the fact that we're usually using linear regression suggests not paying much attention to heteroskedasticity..
%% This is one motivation for other GLMs: Poisson regression for counts (WInter chapter, Baayen example), and Gamma or Inverse-gamma distirbutions for reaction times (see MS Slack w/ Jacob around Nov 1, 2021 (Frontiers paper on why gamma transform))
%%
%%
%they are endemic in other kinds of data, such as in economics and ecology, where dealing with non-constant variances is an important part of data analysis \citep[e.g.,][\S5.1]{zuur2007analyzing}.  Non-constant variances are discussed less frequently than 
 %such as economic data, heteroscedasticity is so common that dealing with it is a primary concern in statistical analysis. Heteroscedasticity is discussed less frequently than 
 % other regression assumptions for linguistic data, but it is unclear (to me) whether this is because non-constant variances 
 % %heteroskedasticity 
 % are less common than in other types of data or just has not been focused on by language scientists.

%\hypertarget{effect-and-solution-1}{%
%\subsubsection{Effect and solution}\label{effect-and-solution-1}}

In general, estimates of least-squares coefficients in the presence of non-constant variance are unbiased, but standard errors will be under- or over-estimated \citep[][\S6.4]{chatterjee2012regression}. This means that confidence intervals will be too narrow/wide and \(p\)-values too low/high.  
%As always, we are more worried about being anti-conservative (here, standard errors too low), which can lead to Type I errors.

An important reason to check for constant variance is that it can suggest changes which should be made to the data and model.  
This is because one can often correct for non-constant variance by transforming the response or predictors,  including a `missing' interaction between predictors or nonlinear effect of a predictor, etc.
%including a `missing' predictor 
(\citealp[][chap.~4]{chatterjee2012regression}, \citealp[][chap.~9]{faraway2015linear}, \citealp[][\S5.1]{zuur2007analyzing}). For example, we will see below (Section~\ref{sec:vot-xfm-example}) that log-transforming the response  will resolve the non-constant variance in our VOT model.
%and the following example shows how non-constant variance can diagnose a missing interaction.

% \subsubsection{Example}
% 
% In addition to the fitted-value/residual plot we can make predictor/residual plots. Figure~\ref{fig:vot-missing-ixn} left shows the distribution of residuals as a function of \ttt{voicing}; the variance is clearly lower for \tsc{voiced} than for \tsc{voiceless} stops.
% 
% <<vot-missing-ixn, out.width='45%', fig.width=default_fig.width*.45/default_out.width>>=
% augment(vot_mod_1) %>% ggplot(aes(x=voicing, y=.resid)) + geom_violin(aes(fill=voicing, color=voicing)) + geom_jitter(size=0.5, width=0.1, height=0) + xlab("Consonant voicing") + ylab("Residuals")
% vot_michael %>% ggplot(aes(x=speaking_rate, y=vot)) + geom_smooth(aes(color=voicing), method='lm') + geom_point(aes(color=voicing)) + xlab("Speaking rate (syll/sec)") + ylab("VOT (msec)")
% @
% 
% One reason for such a pattern could be that a predictor affects \tsc{voiced} and \tsc{voiceless} stops differently---an interaction---in a way not accounted for by the model. More variance would then be `unexplained' for \tsc{voiceless} stops because XX  The pattern would thus spur us to examine empirical plots for possible interactions between \ttt{voicing} and other predictors, leading to  Figure~\ref{fig:vot-missing-ixn} (right), where we see that speaking rate affects \tsc{voiceless} stops much more than \tsc{voiced} stops.  Once this pattern is accounted for in the model by including a \ttt{voicing:speaking\_rate} interaction, the heteroskedasticity goes away.
%% Unfortunately that last part isn't true!

% 
% \begin{boxedtext}{Broader context: Non-constant variance: methods and lingusitic data}
% 
% There are some kinds of linguisti
% Non-constant variances arise frequently in some kinds of linguistic data, such as from lexical statistics \citet[][p. 35]{baayen2008analyzing}, or any situation where Zipf's law kicks in.  For example, in the \ttt{vot\_michael} data, higher frequency words have lower variance in VOT (Figure\ XX); this is ultimately because there are many low-frequency words and few high-frequency words in English. As such, any regression predicting VOT will tend to have non-constant residuals as a function of word frequency, which means the standard errors/$p$-values for the effect of word frequency is incorrect.
% 
% In endemic in some types of data, such as from lexical statistics (\citet{baayen2008analyzing}, p.~35). In other types of data, such as economic data, heteroscedasticity is so common that dealing with it is a primary concern in statistical analysis. Heteroscedasticity is discussed less frequently than other regression assumptions for linguistic data, but it is unclear whether this is because heteroscedasticity is less common than in other types of data or just has not been focused on by language scientists.
% \end{boxedtext}

% One can often correct for heteroscedasticity by using various transformations of the response and predictors to get better estimates (\citet{chatterjee2012regression}, Ch. 4). For example, in the \texttt{halfrhyme} example, it turns out that a stronger effect of \(X\) on \(Y\) (lower \(p\)-value) can be detected once variance is stabilized.

\section{Problems with the model}
\label{sec:lr-model-problems}

Recall that linear regression is a type of `linear model' (Section~\ref{sec:lin-mod-term}), where the relationship between $y$ and $k$ predictors is:
 \begin{equation}
 E(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +
   \cdots + \beta_p x_k
   \label{eq:structural-1}
 \end{equation}
Here, $E(y)$ is the \emph{expectation} of $y$---its average value across many trials. 

This is the basic relationship  assumed between the response and predictors in linear regression (for an `average observation'), abstracting away from the error. Above we considered problems related to the error term.  This section describes problems 
related to 
%which can result when
equation~(\ref{eq:structural-1}) not being a good model for the data.


% It is often convenient to write out the \emph{expectation} of $y$ for a regression equation, written $E(y)$---its average value across many trials.  In this case:
% \begin{equation}
% E(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +
%   \cdots + \beta_p x_p
%   \label{eq:structural-1}
% \end{equation}
% 
% This is sometimes called the \emph{structural part} of the model---the basic relationship which is being assumed between the response and predictors (abstracting away from individual observations and error terms---as in our definition of linear models, equation~\ref{eq:gen-linreg1}).

\subsection{Bounded or discrete $y$}
\label{sec:bounded-discrete-y}

% At a high level, the model (Eq.~\ref{eq:structural-1}) has to make sense for the data and research questions at hand; this broad assumption is often not even stated.  Some examples follow.
% 
% \paragraph{Bounded or discrete $y$}

%For example, 
If you are modeling data where $y$ is constrained to be between lower and/or upper bounds, fitting a linear regression to the data doesn't make conceptual sense a priori, because
%Technically, the reason is that 
equation \eqref{eq:structural-1} can give non-nonsensical values of $y$ (outside the bounds) for new predictor values. 

For example, suppose we'd like to predict the proportion of subjects who correctly identified each word 
%in the lexical decision task
($y$: \ttt{CorrectLexDec}$/30$) for the \ttt{english} data.  A linear regression model wouldn't be appropriate here because $y$ can only be between 0 and 1, but this model 
%linear regression model 
would allow $y$ to take on values outside $[0,1]$ for some predictor values.  For example, Figure~\ref{fig:correct-ld} shows the relationship between $y$ and a word's subjective familiarity ($x$: \ttt{Familiarity}).  A simple linear regression of $y$ on $x$ predicts $y>1$ even for values of $x$ in the data the model is fitted to. It makes sense that $x$ would strongly affect $y$ (more familiar words are more often identified correctly), %\ttt{Familiarity} would strongly affect \ttt{CorrectLexDec} would strongly affect a word's likelihood of being identified correctly,
but a linear regression is not an appropriate model of the relationship.

<<correct-ld, echo=FALSE, out.width='55%', fig.width=default_fig.width*.55/default_out.width, fig.cap='An inappropriate linear regression for the \\ttt{english} data, which predicts impossible $y$ values (proportion of subjects who correctly identified a word can be at most 1).'>>=
library(languageR) ## makes the 'english' dataset available
ggplot(aes(x = Familiarity, y = CorrectLexdec / 30), data = english) +
  ## points slighly jittered to make line more visible
  geom_jitter(alpha=0.1, size=0.5, width=0.01, height=0.01) +
  geom_smooth(color='black', method = "lm", size=1) +
  scale_y_continuous(breaks = seq(0,1,.2)) +
  labs(y="Prop. correctly identifying")
@

A more common case is where the fitted model predicts valid $y$ values for data the model was fitted on, but \textbf{would} predict invalid $y$ values for new data (with predictor values not in the training data).

This kind of problem can be detected by just thinking about the data, or making exploratory plots of $y$ against possible predictors.
%%$x$ 
%(which is a good idea before fitting a regression model, regardless). 
Sometimes it's possible to get around the issue by transforming $y$ (an example is in Section~\ref{sec:vot-xfm-example}); sometimes a different method from linear regression is needed (logistic regression, in this example).

% FUTURE: any reincorporation of this part, originally in first section of last chapter:

% It is important to remember that the \textbf{validity of a regression analysis depends on the assumptions of the data and model}.

% For example, if you are modeling data where \(Y\) has a maximum value, fitting a simple linear regression (= a line) to this data doesn't make conceptual sense, a priori. Here's an example using the \texttt{english} dataset:


% Because \texttt{CorrectLexdec} has a maximum value of 30, fitting a line doesn't make sense---the predicted value when \texttt{Familiarity}=6 is above 30, but this is impossible given the definition of \texttt{CorrectLexdec}.
% 
% 
% Note that R almost never checks whether your data and model meet regression analysis assumptions, unlike other software (e.g.,~SPSS, sometimes).
% 
% \hypertarget{linear-regression-assumptions}{%
% \section{Linear regression assumptions}\label{linear-regression-assumptions}}
% 
% Up to now, we have discussed regression models without worrying about the assumptions that are made by linear regression, about your data and the model. We will cover six main assumptions, the first four have to do with the form of the model and errors:
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   Linearity
% \item
%   Independence of errors
% \item
%   Normality of errors
% \item
%   Constancy of errors (\emph{homoscediasticity})
% \end{enumerate}
% 
% followed by two assumptions about the predictors and observations:
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \setcounter{enumi}{4}
% \item
%   Linear independence of predictors
% \item
%   Observations have roughly equal influence on the model
% \end{enumerate}
% 
% We'll discuss each in turn.
% 
% Our presentation of regression assumptions and diagnostics is indebted to Chapters 4, 6 and 9 of \citet{chatterjee2012regression}, where you can find more detail.
% 
% \hypertarget{visual-methods-1}{%
% \subsection{Visual methods}\label{visual-methods-1}}
% 
% Visualization is crucial for checking model assumptions, and for data analysis in general. A famous example illustrating this is \emph{Anscombe's quartet}: a set of four small datasets of \((x,y)\) pairs with:
% 
% \begin{itemize}
% \item
%   The same mean and variance for \(x\)
% \item
%   The same mean and variance for \(y\)
% \item
%   A correlation(\(x\), \(y\)) = 0.816
% \item
%   The same regression line (\(y = 3 + 0.5\cdot x\))
% \end{itemize}
% 
% in each case---and yet the datasets show qualitatively different patterns, as can be seen by plotting \(y\) against \(x\):
% 
% %\includegraphics{images/anscombe.png}
% 
% (Source: unknown, but definitely taken from somewhere)
% 
% With more than one predictor it becomes difficult to check regression assumptions by just plotting the data, and visual methods such as \emph{residual plots} (presented below) are crucial.

By the same logic, it isn't technically valid to model $y$ which can only take on a finite set of values (e.g.,\ a 1--7 Likert scale), as opposed to a continuous variable (e.g.,\ VOT), with a linear regression, because $y$ is bounded (same problem as above) and the linear regression model predicts that $y$ can lie between the discrete values.
%instead  % This is the linear regression model 
%predicts that $y$ can lie between the discrete values, or be larger than the minimum/maximum value (the same problem as above).   
%The latter problem is the same as above; 
The latter problem rapidly diminishes as the number of levels increases.  It is a reasonable approximation to model $y$ as continuous the more the distribution of $y$ looks continuous: the number of levels is large (10+, say), and most levels are actually used \citep[][\S6.5]{gelman2007data}, e.g.,\ responses are not clustered near the highest and lowest levels. Otherwise a method that treats $y$ as `ordinal' should be used \citep{liddell2018analyzing}.
% 
% Various tutorials imply that a
% A commonly-used rule of thumb is that a 
% response variable $y$ `can be' treated as continuous if it takes on at least 7 levels, but I don't know if there is a good justification for this exact value or under what conditions it holds. Certainly it seems less justified the more $y$'s distribution is concentrated at the highest and lowest levels.

%% FUTURE: is there actual work on this somewhere to cite for learning more? surely it depends on the distribution of the variable.  Didn't find much in a search through Harrell, but didn't go into references he gives on ordial models.

\subsection{Linearity}
\label{sec:linearity-assumption}

A primary assumption of a linear regression model is that the relationship between the response and the predictors is indeed linear---in the right-hand side of equation \eqref{eq:mlr-1}, the predictors can be each multiplied by a constant and added together to get their effect on $y$.  While seemingly obvious, this assumption is very important---modeling a curve as a line can lead to serious errors.

%if it is violated, the model's predictions can be in serious error. 
%The linearity assumption can be violated in two ways.  

\paragraph{Non-linearity in a single predictor}

% removed -- just confusing.
% The linearity assumption holds trivially for a categorical predictor (factor) $x_1$ by itself---there is no sense in which $x_1^2$ can be added to the model---though there may be missing interactions with $x_1$ (above). 
% %by themselves--- but there may be missing \textbf{interactions} involving factors, in which case the though there may be missing interactions involving factors, discussed below 

The linearity assumption can be  a concern for any continuous predictor $x_i$---intuitively, its effect on $y$ may not look like a line.  Such nonlinearities often show up in diagnostic plots of one or more $x_i$ versus residuals.
%another reason these are good to examine as part of model validation  
For example, Figure~\ref{fig:rate-voicing-resids} (left) plots the distribution of residuals for \ttt{vot\_mod\_1}, as a function of two predictors---\ttt{speaking\_rate} and \ttt{voicing}. The non-linear trends in each panel suggest there may be a nonlinear effect of \ttt{speaking\_rate}, while the differing trends in the two panels suggest a possible omitted \ttt{speaking\_rate}:\ttt{voicing} interaction.   The existence of nonlinearity and an interaction with \ttt{voicing} is also suggested by the empirical plot in Figure~\ref{fig:rate-voicing-resids} (right).
You can confirm (Exercise~\ref{ex:vot-missing-terms}) that after adding these terms to the model, the residual plot looks better.

<<rate-voicing-resids, echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Residuals for \\ttt{vot\\_mod\\_1}  as a function of predictors \\ttt{speaking\\_rate} and \\ttt{voicing} (left), and \\ttt{vot} as a function of these two predictors for the \\ttt{vot\\_michael} data (right), with nonlinear smoothers (LOESS) and 95\\% CIs.'>>=
vot_mod_1 %>%
  augment() %>%
  ggplot(aes(y = .resid, x = speaking_rate)) +
  geom_point(alpha=.5, size=0.5) +
  geom_smooth(color='black') +
  facet_wrap(~voicing, labeller = label_both) +
  labs(x="Speaking rate", y="Residuals")
vot_mod_1 %>%
  augment() %>%
  ggplot(aes(x = speaking_rate, y = vot)) +
  geom_smooth(aes(color = voicing, fill = voicing)) +
  geom_point(aes(color = voicing), size = 0.5, alpha=.5) +
  scale_color_grey() + scale_fill_grey() +
  labs(x="Speaking rate", y="VOT") +
  theme(legend.position=c(0.85,0.85))
@

A closely related technique is to make \emph{added-variable} plots 
%(a.k.a.\ `partial regression'), 
which isolate the effect of each $x_i$ after all other predictors are controlled for (e.g.,\ \citealp[][\S6.3]{faraway2015linear}; \citealp[][\S8.2]{fox2019r}).

More simply, linearity can be partially checked by making plots of $y$ as a function of each (continuous) predictor $x_i$, which is a good idea to do before statistical analysis in any case.  Since you'll also typically want to examine relationships between the predictors themselves as well to assess collinearity (Section~\ref{sec:linear-independence-of-predictors}), it is efficient  to make \emph{pairwise plots} for each pair of a set of variables.
%\footnote{We make such plots using the \ttt{ggpairs} function from the (tidyverse) \ttt{GGally} package (code not shown, but in the source files). A nice (non-tidyverse) function for this purpose is \ttt{pairscor.fnc}, from \ttt{languageR}, which has produced plots in hundreds of published analyses of linguistic data.} 
It helps to include a nonlinear smoother---a smooth curve interpolated based on nearby observations, which we describe in a later chapter (Section~\ref{sec:nonlinear-smoothers})---in each plot.  If a line is not compatible with the 95\% CI of this curve (meaning you could draw a straight line within the CI), 
%curve does not look like a line, 
any effect of $x_i$ on $y$ may not be linear.

<<pairwise-1, echo=FALSE, fig.asp=.6, out.width='90%', fig.width=default_fig.width*.9/default_out.width, fig.cap='Pairwise plots of several variables from the \\ttt{english} dataset, with nonlinear (GAM) smoothers and 95\\% CIs (lines/shading), and Pearson correlation for each pair. Diagonal panels are empirical distributions of each variable.'>>=
# hi
library(GGally)
english_young %>%
  select(RTlexdec, RTnaming, WrittenFrequency, Familiarity) %>%
  ggpairs(., lower = list(
    continuous = wrap("smooth_loess", alpha = 0.1, size = 0.1)
    )) + 
    theme(strip.text.y = element_text(angle=0, hjust=0))
@

For example, Figure~\ref{fig:pairwise-1} shows pairwise plots between two possible responses (\ttt{RTlexdec}, \ttt{RTnaming}) and two possible predictors (\ttt{WrittenFrequency}, \ttt{Familiarity}) for the \ttt{english} data for \tsc{young} speakers.    The relationship between \ttt{RTnaming} and each predictor looks plausibly linear, while the relationship between \ttt{RTlexdec} and each predictor looks clearly non-linear---for lower and higher-frequency words, the decrease in reaction time `levels off'. 

% 
% \hypertarget{example-10}{%
% \subsubsection*{Example}\label{example-10}}
% \addcontentsline{toc}{subsubsection}{Example}
% 
% Consider relative pitch, intensity, and duration in \protect\hyperlink{altdata}{the \texttt{alternatives} dataset}.
% 
% We can make \emph{pairwise plots} of these variables using the \texttt{pairscor.fnc()} function \texttt{languageR}, to see if any of these variables might be a function of the other two.
% 
% \begin{quote}
% \textbf{Questions}:
% 
% \begin{itemize}
% \tightlist
% \item
%   Is this the case?
% \end{itemize}
% \end{quote}
% 
% Let's examine the relationship between realtive duration and relative intensity more closely:
% 
% <<>>=
% alt %>% ggplot(aes(rduration, rintensity)) + 
%   geom_point()
% @
% 
% We can try to fit a line to this data, but if we compare to using a nonlinear smoother, it seems clear that the relationship is not linear:
% 
% <<message=FALSE>>=
% alt %>% ggplot(aes(rduration, rintensity)) + 
%   geom_point() + geom_smooth(col='red', se=F) +
%   geom_smooth(method="lm", col="blue", se=F)
% @
% 

% We address solutions below.

% 
% FUTURE: re-incorporate any of this? 
% % 
% % It should be noted tthat linearity is an assumption that inearity is an assumption that can never be exhaustively checked for multiple regressions.  Plots of $y$ (or residuals) versus the predictors may be misleading because nonlinearity might only become apparent in plots of several predictors simultaneously $y$ is plotted as a function of several predictors (simultaneously).  More commonly, a plot of $y$ versus any one predictor $x_i$ can be misleading because
% 
% - So empirical plots should be taken as one step, but not definitive??

% \hypertarget{interim-summary}{%
% \subsection{Interim summary}\label{interim-summary}}
% 
% \begin{itemize}
% \item
%   \textbf{Linearity}
% 
%   \begin{itemize}
%   \item
%     Serious violation if not met.
%   \item
%     Fit data with non-linear trend (e.g.,~quadratic)
%   \item
%     Transformed predictor/response to normality (e.g.,~log-transform)
%   \end{itemize}
% \item
%   \textbf{Independence of error}:
% 
%   \begin{itemize}
%   \tightlist
%   \item
%     In linguistic data: use mixed-effects regression\footnote{Or another method that accounts for non-independence of errors, such as repeated measures ANOVA.}
%   \end{itemize}
% \item
%   \textbf{Normality of errors}:
% 
%   \begin{itemize}
%   \item
%     Not too serious violation if not met, but may signal issues with model/data
%   \item
%     Remove outliers; transform \(X\)/\(Y\) to normality
%   \end{itemize}
% \item
%   \textbf{Constancy of variance}:
% 
%   \begin{itemize}
%   \item
%     Not commonly checked in linguistic data
%   \item
%     Leads to uncertain regression estimates
%   \item
%     Transform predictor/response to normality
%   \end{itemize}
% \end{itemize}

\begin{boxedtext}{Broader context: Missing interactions as non-linearity}
\label{box:missing-interactions}

In addition to 
%`Non-linear effects' usually refers to
nonlinear effects of continuous predictors,
%(discussed in the text),   But 
the linearity assumption also covers another common case: 
whenever  the effect of a predictor $x_1$ depends on the value of $x_2$---that is, there is an interaction---omitting the interaction term is a linearity violation.  To see why, recall that the models with and without an interaction term would be:
\begin{eqnarray*}
y & = & \beta_0 + \beta_1 x_1 + \beta_2 x_2 \\
y & = &\beta_0 + \beta_1 x_1 + \beta_2 x_2  + \beta_3 x_1 \cdot x_2
\end{eqnarray*}
The existence of an interaction means that the second model is the correct one, with $\beta_3 \neq 0$, so $x_1$ and $x_2$ do not linearly predict $y$---the first model is incorrect. Defining $x_3 = x_1 x_2$, the second model satisfies linearity---$y$ is a linear function of $x_1$, $x_2$, and $x_3$.

The existence of missing interactions can be assessed with exploratory plots (of $y$ as a function of each pair of predictors) or by variable selection methods (Section~\ref{sec:variable-selection}).  How much to look for `missing' interactions is a  tricky subject in practice, because the number of possible interactions increases exponentially with the number of predictors, but every new term considered for inclusion in a regression model carries potential benefits (Section~\ref{sec:omitted-variable-bias})  and risks (spurious effects).

\end{boxedtext}


\section{Transformations}
\label{sec:transformations-problems}

We take a detour from problems to discuss a type of solution.  Transformations of the response or predictors ($y$, the $x_i$) can be useful to address violations of model assumptions, or for interpretability of model coefficients.

\subsection{Nonlinear effects: Polynomials}
\label{sec:nl-effects-polynomials}

The most obvious example is for the kind of nonlinear effect of a predictor shown just above. This kind of nonlinear relationship {can} be included in a linear regression model---it just has to be written as a sum of linear terms.  For the \ttt{RTlexdec}$\sim$\ttt{WrittenFrequency} relationship, for example, it turns out the nonlinear relationship is decently approximated by a cubic polynomial.

%(Exercise FUTURE-incommentshere).

% (Figure~\ref{fig:nonlinear-ex1})
% <<nonlinear-ex1, echo=FALSE, fig.cap='Nonlinear smoother (GAM: blue) and cubic polynomial (red), with 95\\% CIs (\\ttt{english} data, \\tsc{young} speakers).'>>=
% english_young %>% ggplot(aes(x=WrittenFrequency, y=RTlexdec)) + geom_point(size=0.5) + stat_smooth(method='lm', formula = y ~ poly(x,3), col='red') + geom_smooth()
% @
So the relationship can be captured in a {linear} regression with three predictors by setting $x_1$, $x_2$, and $x_3$ to be frequency, frequency$^2$, and frequency$^3$.  We could fit this model
%in R 
as follows:

<<eval=FALSE>>=
lm(RTlexdec ~ WrittenFrequency + I(WrittenFrequency^2) +
    I(WrittenFrequency^3), english_young)
@

(Here, the \verb'I(x^2)' notation is used because \verb'x^2' would be interpreted as \ttt{x + x:x}: Section~\ref{sec:ixn-example}.)


%\ttt{WrittenFrequency}, its square, and 
% \begin{align*}
% x_1 = \ttt{WrittenFrequency}, x_2 = \ttt{WrittenFrequency}^2 \\
% x_3 = \ttt{WrittenFrequency}^3.
% \end{align*}

% In particular, it looks like there is a quadratic trend. This means that we can in fact fit a linear regression, we just need to include coefficients for both \texttt{rduration} and its square, like so:
% 
% <<>>=
% mq <- lm(rintensity ~ rduration + I(rduration^2), alt)
% summary(mq)
% @

We will cover nonlinear effects of predictors in detail later (Section~\ref{sec:nonlinear-effects}). The  point here is that a model of \ttt{RTlexdec} with just \ttt{WrittenFrequency} as a predictor would violate the linearity assumption, while a model with squared and cubed terms might not.
% \protect\hyperlink{nonlinear-effects}{a later chapter}. The important point here is that a model with just \texttt{rduration} as a predictor would have violated the linearity assumption, but a model with both \texttt{rduration} and \texttt{rduration\^{}2} as predictors doesn't (arguably).

\subsection{Transforming to normality}
\label{sec:transforming-to-normality}

Normality of the distribution of the response and predictors ($y$ and the $x_i$) is not an assumption of linear regression. This is a common misconception.
%perhaps because normality is an assumption of other basic statistical inference tools, such as \(t\)-tests.

However, there is still good reason to be circumspect if $y$ or (continuous) $x_i$  are not normally distributed, because this can often lead to violations of regression assumptions. This is why it is often useful to transform $y$ or $x_i$ to normality---to fix various problems covered so far (normality of errors, constant variance, linearity, etc.).  
%Because non-normality of $y$ or $x_i$ can easily lead to violations of regression assumptions, it is sometimes recommended to transform them to normality just to be safe. 
Transforming one or more variables to normality makes it less likely that a regression assumption will be violated, but also changes the interpretation of the transformed variable(s), which may make it harder to interpret the model's results.

For linguistic data, logarithmic transformations are often useful when working with skewed distributions, because many common variables 
%of linguistic data 
are roughly \emph{log-normally} distributed, meaning the log-transformed variable is normally distributed.  Some examples are lexical statistics (e.g.,\ word frequency), reaction times,
%(e.g.,\ lexical decision or naming latencies),
or duration measures in phonetics (e.g.,\ VOT, vowel duration).  For example,  Figure~\ref{fig:freq-transform} shows the distribution of word frequencies in the \ttt{english} dataset before log-transforming ($\sim$log-normal) and after transforming ($\sim$normal). 

Other transformations besides log are also used: reaction times are sometimes inverse or inverse-log-transformed (1/RT, log(1/RT)), and durations are sometimes square root-transformed.  

For visualization of transformed variables,  it is often more interpretable to make plots showing untransformed values on a transformed scale,  as in Figure~\ref{fig:freq-transform} (center)---word frequencies are shown in words-per-million, but the $x$-axis is log-transformed.

<<freq-transform, echo=FALSE, out.width='30%', fig.asp=.85, fig.width=default_fig.width*.3/default_out.width, fig.cap='Distirbution of \\ttt{WrittenFrequency} for words in the \\ttt{english} dataset, before (left) and after log-transforming frequency, with $x$-axis in log (center) and original (right) scale.'>>=
## use just english_young to get each lemma only once
english_young <- english_young %>%
  mutate(WrittenFrequency_raw = exp(WrittenFrequency))

english_young %>%
  ggplot(aes(x = WrittenFrequency_raw)) +
  geom_histogram(bins = 50) +
  xlab("Frequency\n(words/million) (raw)")

english_young %>%
  ggplot(aes(x = WrittenFrequency_raw)) +
  scale_x_log10() +
  annotation_logticks(sides = "b") +
  geom_histogram(bins = 50) +
  xlab("Frequency\n(words/million) (raw)")

english_young %>%
  ggplot(aes(x = WrittenFrequency)) +
  geom_histogram(bins = 30) +
  xlab("Log-transformed\nfrequency")
@

%Again, the goal is never to obtain a normally-distributed $y$ or $x_i$ for its own sake, but to make violations of regression assumptions less likely.

\subsubsection{Example: Transforming a predictor}
\label{sec:predictor-transform-example}

Consider a small ($n=40$) subset of the \ttt{english} data, which we will use in a few examples:
<<>>=
english_young <- filter(english, AgeSubject == "young")
set.seed(2900)
english_40 <- sample_n(english_young, 40)
@

This subset simulates a small-scale version of the \ttt{english} experiment where only 40 words have been examined, for \tsc{young} speakers. It is easier to see the effects of problems which can affect linear regressions for smaller samples.

Consider a simple linear regression of \ttt{RTnaming} as a function of raw written frequency, before and after log-transforming:

<<>>=
english_40$WrittenFrequency_raw <- exp(english_40$WrittenFrequency)
slr_mod_4 <- lm(RTnaming ~ WrittenFrequency_raw, data = english_40)
slr_mod_5 <- lm(RTnaming ~ WrittenFrequency, data = english_40)
@
<<raw-freq-bad-plot, echo=FALSE, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='\\ttt{RTnaming} as a function of untransformed word frequency (\\ttt{WrittenFrequency\\_raw}: left) and log-transformed word frequency (\\ttt{WrittenFrequency}: right),  with line of best fit (blue) and 95\\% CI.'>>=
# hi
english_40 %>% ggplot(aes(WrittenFrequency_raw, RTnaming)) +
  geom_point(size = 0.75) +
  xlab("Raw written frequency") +
  geom_smooth(color='black', method = "lm") +
  ylab("Naming reaction time\n(log msec)")

english_40 %>% ggplot(aes(WrittenFrequency, RTnaming)) +
  geom_point(size = 0.75) +
  xlab("Log written frequency") +
  ylab("Naming reaction time\n(log msec)") +
  geom_smooth(color=default_line_color, method = "lm")
@
The first regression corresponds to the line of best fit in Figure~\ref{fig:raw-freq-bad-plot} (left).  It is clear that fit is poor, and any effect of (raw) frequency on reaction time may be non-linear.
%By superimposing a nonlinear smoother (green), it is clear that any effect of (raw) frequency on reaction time is non-linear. 
Making other diagnostic plots (e.g.,\ \ttt{plot(slr\_mod\_4)}) will show a host of other problems (non-constant variance, etc.).  All these issues are resolved by log-transforming frequency, so that$x$ = \ttt{WrittenFrequency}: the effect of $x$ on $y$ looks closer to linear (Figure~\ref{fig:raw-freq-bad-plot} right), and so on.
%\ttt{RTnaming}-\ttt{WrittenFrequency} panel in Figure~\ref{fig:pairwise-1}), and so on.

\subsubsection{Example: Transforming the response}
\label{sec:vot-xfm-example}

The VOT model fitted in Section~\ref{sec:vot-ex-1} has a number of problems, including non-normal residuals and non-constant variance of residuals.  In addition, there is a conceptual problem with the model because $y$ is bounded---VOT can only be positive, by definition (Section~\ref{sec:vot-dataset})---but in a model where $y$ is \ttt{vot} nothing stops negative $y$ from being predicted.

There are several reasons to think these issues can be resolved by log-transforming $y$ (or square root-transforming, etc.):
\begin{enumerate}
\item A model predicting $y$ = log(VOT) would allow $y$ to be positive or negative but can only predict VOT$>$0, addressing the conceptual problem.

\item $y$ itself is  not normally distributed, within \tsc{voiced} or \tsc{voiceless} stops (Figure~\ref{fig:resid_plot_2}), and log-transforming often brings right-skewed distributions closer to normal.

\item The Q-Q plot of residuals (Figure\ref{fig:resid_plot_1} left) implies a skewed distribution, which can result from a skewed distribution of $y$.

\item The `fan' pattern in the fitted-residuals plot (Figure~\ref{fig:resid_plot_1} center), where variance increases as a function of $y$, often is fixed by log or square root transforming $y$.
% can go
%\footnote{Such transformations are sometimes called `variance stabilizing' for this reason---they (can) result in residual variance which does not grow as a function of $y$ or an $x_i$.}
\end{enumerate}

% unnecessary?
%Points (1) and (2) just require thinking about your data and making empirical plots, while (3) and (4) require some experience with residual plots.

Let's refit the model with $y$ = \ttt{log\_vot}:
<<>>=
## Change the response in the model formula
vot_mod_2 <- update(vot_mod_1, log_vot ~ .)
@

The diagnostic plots now look better (Figure~\ref{fig:resid_plot_4}; compare to Figure~\ref{fig:resid_plot_1}): the Q-Q plot suggests a residual distribution closer to normality,  and the fitted-value and scale-location plots suggest roughly constant variance.\footnote{There is some evidence for non-constant variance which should be investigated further (especially around $\hat{y} = 3$).}

<<resid_plot_4, echo=FALSE, out.width='30%', fig.asp=.75, fig.width=default_fig.width*.3/default_out.width, fig.cap='Diagnostic plots for \\ttt{vot\\_mod\\_2}: Q-Q plot of residuals (left), fitted value-residual plot (middle), scale-location plot (right).'>>=
augment(vot_mod_2) %>% ggplot(aes(sample = .resid)) +
  geom_qq(size=0.5, color='darkgrey') +
  geom_qq_line(color=default_line_color, size=1) +
  xlab("Theoretical quantiles") +
  ylab("Residuals")
augment(vot_mod_2) %>% ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(size=0.5, color='darkgrey') +
  geom_smooth(color=default_line_color, se = F, size=1) +
  geom_hline(aes(yintercept = 0), lty = 2) +
  xlab("Fitted values") +
  ylab("Residuals")
augment(vot_mod_2) %>% ggplot(aes(x = .fitted, y = sqrt(abs(.std.resid)))) +
  geom_point(size=0.5, color='darkgrey') +
  geom_smooth(color=default_line_color, se = F, size=1) +
  xlab("Fitted values") +
  ylab(expression(sqrt(abs("residual"))))
@

\subsection{Interpretation issues}

%It is important to be aware that
The interpretation of every coefficient in a regression model changes when $y$ is transformed.  For example, for the log transformation, the effects of predictors on $y$ now {multiply} rather than add \citep[][\S9.1]{faraway2016extending}.  To see why, consider a simple linear regressions of $\log(y)$ on $x$:
%and (b) $y$ on $x$.  The  model for (a) would be:
\begin{equation*}
\log(y)   = \beta_0 + \beta_1 x  + \epsilon 
\end{equation*}
Exponentiating both sides gives: 
\begin{equation*}
 y   = \exp{\beta_0} \cdot (\exp{\beta_1})^x \cdot \exp{\epsilon}
\end{equation*}

% The model for (b) has a familiar form:
% \begin{equation*}
% \text{Untransformed:} \quad y   = \beta_0 + \beta_1 x  + \epsilon 
% \end{equation*}

In terms of $y$, each non-intercept coefficient $\beta$ in this model has the interpretation `$\exp(\beta)$ is how much $y$ is multiplied by for a unit change in $x$, holding other predictors constant'.  This means that increasing $x$ by 1 will multiply $y$ by a number ($e^\beta$), increasing $x$ by 2 will multiply $y$ by the square of that number, and so on.  When there are multiple predictors, the right-hand side of the model now has one term ($\exp{\beta_i})^{x_i}$) per predictor, which multiply together to predict $y$.\footnote{Interpretation also changes when a continuous $x_i$ is transformed, but only for the coefficients involving that predictor. For example, if we use $\log(x_i)$ instead of $x_i$ as a predictor in a model, the effect of $x_i$ on $y$ is now multiplicative.  The coefficient $\beta_i$ now means not `how much $y$ changes for a unit change in $x_i$, but `how much $y$ changes for a unit change in $\log(x_i)$'---which is equivalent to `when $x_i$ is multiplied by $\sim$1.7'. (Because $e-1 \approx 1.7$.)}

For our example, consider just the estimated \ttt{voicing} coefficient for  our VOT models \ttt{vot\_mod\_1} (response: \ttt{vot}) and \ttt{vot\_mod\_2} (response: \ttt{log\_vot}):
%Comparing the coefficients of \ttt{vot\_mod\_2} with \ttt{vot\_mod\_1}, we see that modeling $y$ instead of $log(y)$ has changed every coefficient's value

<<>>=
coefficients(vot_mod_1)[["voicingvoiceless"]]
coefficients(vot_mod_2)[["voicingvoiceless"]]
@

<<echo=FALSE>>=
coeff1_1 <- coefficients(vot_mod_1)[["voicingvoiceless"]]
coeff2_1 <- exp(coefficients(vot_mod_2)[["voicingvoiceless"]])
@


The interpretation of these coefficients are:
\begin{itemize}
\item  `\tsc{voiceless} stops have VOT \Sexpr{coeff1_1} msec longer than \tsc{voiced} stops'.
\item `\tsc{voiceless} stops have VOT 
\Sexpr{coeff2_1} (= $e^{\Sexpr{coefficients(vot_mod_2)[['voicingvoiceless']]}}$) times higher than \tsc{voiced} stops'.
\end{itemize}

The \ttt{vot\_mod\_2} result is less interpretable, because linguists have almost always thought about VOT in terms of msec and msec differences, not proportions. A \Sexpr{coeff1_1} msec difference is immediately interpretable to a phonetician (it's a big effect), while a \Sexpr{coeff2_1}x increase is not.  So even though \ttt{vot\_mod\_2} is `better' in terms of satisfying regression assumptions, it is `worse' in terms of interpretability.  This is often the situation when $y$ is transformed.
%% FUTURE: add this footnote back in and complete?
%% It seems good somewhere to work through an example of how coeff interpretations
%% change when y transformed.
%\footnote{It is possible to summarize coefficients in a model of a transformed variable (e.g.,\ log(VOT)) on the original scale (e.g.,\ VOT)---as in XX---but these will still mean the same thing as coefficients in a model of $y$.}


% 
% Model 2 is a better model--except for interpretability, because phoneticians have typically thought about VOT in terms of msec, not proportions.  50 msec difference immediatley interpretable; 4.8x is not.  It's possible to summarize coefficients in terms of msec differences, but this has issues.  (Could refer to published work for examples)

% - more on this: faraway (transformations chapter)

\subsection{Centering and scaling predictors}
\label{sec:centering-scaling}

An important aspect of interpreting regression model results is being able to interpret and compare coefficient values.  This is not the case for our models so far because of how the predictors are coded.
%because predictors are on different scales.

To see the issue, consider a model of lexical decision time as a function of three predictors for the \ttt{english} data: word frequency,  the length of the word in letters, and subject age:

<<>>=
# Add to model containing WrittenFrequency and AgeSubject
english_mod_3 <- update(english_mod_2, . ~ . + LengthInLetters)
tidy(english_mod_3)
@

Two issues make the coefficients difficult to interpret.  First, the \textbf{intercept value} is not interesting or meaningful. The intercept here means `predicted reaction time when all predictors are 0', that is `for \tsc{young} subjects, when \texttt{WrittenFrequency} = 0 and \texttt{LengthInLetters} = 0.'  But zero frequency is not an interesting value (it means an extremely low frequency word), and zero length is impossible.  Even without the latter issue (suppose \texttt{LengthInLetters}=0 actually meant `one letter long'), `very low frequency and very short length' describes a word that is not representative of English---shorter words tend to have higher frequencies (in every language), so the intercept describes a very unusual word.  Second, the \textbf{coefficient values aren't comparable}, because the predictors are on different scales.   A unit change in \texttt{AgeSubject} covers 100\% of the data (because 0, 1 = \tsc{young}, \tsc{old}), while a unit change in \texttt{WrittenFrequency} only covers part of the data (range: 0--\Sexpr{max(english$WrittenFrequency)}).  Comparable coefficient values are valuable to assess effect size (Section~\ref{sec:in-context}).

%\hypertarget{solutions}{%
%\subsubsection{Solutions}\label{solutions}}

We can address these two issues by transforming predictors before fitting the model.\footnote{Alternatively one can transform the coefficents after fitting the model, to get `standardized' coefficients.}
%typically dividing each one by the standard deviation of the corresponding predictor); we discuss the first option.
There are a few common ways to `standardize' predictors to address, and
%considering for now just continuous predictors and binary factors (no multi-level factors. 
it is worth being aware of different options because they are all widely used (Box~\ref{box:standardizing}). For clarity of presentation we discuss just a single option which we use going forward, following \citet[][\S4.2]{gelman2007data}:\footnote{For more in-depth discussion of the advantages of centering/standardizing variables, and how it affects model interpretation, see e.g., \citet[][]{schielzeth2010simple,cohen2002applied,dalal2012some}.}
\begin{enumerate}
\item Continuous predictors are centered (subtract the mean) and divided by two standard deviations (2$\sigma$).
\item Binary factors are transformed to 0 and 1, then centered.
\end{enumerate}

This results in a factor with equal numbers of observations for levels \tsc{a} and \tsc{b} (a \emph{balanced} factor) being coded as -0.5 and 0.5; a factor with 25\% \tsc{a} and 75\% \tsc{b} observations being coded as -0.75 and 0.25, and so on.

Because all predictors have been centered, the intercept is now interpretable as the predicted $y$ at the average value of each predictor---which tends to be more meaningful than setting all predictors to 0.  Dividing by 2$\sigma$ makes the effect sizes of coefficients for continuous and binary predictors comparable: as long as the data is not too unbalanced, a unit change now corresponds to a change of 2$\sigma$  for {any} predictor.\footnote{This is because $\sigma \approx 0.5$ for a Bernoulli-distributed variable with $p \approx 0.5$---see Box~\ref{box:bernoulli-se}.  For (very) unbalanced data, $\sigma << 0.5$, so the coefficients for binary predictors still mean `difference between \tsc{a} and \tsc{b}' but their sizes are not comparable with coefficients for continuous predictors.}
%The coefficeints for binary predictors still mean  `difference between levels \tsc{a} and \tsc{b}', which is their most natural interpretation.
%Interpretation of ``unit change of 1'' is similar for continuous and binary predictors \(\Rightarrow\) can compare effect sizes.

We often use \emph{rescaling} in this book to refer to the particular standardization in (1)--(2), following the usage of \citet{gelman2007data} and the \ttt{rescale()} function in the {arm} package, which can be used to do (1)--(2) automatically.  However, this is not standard terminology, and it is always important when writing up results to say exactly how you standardized variables, e.g.,~``continuous variables were standardized by centering and dividing by two standard deviations.'' %(Box~\ref{box:standardizing}).

\begin{boxedtext}{Practical note: Standardizing/normalizing/scaling}
\label{box:standardizing}

`Standardizing' (or `normalizing') predictors is commonly done in a few different ways.
%  \citep[e.g.,][\S7.2]{faraway2015linear}
%is one of many (simple) things in applied statistics that have no fixed definition yet are widely done without explanation. 
%It's useful to be aware of a few widely-used options and their pros/cons (e.g.,\ .

For continuous predictors, it is common to \emph{$z$-score} (center, then divide by 1$\sigma$).
%and less common to center then divide by 2$\sigma$. 
$z$-scoring makes interpretation of coefficients for continuous predictors more intuitive, while dividing by 2$\sigma$ makes the interpretation of binary predictors  more intuitive (`difference between levels \tsc{a} and \tsc{b}') and the effect sizes of binary and continuous predictors comparable.

For binary predictors, a few options are common:
\begin{enumerate}
\item Leave as 0 and 1 (no centering---R default)
\item Change to -0.5 and 0.5 (centering, for balanced data)
\item (1) then subtract mean (centering)
\item (3) then divide by 2$\sigma$ (centering and scaling)
\end{enumerate}
The advantage of (4) is that the coefficients for binary predictors are truly comparable with those of continuous predictors, even for unbalanced data; the disadvantage is that the coefficients no longer are interpretable as `difference between levels \tsc{a} and \tsc{b}' (as for (1)--(3)).  We discuss different ways of coding categorical variables as numeric predictors (`contrast coding') further in Chapter~\ref{chap:practical-regression-topics}.

% Michaela: combined w sentence just above box
% When you report standardization of variables in a paper, you should report the information needed to replicate what you did---not the name of a particular software package or function used, or a term like `standardized' or `rescaled' without explanation. For example, 
% ``continuous variables were standardized by centering and dividing by two standard deviations.''
\end{boxedtext}
%not ``continuous variables were standardized using the \texttt{rescale()} function in the \texttt{arm} package'' or ``continuous variables were rescaled.'' The latter kinds of report are common and unhelpful.  If you want to give a citation, it would be for the practice of dividing by two standard deviations \citep[e.g.,][]{gelman2008scaling}.
%(And you don't need to worry about giving package authors credit for such a simple function.)
%Note also that despite our usage in this book, `rescaling' or `rescaled predictors' are  are not standard terminology.  So in a write-up it is better to refer to `standardized' variables, defining at some point what  `standardized' means.

% 
% \begin{itemize}
% \item \emph{$z$-scoring}: center (subtract the  mean) and divide by the  standard deviation for continuous $x_i$ (Section~\ref{sec:z-scores})
% \item 
% \end{itemize}
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   \emph{z-scoring}: subtract mean (center) and divide by standard deviation
% 
%   \begin{itemize}
%   \item
%     This measure is similar to Cohen's \(d\) for binary predictors (which we'll discuss soon)
%   \item
%     \texttt{scale()} function in R
%   \end{itemize}
% \item
%   Alternative method (\citet{gelman2007data} 4.2):
% 
%   \begin{itemize}
%   \item
%     Continuous predictors: center and divide by \textbf{2} SD.
%   \item
%     Binary predictors: transform to have mean 0 and difference of 1 between values (i.e.,~-0.5/0.5 for balanced data)
%   \item
%     Interpretation of ``unit change of 1'' is similar for continuous and binary predictors \(\Rightarrow\) can compare effect sizes.
%   \item
%     Use \texttt{rescale()} in the \texttt{arm} package.
%   \end{itemize}
% \end{enumerate}
% 
% Method 2, while less standard, has advantages discussed by \citet{gelman2007data}, and we'll use it going forward.

After rescaling predictors, the interpretations of the coefficients are more intuitive:
\begin{enumerate}
\item Intercept ($\beta_0$): the predicted $y$ when all predictors are held at their mean values.
\item Main effect coefficient ($\beta_i$) for continuous  $x_i$: the predicted change in $y$ when $x_i$ is changed by 2$\sigma$, with other predictors held at their mean values.
\item Main effect coefficient ($\beta_j$) for binary  $x_j$: the predicted difference in $y$ between the levels of $x_j$, with other predictors held at their mean values.
\end{enumerate}

Holding other predictors at their mean values in this case (all $x_i$ are continuous or binary) is the simplest example of \emph{marginalizing}---summarizing the effect of one predictor,  averaged across values of other predictors in some way---which we will discuss further for logistic regression (Section~\ref{sec:viz-effects-logistic}, 
\ref{sec:average-marginal-effects}), where this method is particularly useful.

Technically in (2) and (3) we could think of other predictors being `held constant' at any value.
%rather than at their mean values. 
It is useful to use the wording in (2) and (3), where predictors are held constant at mean values, because it carries over to when interactions are included in the model: the main effect $\beta_i$ is then interpretable as the effect of $x_i$ marginalizing over other predictors---even 
%(here, holding them all at their mean values)---even 
in the presence of interactions between $x_i$ and other predictors.



% 
% New interpretations of coefficients:
% 
% \begin{itemize}
% \item
%   \textbf{Intercept}: predicted \(Y\) when all predictors are held at mean value
% \item
%   \textbf{Main effect} coefficients for continuous predictors: predicted change in \(Y\) when predictor changed by 2 SD, with other predictors held at \textbf{mean} values.
% \item
%   \textbf{Main effect} coefficients for binary predictors: difference between the two levels. (Which is the same as 2 SD.)
% \end{itemize}

\paragraph{Example}

As a concrete example of what rescaling does to coefficient interpretations, let's refit the model above (\ttt{english\_mod\_3}) with rescaled predictors.

<<>>=
english_2 <- mutate(english,
  WrittenFrequency = rescale(WrittenFrequency),
  LengthInLetters  = rescale(LengthInLetters),
  AgeSubject       = rescale(AgeSubject)
)
english_mod_4 <- update(english_mod_3, data = english_2)
@

%(By default, the \ttt{rescale} function does (1)-(2).)  

The coefficient tables for the two models are:

<<>>=
# Raw predictors
tidy(english_mod_3)
# Rescaled predictors
tidy(english_mod_4)
@
% 
% Model of \texttt{RTlexdec} as a function of \texttt{WrittenFrequency}, \texttt{LengthInLetters}, \texttt{AgeSubject}, using raw and standardized predictors:
% 
% Before standardizing:
% 
% <<>>=
% summary(m6)
% @
% 
% After standardizing:
% 
% <<>>=
% english2 <- mutate(english,
%        WrittenFrequency = rescale(WrittenFrequency),
%        LengthInLetters  = rescale(LengthInLetters),
%        AgeSubject       = rescale(AgeSubject))
% m7 <-  lm(RTlexdec ~ WrittenFrequency + LengthInLetters + AgeSubject, english2)
% summary(m7)
% @

% FUTURE: make clearer what is lost with standardized predictors -- interpretability. An "average subject" is not intuitive.  JPK p. 141 comments
The interpretations of coefficients in the two models are:
%using raw and rescaled predictors are:

\begin{tabular}{lp{4cm}p{4cm}}
\toprule
Term & raw & rescaled \\
\midrule
\ttt{(Intercept)} & reaction time (RT) for a \tsc{young} speaker and a word with \ttt{WrittenFrequency}=0 and \ttt{LengthInLetters}=0
&  RT for an `average' speaker (across \tsc{old} and \tsc{young}) and `average' word (mean \ttt{WrittenFrequency} and \ttt{LengthInLetters} values) \\

\ttt{WrittenFrequency} & RT difference when \ttt{WrittenFrequency} increased by 1  & 
RT difference for a 2$\sigma$ increase in \ttt{WrittenFrequency}
\\
\midrule
\ttt{LengthInLetters} & \multicolumn{2}{c}{similar to \ttt{WrittenFrequency}} \\
\midrule
\ttt{AgeSubject} & RT difference between \tsc{old} and \tsc{young} speakers & same \\
\bottomrule
\end{tabular}

Note that the \texttt{AgeSubject} coefficient is the same in the two models. Although \texttt{AgeSubject} is a two-level factor in one model and a centered numeric variable in the other model, the regression coefficient captures the difference between the two levels (\tsc{old} and \tsc{young} speakers) in each case.

We can read off the relative importance of the three predictors from the magnitudes of the coefficients in the model using rescaled predictors: \texttt{AgeSubject} \textgreater{} \texttt{WrittenFrequency} \textgreater{} \texttt{LengthInLetters}.


\section{Problems with predictors}
\label{sec:lr-predictor-problems}

\subsection{Non-independent predictors}
\label{sec:linear-independence-of-predictors}

A crucial assumption of linear regression is that the predictors are \emph{linearly independent}, meaning it isn't possible to write one predictor as a linear function of the others. If you can do so, it's impossible to disentangle the effect of different predictors on the response. For example, temperatures in Fahrenheit and Celsius are related by a linear function (F = \(9/5\)C + 32), so F and C are linearly dependent.  Another way of thinking of linear dependence is that in a linear regression where you predict one predictor as a function of the others, the \(R^2\) would be 1.
Linear dependence of predictors will either give a model error or a weird model output if you fit a linear regression in R, because the math to find least-squares estimates of coefficients doesn't work out if there is linear dependence: there is no longer a unique optimal solution. (For example, if the slope of F and C in the optimal model were \(\beta_1 = 2\) and \(\beta_2 = 0\), then a  model using slopes of 0 and \(10/9\) would also be optimal.)


% 
% Using a set of linearly dependent predictors happens surprisingly often, by accident. For example, previous work suggests that three measures of how predictable a word $w_2$ is in context $w_1$ helps explain its pronunciation (e.g.,\ whether it is `reduced') in a $w_1 w_2$ sequence:
% \begin{enumerate}
% \item Conditional probability: $P(w_2|w_1)$
% \item Bigram probability: $P(w_1, w_2)$
% \item Probability of $w_1$: $P(w_1)$
% \end{enumerate}
% 
% We might want to include all of these in a study of how $w_1 w_2$ sequences are produced, as in Tanner-et-al-XX  Furthermore, probabilities
% 
% 
% modeling how the predictability of words affects their pronunciation (e.g.,)

Using a set of linearly dependent predictors happens surprisingly often, by accident.  For example, when modeling VOT we might want to include these predictors, all of which have been found in laboratory studies to affect VOT:
\begin{enumerate}
\item Speaking rate
\item Number of syllables in the word
\item Word duration
\end{enumerate}

Furthermore, we might log-transform each predictor, since all will have right-skewed distributions.  However, if speaking rate is defined as `syllables per second', then the predictors are linearly dependent:
$$
\log(\text{speaking rate}) = \log(\text{\# syllables}) - \log(\text{word duration})
$$

%% Removed -- seems unnecc
% Note that it is possible for one predictor to be completely predictable from other predictors even when $R^2 < 1$---as long as this dependence is not a linear function. Usually it does not make conceptual sense to have several predictors related in this way in a model; for example, even though (1)--(3) in the speaking rate example above are not technically linearly dependent (without log-transforming), it would not make sense to have them all as predictors.  For the special case of nonlinear effects, where multiple related terms are included in a model (e.g.,\ frequency and frequency$^2$: Section~\ref{sec:nl-effects-polynomials}), it does make sense.


%\ttt{frequency}\_rate} and \ttt{speas it does make sense
%
%(e.g.,~VOT and log(VOT)), while in other settings it does (e.g.,~linear and quadratic terms for the same variable).}

% As another example, Tanner-et-al XXX considered 
%However, if speaking rate is defined as ``syllables per second'', then (1) is effectively (2)$-$(3), so the three predictors are linearly dependent.  

% 
% \hypertarget{exercise-3}{%
% \subsubsection*{Exercise}\label{exercise-3}}
% \addcontentsline{toc}{subsubsection}{Exercise}
% 
% Suppose we are trying to predict the duration of the first vowel of every word in a dataset of conversational speech, using these four predictors:
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   log(speaking rate)
% \item
%   log(\# of syllables in the word)
% \item
%   log(duration of word)
% \item
%   log(first syllable duration)
% \end{enumerate}
% 
% where ``speaking rate'' is defined as ``syllables per second''.
% 
% Why are these predictors linearly dependent?

% TDOO: take out, on R2 advice?
\begin{boxedtext}{Broader context: Singularities}
\label{box:linear-dependence}
You may encounter the terms `singular' and `singularity' in R output; this usually means that 
%it is useful to know that when you encounter the terms singular/singularity in R output, it usually means 
some variable can be perfectly predicted from others, and this may be a problem.
The source of these terms is not important, but if you know some linear algebra: a set of predictors is linearly dependent when one column of the \emph{design matrix} \(X\) (where each column is the values of predictor \(i\), across all observations) can be predicted from the others. The matrix \(X^T X\), which is inverted to find the least-squares regression coefficient estimates, then has no inverse, for which the technical term is \emph{singular}; alternatively, there is a \emph{singularity}. 
%You should investigate to figure out why; often a singular fit error signals an important issue with your data, like the VOT example in the text.
\end{boxedtext}


To see what linear dependence looks like in practice, let's simulate 
perfect dependence among predictors for the \ttt{english\_40} data.
%to see what the model output looks like.  
We define a new predictor \ttt{new\_var} which is  the sum of \ttt{WrittenFrequency} and \ttt{Familiarity}, 
%for the \ttt{english\_40} subset, 
and fit a linear regression of lexical decision time (\ttt{RTlexdec}) as a function of these three predictors:

<<output.lines=10:15>>=
bad_mod <- english_40 %>%
  # . notation: use output of mutate as the 'data' model is fitted to
  mutate(new_var = WrittenFrequency + Familiarity) %>%
  lm(RTlexdec ~ WrittenFrequency + Familiarity + new_var, data = .)
summary(bad_mod)
@
The NAs in the row for \ttt{new\_var} indicate there is something wrong. The ``1 not defined because of singularities'' message means the predictors are linearly dependent (Box~\ref{box:linear-dependence})---we have asked the model to use three predictors, but they only contain as much information as two independent predictors.
%(three predictors, minus one `singularity').



% 
% where there are perfect and near-linear dependence among predictors, to see what the R output looks like
% Define a new variable in the \texttt{english} dataset that is the average of \texttt{WrittenFrequency} and \texttt{Familiarity}, then fit a linear regression of \texttt{RTlexdec} as a function of this new variable, \texttt{WrittenFrequency}, and \texttt{Familiarity}. What looks odd in the model output?
% 
% <<>>=
% english_40 %>% mutate(ep = rnorm(nrow(english_40), mean = 0, sd=0.0),
%                    new_var = WrittenFrequency + Familiarity + ep) %>%
%   lm(RTlexdec ~ WrittenFrequency + Familiarity + new_var, data=.) -> mod
% 
% english_40 %>% mutate(ep = rnorm(nrow(english_40), mean = 0, sd=0.01),
%                    new_var = WrittenFrequency + Familiarity + ep) %>%
%   lm(RTlexdec ~ WrittenFrequency + Familiarity + new_var, data=.) -> mod1
% 
% 
% english_40 %>% mutate(ep = rnorm(nrow(english_40), mean = 0, sd=10),
%                    new_var = WrittenFrequency + Familiarity + ep) %>%
%   lm(RTlexdec ~ WrittenFrequency + Familiarity + new_var, data=.) -> mod2
% 
% @



\hypertarget{collinearity}{%
\subsection{Collinearity}\label{sec:collinearity}}

Full linear dependence of predictors is usually a sign that something is conceptually wrong with your data or model structure, and is relatively easy to detect (e.g.,\  `NA' rows in the model above).
%(Box~\ref{box:linear-dependence}).
%as in the example just above).
%In the farenheit/celsius example, it doesn't make conceptual sense to have both as predictors of anything.

However, it is very common for there to be \textbf{partial} dependence between predictors---that is, \(0 < R^2 < 1\) when you regress one predictor on the others. This is called \emph{multicollinearity}, or just \emph{collinearity}.  

%However, (high) \textbf{collinearity is not a violation of the assumptions of linear regression}!

% This figure may be useful to get an intuitive sense of what collinearity is, if you think of X1-X4 as four predictors which affect Y, and may be highly interrelated (right figure) or independent (left figure).

%\includegraphics{images/collinearity.png}
% 
% (Source: \url{http://www.creative-wisdom.com/computer/sas/collinear_stepwise.html})

There can be high collinearity among several predictors,
%---hence the term `multicollinearity---
where one predictor is largely predictable as a function of several others. For example, in the VOT example above, speaking rate (predictor 1) will be more predictable from number of syllables and word duration (predictors 2 and 3) than 
%(related predictors: speaking rate, number of syllables, word duration),  (1) will be more  predictable from (2) and (3) together than 
any single predictor would be from any other.  (For the \ttt{vot\_michael} data, $R^2$ for pairs of (1)--(3) are 0.43--0.5, while $R^2 = 0.75$ for predicting (1) from (2) and (3).)
%linear function of several others.  For example, in the largely predictable as a function of 

%FUTURE: exercise show this


Collinearity is ubiquitous in linguistic data, and can significantly affect the estimates and interpretations of regression coefficients, particularly when collinearity is `high' (using diagnostics in Section~\ref{sec:collinearity-diagnostics}). But collinearity is not a violation of the assumptions of linear regression, so whether it is a `problem' or not is debatable, as discussed below.  We first demonstrate the effects of very high and high collinearity through a couple examples.

\subsubsection{Example: Very high collinearity}

%To get a sense of what very high collinearity can do, 
Consider the same situation as the linear dependence example above, except now we define \ttt{new\_var} to have very high correlation ($R^2 = 0.99$) with \ttt{WrittenFrequency + Familiarity}, rather than perfect correlation.  We fit a model with and without \ttt{new\_var}:

<<>>=
## Without new_var
good_mod <- lm(RTlexdec ~ WrittenFrequency + Familiarity, 
  data = english_40)

set.seed(2000)
## With new_var
english_40_bad <- mutate(english_40, 
  ep = rnorm(nrow(english_40), mean = 0, sd = 0.01),
  new_var = (WrittenFrequency + Familiarity) * (1 + ep)
)
bad_mod_2 <- update(good_mod, . ~ . + new_var, data = english_40_bad)
@

Now compare the output for these models:
<<output.lines=9:17>>=
summary(good_mod)
@
<<output.lines=10:19>>=
summary(bad_mod_2)
@

If we assume that the coefficient estimates for the first model are relatively close to reality (which you can check by redoing the model on the full \ttt{english} dataset), the results of the second model are bizarre. The effects of \ttt{WrittenFrequency} and \ttt{Familiarity} are in the opposite direction from common-sense expectations (e.g., ``higher frequency words should be identified faster''), and neither effect has a low $p$-value.  Furthermore, the model seems `unstable' in the sense that adding a single predictor drastically changes our qualitative conclusions.

Note that nothing in the output of \ttt{bad\_mod\_2} (alone) indicates a problem, despite there being near-perfect linear dependence among predictors. This is part of what's dangerous about collinearity; since it's not a violation of model assumptions, it can easily go unnoticed.




\subsubsection{Example: High collinearity}
\label{sec:credit-assignment-ex}

%For a more realistic example, let's return to a real dataset.  

Suppose that the \ttt{english\_40} data were from a pilot study where we are interested in the effect of word familiarity (\ttt{Familiarity}) on lexical decision reaction time (\ttt{RTlexdec}) by young speakers. We would like to control for a word's \ttt{WrittenFrequency} and \ttt{LengthInLetters}, because both are expected to correlate with \ttt{Familiarity}, and to affect reaction time. For this example, let $\alpha=0.01$ be the significance threshold.

<<english_40_corrs, echo=FALSE,fig.asp=.65, out.width='90%', fig.width=default_fig.width*.9/default_out.width,fig.cap='Pairwise plots of several variables from the \\ttt{english\\_40} dataset, with nonlinear (GAM) smoothers and 95\\% CIs (lines/shading), and Pearson correlation for each pair. Diagonal panels are empirical distributions of each variable.'>>=
english_40 %>%
  select(RTlexdec, Familiarity, WrittenFrequency, LengthInLetters) %>%
  ggpairs(., lower = list(
    continuous = wrap("smooth_loess", alpha = 1, size = 0.5)
  ))  +
  theme(strip.text.y = element_text(angle=0, hjust=0))
@

Consider the correlations between these variables (Figure~\ref{fig:english_40_corrs}). %All are as expected, except that correlations with \ttt{LengthInLetters} look weaker than anticipated.
As expected, both frequency and familiarity are negatively correlated with reaction time, and positively correlated with each other.  Correlations with word length look weaker than anticipated.

% 
% \begin{quote}
% \textbf{Questions}:
% 
% \begin{itemize}
% \tightlist
% \item
%   What correlations are present in the data? Is there collinearity among predictors?
% \end{itemize}
% \end{quote}


Fitting a simple linear regression of just familiarity on reaction time would give a significant negative effect:
%as we'd expect from the plot of these two variables (Figure~\ref{fig:english_40_corrs}).
% 
<<output.lines=9:16>>=
lm(RTlexdec ~ Familiarity, data = english_40) %>% summary()
@

Now consider a model of reaction time as a function of all three predictors:

<<output.lines=10:19>>=
english_mod_5 <- lm(RTlexdec ~ Familiarity + WrittenFrequency + 
    LengthInLetters, data = english_40)
summary(english_mod_5)
@

This model finds that none one of the three variables significantly affect reaction time at the $\alpha=0.01$ level.
%(barely); 
\ttt{Familiarity} in  particular now does not have a significant effect.

Comparing the two models, we can note a couple surprising things. First, although the coefficient estimates for \ttt{Familiarity} are consistent (each is in within 2$\cdot$SE of the other), the standard error of the coefficient roughly doubles in the second model.  This is what is primarily behind the \ttt{Familiarity} effect `losing' significance. 
Second, almost half of the variance in \ttt{RTlexdec} is explained in the second model ($R^2 = \Sexpr{glance(english_mod_5)$r.squared}$), despite no predictor having a low $p$-value.

% 
% \begin{quote}
% \textbf{Questions}:
% 
% \begin{itemize}
% \item
%   Why has the \texttt{Familiarity} effect changed between the two models? (Examine both the coefficient estimates and SEs.)
% \item
%   How is it possible that none of the three variables significantly affect RT, but together they do predict half the variation in RT (\(R^2 = 0.54\))?
% \end{itemize}
% \end{quote}

This kind of situation is a `credit assignment problem': we can tell that some combination of predictors together affects the response, but not whether an individual predictor does, after controlling for other predictors.
% FUTURE: ref for credit assignment problem? this term comes from Minsky, and is more used in AI.

To get a better sense of what this means, consider the  \emph{correlation matrix} of the coefficent estimates, which describes how correlated the estimates of different coefficients are:

<<echo=3:4>>=
cov_mat <- vcov(english_mod_5)
corr_mat <- cov2cor(cov_mat)
## Transform variance-covariance matrix to correlation matrix:
vcov(english_mod_5) %>% cov2cor()
@

For example, the coefficient estimate for \ttt{Familiarity} is highly  correlated with the estimate for \ttt{WrittenFrequency}  ($r = \Sexpr{corr_mat['Familiarity', 'WrittenFrequency']}$) and moderately correlated with the estimate for \ttt{LengthInLetters} ($r = \Sexpr{corr_mat['Familiarity', 'LengthInLetters']}$).  Fig~\ref{fig:coeff-corr-ex} shows 99\% confidence ellipses for each of these two {pairs} of coefficients. The model is confident that the \ttt{Familiarity} and \ttt{WrittenFrequency} coefficients lie somewhere roughly along a line; a higher estimate for one implies a lower estimate for the other. In contrast the estimates for the \ttt{Familiarity} and \ttt{LengthInLetters} coefficients are weakly related. The left plot illlustrates the credit assignment problem: the 99\% confidence ellipse is consistent with either coefficient individually being 0 (overlaps with the dotted lines), but not with {both} coefficients being zero (no overlap with the origin), which is possible because the estimates are highly correlated.

<<coeff-corr-ex, echo=FALSE, out.width='45%',  fig.width=default_fig.width*.45/default_out.width,fig.cap="99\\% confidence ellipses for pairs of coefficient estimates in model \\ttt{english\\_mod\\_5}. Errorbars are 99\\% confidence intervals for point estimates of coefficients.  Dotted lines are 0.">>=
# Dots are point estimates and 
X <- mvrnorm(
  n = 10000, 
  mu = coefficients(english_mod_5), 
  Sigma = cov_mat
) %>% 
  data.frame()
c_fam <- coefficients(english_mod_5)[["Familiarity"]]
c_freq <- coefficients(english_mod_5)[["WrittenFrequency"]]
c_len <- coefficients(english_mod_5)[["LengthInLetters"]]

ci <- confint(english_mod_5, level=0.99)
ci_fam <- ci[2,]
ci_freq <- ci[3,]
ci_len <- ci[4,]

ggplot(aes(x = Familiarity, y = WrittenFrequency), data = X) +
  stat_ellipse(geom = "polygon", alpha = 0.3, level=0.99) +
  geom_hline(aes(yintercept = 0), lty = 2) +
  geom_vline(aes(xintercept = 0), lty = 2) +
  geom_point(aes(x = c_fam, y = c_freq), size = 3, color = "black") +
  geom_errorbarh(aes(xmin =ci_fam[[1]], xmax=ci_fam[[2]], y=c_freq), height=0.01, size=0.2) +
  geom_errorbar(aes(ymin =ci_freq[[1]], ymax=ci_freq[[2]], x=c_fam), width=0.01, size=0.2) + 
  coord_cartesian(xlim = 1.05*ci_fam, ylim = 1.05*ci_freq) +
  labs(x = "Familiarity", 
       y = "Frequency")

ggplot(aes(x = Familiarity, y = LengthInLetters), data = X) +
  stat_ellipse(geom = "polygon", alpha = 0.3, level=0.99) +
  geom_hline(aes(yintercept = 0), lty = 2) +
  geom_vline(aes(xintercept = 0), lty = 2)+
  geom_point(aes(x = c_fam, y = c_len), size = 3, color = "black")  +
  geom_errorbarh(aes(xmin =ci_fam[[1]], xmax=ci_fam[[2]], y=c_len), height=0.01, size=0.2) +
  geom_errorbar(aes(ymin =ci_len[[1]], ymax=ci_len[[2]], x=c_fam), width=0.01, size=0.2) + 
  coord_cartesian(xlim = 1.05*ci_fam, ylim = 1.05*ci_len) +
  labs(x = "Familiarity", 
       y = "Length in letters")
@


% FUTURE: clean up this section, reorganize some.  Michaela has good suggestions, but I just don't have time to implement (12/2021).
\subsection{Effects of collinearity and diagnostics}
\label{sec:collinearity-diagnostics}

High collinearity can have effects that seem undesirable, each  of which we have observed in the examples above \citep[e.g.,][\S9.4]{chatterjee2012regression}:

\begin{itemize}
\item
\textbf{Unstable coefficients}: large changes in values of the \(\hat{\beta}_i\) when predictors/data points are added/dropped.
\item
\textbf{Nonsensical coefficients}: signs of \(\hat{\beta}_i\) don't conform to prior expectations.
\item
\textbf{Unexpected non-significance}:  values  of \(\hat{\beta}_i\) for predictors expected to be important (e.g.,~from exploratory plots) have large SEs and 
%low \(t\)-values, and
high \(p\)-values, or there are no `significant' coefficients despite $R^2$ well above 0.
\end{itemize}

Any of these
%signs
%, especially together, 
can be thought of as a `warning sign' that there may be substantial collinearity in your data.  They all follow from the fact that when data is highly collinear, a linear relationship between predictors almost holds, so there are many regression coefficient estimates that give models almost as good as the least-squared estimates.  A common situation with high collinearity is where several closely-related predictors are each correlated with the response (as in the `credit assignment problem' above).  In general, collinearity does not affect the actual values of coefficient estimates, just their standard errors.

However, should we even be worried about these `issues'?  Is (high) collinearity a problem at all?  There are two philosophies here (Box~\ref{box:collinearity-problem}); the first one (`yes') has been influential in statistics-for-linguistics, while I am personally partial to the `no' philosophy. But this ultimately comes down to a choice between different analysis strategies with different risks.
%(especially Type I vs.\ Type II error).

One must also distinguish between  `essential collinearity'---where predictors are inherently related---and `nonessential collinearity', which can be eliminated by centering all predictor variables
%%and there
%are common circumstances where one can decrease collinearity without any loss of information, most commonly centering all predictor variables 
%`nonessential collinearity'. In particular, centering all predictor variables eliminates `nonessential collinearity' while preserving `essential collinearity'
\citep[][\S7.2]{cohen2002applied}.  Only essential collinearity is potentially a problem.  In addition, there are common situations where terms are collinear by construction, so collinearity can't be a `problem': nonlinear effects (we expect \ttt{x} and \verb'x^2' to be correlated) or interactions (\ttt{x:y} will usually be correlated with \ttt{x} and with \ttt{y}). There are other good reasons in these cases to consider transformations that would reduce collinearity (e.g.,\ interactions are more interpretable for centered predictors: \citealp[][\S7.2]{cohen2002applied}; \citealp{dalal2012some}),
but this is a side effect rather than the goal.
%decreases collinearity, without any loss of information.

Whichever perspective on collinearity one takes, it is important to be able to diagnose collinearity,
%\textbf{in a set of predictors}, 
to decide to what extent it could affect inference for each predictor (mostly standard errors, and thus $p$-values).
%it could be affecting the standard error of regression coefficeints. %(Note that the `warning signs' above require examining an actual model, not just the set of predictors.)  
%Because collinearity is not a violation of model assumptions, it won't show up in any of the diagnostic plots we have considered above.  

The qualitative `warning signs' above are not good diagnostics, because they are neither necessary nor sufficient to detect how much collinearity is present and whether it may be `harmful' \citep[][92]{belsley2004regression}.
%In addition to the qualitative `warning signs' above, 
The degree of collinearity can be quantified in a couple common ways.


\begin{boxedtext}{Broader context: Is collinearity a problem?}
\label{box:collinearity-problem}

\paragraph{Yes}

Collinearity is often treated as a problem (e.g.,\ \citealp{chatterjee2012regression}; \citealp{zuur2007analyzing}; \citealp{belsley2004regression}, \citealp[][\S7.3]{faraway2015linear}; for language sciences \citealp{baayen2008analyzing,levshina2015linguistics,tomaschek2018strategies}), because: it can cause the issues discussed in the text; it can also slow down or foil model fitting
%, and even lead to inaccurate estimates
(historically a major issue, less so with modern statistical software); and increased standard errors will increase the likelihood of Type II errors.
%---concluding a predictor has no effect on the response, when it actually does. 
%At the extreme, in a `credit assignment' situation, if we assume that at least one predictor does in fact affect the response, we \textbf{know} we are making a Type II error (if all individual coefficients are not significant)---which feels intuitively like something must be wrong.
%a sure sign something is wrong.

Therefore, collinearity should be somehow dealt with,  by removing predictors,  defining new predictors with the effects of others `residualized' out, or dimensionality reduction to combine  collinear predictors into a single predictor giving equivalent information (e.g.,\ `principal components'), or a regression technique such as `ridge regression' which shrinks coefficient values (\citealp[][\S4.7]{harrell2015regression}, \citealp{tomaschek2018strategies}) .

\paragraph{No} 

The other perspective is ``There is no general sense in which collinearity is a problem'' \citep[][1]{morrissey2018multiple}---because collinearity is a property of the data,
%(and the model fit to it),   
not a violation of model assumptions (e.g.,\ \citealp{gelman2007data,obrien2007caution}; for linguistics \citealp{levy2012probabilistic,vanhove2021collinearity}).
% FUTURE: more refs for linguistics
The issues listed above reflect a lack of information in your data, and how hard it is to detect effects of  variables of interest using this dataset. As such, collinearity may help explain the results of a model (e.g.,\ high collinearity suggests a Type II error is likely), but is not a problem in itself---in fact, it can provide useful information. Therefore, you should either fit the model with your original set of predictors (which acknowledges the lack of information), or collect more data (which raises power, decreasing the risk of a Type II error).
%To the extent that an effect is `missed' (a Type II error) in the presence of collinearity, this is just because of the structure of the data; with a larger sample size, the effect would be detected. 
%---given this dataset, of this size, .
In the example above, it makes intuitive sense that it is harder to detect a `real' effect of \texttt{Familiarity} when \texttt{WrittenFrequency} is added as a predictor, because of the correlation between \texttt{Familiarity} and \texttt{WrittenFrequency}. 

%Most 
Methods to `deal with' collinearity can have unintended consequences on interpretability or lead to biased coefficents. `Residualizing' one predictor, the most commonly used strategy to deal with collinearity in linguistics over the past $\sim$15 years \citep[e.g., in my work:][]{carlson2014global},  is generally not a good idea, because it complicates the interpretation of the regression coefficients in unintended ways \citep{york2012residualization,wurm2014residualizing}.
%, as illustrated for psycholinguistic data by \citet{wurm2014residualizing}.  
Dimensionality reduction can result in predictors whose coefficients are hard to interpret. Most importantly, dropping predictors will in general bias the coefficient estimates of remaining predictors, %\citep[e.g.,][]{morrissey2018multiple}, 
as does ridge regression, potentially resulting in Type I errors.  These methods may still all make sense in a given case (except residualizing), but you should be aware that they are not without issues.

\paragraph{What to do?}

Whether to `deal with' collinearity or not is an \textbf{analysis choice}, which often  comes down to Type I versus Type II error risk (Section~\ref{sec:error-trade-offs}).
%---not one where there is a right or wrong answer.   
(This assumes you have already minimized `nonessential collinearity', which is uncontroversial.)  More generally: `dealing with' collinearity is often  convenient from the perspective of actually answering one's research questions, but can require more complex interpretation and/or caveats in reporting results. `Not dealing with' collinearity is technically more correct, especially if we prefer avoiding Type I errors, but can lead to less useful conclusions about research questions. (Usually one would like to say more than `more data is needed'.) 

If you work with linguistic data where collinearity among theoretically-important predictors is an important issue---such as disentangling frequency and probability effects on language production---it is worth reading a full treatment of these issues, which are  subtle but not difficult to understand. \citet{morrissey2018multiple} is particularly readable.
%, with minimal math.

\end{boxedtext}

% - foundational on pointing out such issues: \citet{obrien2007caution}
% 
% quote: "To the extent that an analysis can be considered successful if it can be relied upon to give unbiased estimates of the direct effects of predictor variables on responses, so far as available data allow, and that the analysis can yield credible statements about our uncertainty in these estimates, collinearity is not a problem, and procedures to address the supposed problem lead to unsuccessful analysis."


\subsubsection{Condition number}
\label{sec:condition-number}

First, we can quantify `How far is the whole set of predictors from linearly independent?' with the \emph{condition number} $\kappa$, which is calculated for the design matrix (where each column = values of one predictor, across the dataset).  $\kappa$ characterizes how sensitive the coefficients of a linear regression are to changes in the predictors (lower sensitivity is better).
% Seems like unnecessary details
%\footnote{Mathematically, the condition number is the square root of the ratio of the highest and lowest `singular values' of $X$, and characterizes how sensitive the coefficients of a linear regression using $X$ are to changes in the predictors.  Lower sensitivity is better.}  
$\kappa > 30$ and $\kappa < 5$ are common cutoffs for `high' and `no' collinearity, but like all quantitative diagnostic measures what counts as `bad' is primarily an empirical question. (See \citealp[][chap.~3]{belsley2004regression} for details.)
%gives $\kappa > 30$Experiments by   $\kappa > 30$ is a common cutoff for `potentially harmful' collinearity, though and $\kappa < 6$ is `no collinearity'  \citet{belsley2004regression} \citet{belsley1980regression}.
%(\citet{baayen2008analyzing}, p.~200, citing \citet{belsley1980regression}):.
%these are called the `singul$\sqrt{\lambda_{high}/\lambda_{low}$, where $\lambda_{high}$ and $\lambda_{low}$ are the highest and lowest singular values of the design matrix $X$. I am not aware of an intuitive interpretation}
%above 30 indicate `potentially harmful' collinearity and 


For example, we can apply \ttt{collin.fnc()} from {languageR} to the matrices of predictors for the examples above to calculate their condition numbers:\footnote{This implementation first adds the intercept to the design matrix and scales each predictor (divided by one SD), as recommended by \citet[][157]{belsley2004regression}.}

<<echo=1:6>>=
# 'Very high' collinearity example
X1 <- english_40_bad %>% select(WrittenFrequency, Familiarity, new_var)
collin.fnc(X1)$cnumber
# 'High' collinearity example
X2 <- english_40 %>% select(Familiarity, WrittenFrequency, LengthInLetters)
collin.fnc(X2)$cnumber
c1 <- collin.fnc(X1)$cnumber
c2 <- collin.fnc(X2)$cnumber
@

The `very high collinearity' example has $\kappa = \Sexpr{c1}$, which indicates extreme collinearity. The `high collinearity' example has $\kappa = \Sexpr{c2}$, indicating moderate collinearity (by the cutoffs above).
% 
% 
%  `very high collinearity' and `high collinearity' examples above, the condition numbers (calculated using \ttt{collin.fnc} in \ttt{languageR}) are:
% % 
% % For example, for the `very \texttt{frequency} and \texttt{AoA} in the example above, the condition number is:
% % 
% % <<warning=FALSE>>=
% % library(languageR)
% % collin.fnc(dplyr::select(d, Familiarity, WrittenFrequency, LengthInLetters))$cnumber
% % 
% % @
% 
% As a rule of thumb, condition numbers can be interpreted as (\citet{baayen2008analyzing}, p.~200, citing \citet{belsley1980regression}):
% 
% \begin{itemize}
% \item
%   CN \textless{} 6: ``no collinearity''
% \item
%   CN \textless{} 15: ``acceptable collinearity''
% \item
%   CN \textgreater{} 30: ``potential harmful collinearity''
% \end{itemize}
% 
% 

% Because collinearity isn't a violation of model assumptions, high collinearity cannot be diagnosed using diagnostic plots, such as those considered for Assumptions 1-4 above.
% 
% \hypertarget{diagnosing-collinearity}{%
% \subsubsection{Diagnosing collinearity}\label{diagnosing-collinearity}}
% 
% How can one then detect collinearity, and decide if it could be affecting (the standard errors of) regression coefficients?
% 
% Some warning signs that there may be substantial collinearity in your data (\citet{chatterjee2012regression}, 9.4):
% 
% \begin{itemize}
% \item
%   \textbf{Unstable coefficients}: large changes in values of the \(\hat{\beta}_i\) when predictors/data points are added/dropped.
% \item
%   \textbf{Nonsensical coefficients}: signs of \(\hat{\beta}_i\) don't conform to prior expectations.
% \item
%   \textbf{Unexpected non-significance}: values of \(\hat{\beta}_i\) for predictors expected to be important (e.g.,~from EDA) have large SEs, low \(t\)-values, and high \(p\)-values.
% \end{itemize}
% 
% These diagnostics all follow from the fact that when data is highly collinear, a linear relationship between predictors \textbf{almost} holds, so there are many regression coefficient estimates that give models \textbf{almost} as good as the least-squared estimates.

\subsubsection{Variance inflation factors}

Another measure of collinearity uses both the predictors and the response ($y$). Since the main `problem' caused by collinearity is to increase standard errors for each predictor $x_i$, an intuitive measure of `how much does collinearity increase SE?' is a \emph{variance inflation factor} (VIF): the ratio of (sample) variance for $x_i$ in the multiple regression ($y$ $\sim$ all predictors) to a simple regression ($y \sim x_i$).  This turns out to be just:
$$
\text{VIF}_{i} = \frac{1}{1-R_i^2},
$$
where $R_i^2$ is $R^2$ for a regression of $x_i$ on all other predictors.  $\text{VIF} > 10$, meaning at least one coefficient has variance `inflated' at least 10-fold, is a common (if arbitrary) diagnostic for collinearity that ``may be causing problems'' \citep[][238]{chatterjee2012regression}.  

A common way to `do something' about collinearity if this choice is made (Box~\ref{box:collinearity-problem}) is to use VIF or $\kappa$ cutoffs, typically to remove or combine predictors. This is a controversial practice, not least since neither measure accounts for sample size (for high enough sample size, even highly correlated effects can be precisely estimated), but also very common, so it's good to know about.
%and because removing predictors will in general bias the coefficient estimates of remaining predictors \citep[e.g.,][]{morrissey2018multiple}.  But it is also common practice, so it's good to know about.

VIFs can be calculated using \ttt{vif()} in the {car} package. For the examples above:
%, for the `very high' and `high collinearity' examples above:

<<>>=
vif(bad_mod_2)
vif(english_mod_5)
@

By the VIF = 10 cutoff, we would conclude that there is potentially problematic collinearity in \ttt{bad\_mod\_2}, with the variance of \ttt{new\_var} particularly inflated (by over 800x), but that the degree of collinearity in \ttt{english\_mod\_5} is acceptable (maximum VIF = \Sexpr{max(vif(english_mod_5))}).



\begin{boxedtext}{Broader context: Measurement error}

% One further assumptions of linear regression related to predictors must be mentioned,  even if we usually can't do anything about them.
% 
% It is assumed that the predictors $x_i$ are not random variables---they are fixed in advance. This is almost never 100\% true: even in an experimental setting, where the predictors of interest are fixed by the experimenter (e.g.,\ what condition is a subject in), one usually needs to control for things not under the experimenter's control (e.g.,\ the subject's score on a post-test, or their age).  In observational studies (e.g.,\ any corpus study, in linguistics) all predictors are random variables.  The practical consequence is that the regression's results always have to be interpreted conditional on the observed data. 
% (FUTURE: read more. this comes from Faraway 7.1? very short) - def need reference if this box is kept in

A further assumption of most regression models is that all predictor values---$x_{11}$, $x_{52}$, and so on---are measured without error.  This is often not the case; for example in the \ttt{vot} dataset, speaking rate is based on an automatic algorithm with some margin of error, word frequencies were determined based on a finite corpus, and so on. The effects of measurement error are complex, but as a rule if measurement error \textbf{is} taken into account for $x_i$, its effect in the fitted model will be weaker ($\beta_i$ closer to 0). %\citep[][\S7.1]{faraway2015linear}.
% FUTURE: this is definitely a general rule but not always true..
Thus, usually our regression coefficients are biased away from 0 (meaning anti-conservative $p$-values, etc.), and we should just keep this in mind in interpreting a model.  Effects of measurement error are small as long as measurement errors are small relative to the residual error ($\epsilon_i$).  If you know that there is non-trivial measurement error for an important predictor in your model, by this metric, it's worth reading more  
(e.g.,~\citealp[][\S7.1]{faraway2015linear};~\citealp[][140--142]{kline2013beyond}).
% FUTURE: those are not ideal references.. Faraway is an OK start, Kline is short.
% Hunter & Schmidt 2014 meta-analysis book?
\end{boxedtext}


\section{Problems with observations}
\label{sec:lr-problems-with-observations}

The main linear regression assumption about observations is that they are roughly equally 
%Two linear regression assumptions about observations is that they are equally \textbf{reliable}, and roughly equally
\textbf{influential}.  
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   Equally \textbf{reliable}
% \item
%   Roughly equally \textbf{influential}
% \end{enumerate}
%The first assumption is hard to check in practice, and we won't consider it further. 
This assumption is important, 
%The second assumption is important, 
whether our goal is estimation
%(of population values of parameters),
%or 
%we do not want a few observations to strongly influence the model we fit, 
%because
% %observations should have ``a roughly equal role in determining regression results and in influencing conclusions'' (\citet{chatterjee2012regression}, p.~88). This is because 
% our goal in statistical analysis usually involves estimation of \textbf{population} values of parameters (like the slope of a line of best fit), 
or prediction---both 
%of $y$ for new observations---both
abstract  away from our finite sample. If certain observations are much more influential than others, they skew the regression results to reflect not the population, but the particular sample we happened to draw.

We are most interested in diagnosing observations with high \emph{influence}.  These tend to be observations which have extreme predictor values,
%relative to other observations,
called high \emph{leverage}, or observations which the model fits particularly badly ($\sim$large residuals),
%(high $\hat{y} - y$),
called \emph{outliers}.  It is useful to identify outliers or high leverage points for two reasons. First, they could be influential, and
for many regression models (e.g.,\ mixed-effects models) it is easy to find outliers or compute leverage, but hard (computationally-intensive) to compute influence.  
%% FUTURE: this sentence is my guess, not supported by a source.  the motivation for examining leverage/outliers *seems* to be to get at influential points, since leverage or outliers by themselves aren't model assumption violations.
Second, they are worth examining for their own sake, as they often indicate a problem with the data or model.   What to do about influential observations (or high leverage/outlier observations) when they don't reflect errors is more subjective.  Often, some points are inherently more influential than others (say, a couple participants with behavior very different from others), and we need to decide how to proceed in fitting and interpreting the model.  

The presence of highly-influential observations can lead to either Type I or Type II errors: the influential observations might be responsible for a spurious result, or they might obscure a pattern that would be clear if they
%observations
were excluded.

% This isn't always the case, though---often, some points are inherently more influential than others (say, a couple participants with behavior very different from others), and we need to decide how to proceed in fitting and interpreting the model.

\subsection{Leverage}
\label{sec:leverage-lr}

The leverage of observation $i$, written $h_i$, is how far out in predictor space the observation is from an `average' observation (each predictor = its mean value), on a scale from 0 to 1.\footnote{Formally, $h_i$ is the Mahalanobis distance of observation $i$ from the average observation, using the covariance matrix of the predictors \citep[][84]{faraway2015linear}.}  These are also called `hat values' (hence the `h'), and can be calculated in R using \texttt{hatvalues()}, or just as a column resulting from \ttt{augment()} (example in Section~\ref{sec:vot-ex-1}).
% <<output.lines=1:5>>=
% ## Adds the .hat, .cooksd, etc. columns
% slr_mod_4 %>% augment()
% @


For example, consider model \ttt{slr\_mod\_4} (from Section~\ref{sec:predictor-transform-example}), which corresponds to the relationship in Figure~\ref{fig:raw-freq-bad-plot} between raw frequency ($x$) and naming latency ($y$) in the \ttt{english\_40} data.  Plotting a histogram of leverage ($h_i$) values shows that there are two extreme observations
%in predictor space 
(Figure~\ref{fig:slr-mod-4-plots} left).

<<slr-mod-4-plots, echo=FALSE, out.width='40%',  fig.width=default_fig.width*.4/default_out.width, fig.cap='Distribution of leverage and influence measures for model \\ttt{slr\\_mod\\_4}.'>>=
## adds the .hat, .cooksd, etc. columns
slr_mod_4_augmented <- slr_mod_4 %>% augment()
slr_mod_4_augmented %>% ggplot(aes(x = .hat)) +
  geom_histogram() +
  xlab("Leverage (h_i)")
slr_mod_4_augmented %>% ggplot(aes(x = .cooksd)) +
  geom_histogram() +
  xlab("Cook's distance")
@

These are the observations with raw frequency above 5000.  It is visually clear that these observations will have much greater influence than other observations; hence the very large confidence intervals for this frequency range.

This is a simple example of a common cause of high leverage: a non-normally distributed predictor.  When this is the case, often transforming the predictor to normality results in more equal leverage across observations.
%% FUTURE
%%(Exercise~\ref{ex:better-leverage}).



%This example is a (simple)  of a common: often a non-normally distributed response or predictors can leads to some points influencing the model much more than others, and the problem can be fixed by transforming to normality---as for the word frequency example (right plot). After log-transforming frequency, removing the seven most extreme X values hardly affects the fitted line.




% When raw frequency is used as the predictor (left plot), the handful of `extreme' observations with frequency above 25000 have a much greater effect on the slope of the line than other observations, resulting in larger confidence intervals.

%\protect\hyperlink{engdata}{the \texttt{english} dataset}. (The same data as in \protect\hyperlink{c2ex3}{this example above}, but now fitting a simple line of best fit in each case.)
% 
% <<message=FALSE, fig.align='center', fig.height=3, fig.width=4, out.width='45%', echo=FALSE>>=
% ggplot(young, aes(WrittenFrequency_raw, RTnaming)) + 
%   geom_point(size=0.5) + 
%   xlab('Raw written Frequency') + 
%   geom_smooth(method='lm')
% 
% ggplot(young, aes(WrittenFrequency_log, RTnaming)) + 
%   geom_point(size=0.5) + 
%   xlab('Log written Frequency') + 
%   geom_smooth(method='lm') 
% @


% Often, a non-normally distributed response or predictors leads to some points influencing the model much more than others, and the problem can be fixed by transforming to normality---as for the word frequency example (right plot). After log-transforming frequency, removing the seven most extreme X values hardly affects the fitted line.

\subsection{Outliers}
\label{sec:outliers}

There are two aspects to defining outliers: choosing a metric (`outlier' in what space?) and a criterion for outlier-hood in that space.

\subsubsection{Defining outliers}

Informally, researchers often define outliers as points with an unusual $y$ value, sometimes using a cutoff like `3 SD away from the mean'---so, the metric is `$z_i$, the standardized value of the $y$th observation'.  This can be a good first pass, especially to check for observations with gross problems (like annotation errors). But since the distribution of $y$ does not control for any predictors (which is a primary motivation for doing a regression), it makes more sense to define outliers in terms of {how well the model fits observations}---which requires a metric for comparing $\hat{y}$ and $y$.  

The (absolute value of the) residual, $|\hat{\epsilon}_i| = |\hat{y}_i - y_i|$, seems like a natural metric, but it has two issues: it is not on an interpretable scale (we would like `3' to mean `3 SD from the mean'), and it does not account for the fact that observations further from the mean have higher variance (Box~\ref{box:standardized-residuals}), i.e.,\ lower leverage ($h_i$).  
Defining the \emph{standardized residuals} $r_i$ addresses both issues:
\begin{equation*}
r_i = \frac{\hat{\epsilon}_i}{\hat{\sigma}\sqrt{1-h_i}},
\end{equation*}
where $\hat{\sigma}$ is the residual standard error (equation~\ref{eq:rss}), the estimate for  `how much variability is left' in the data.

Outliers are points which have unusual values of $r_i$ by some criterion: either visual inspection
% (which points look odd given the distribution of others?) 
or a formal test, such as the Bonferroni outlier test (\ttt{outlierTest()} in the {car} package;  \citealp[e.g.,][6.2.2]{faraway2015linear}).  As always, visual inspection is preferred to quantitative diagnostics for model validation, but sometimes visual inspection is impractical, like if you are running dozens of models.  If you do use a quantitative diagnostic for outlier detection, it is better to use a test
%-based method  (like \ttt{outlierTest()}) 
than a  cutoff (`any observation with $|r_i| > 3$ is an outlier').
%even though the latter is common practice.

So to summarize: $r_i$ is a better metric than $z_i$, and for deciding outlier-hood visual inspection is better than a formal test, which is better than a strict cutoff.

\subsubsection{Example}

The standardized residuals can be calculated using \ttt{augment()} (the \ttt{.std.resid} column) or \ttt{rstandard()}. Standardized residuals are often used in a \emph{residual-leverage} diagnostic plot (one of those made automatically when \ttt{plot()} is applied to a linear regression model),  
%distribution of standardized residuals feature in three of the four diagnostic plots made automatically by  \ttt{plot} applied to a linear regression model in R, as shown in Figure~\ref{fig:resid_plot_4} for \ttt{vot\_mod\_2}.  The only one we have not seen before is the \emph{residual-leverage} plot,
which shows $h_i$ ($x$-axis) versus $r_i$ ($y$-axis). Points which have high values on either $x$ or $y$ may have high influence and should be flagged for further inspection.  Often it is useful to determine \textbf{why} they are outliers---why is the prediction unusual given the predictor values?  

<<resid_plot_5, echo=FALSE, fig.asp=0.75, out.width='45%', fig.width=default_fig.width*.45/default_out.width, fig.cap='Diagnostic plots for \\ttt{vot\\_mod\\_2}: Q-Q plot of residuals (left) and residual-leverage plot  (right).'>>=

augment(vot_mod_2) %>% ggplot(aes(sample = .resid)) +
  geom_qq(color='darkgrey') +
  geom_qq_line(color=default_line_color, size=0.75) +
  xlab("Theoretical quantiles") +
  ylab("Residuals")

augment(vot_mod_2) %>% ggplot(aes(x=.hat, y=.std.resid)) + 
  geom_point(color='darkgrey') +
  geom_smooth(color=default_line_color, se=F, size=0.75) +
  xlab("Leverage") + ylab("Standardized residuals")

#plot(vot_mod_2, which = c(2,5), sub.caption = '')
@

Figure~\ref{fig:resid_plot_5} shows a Q-Q plot of standardized residuals and a residual-leverage plot for  model \ttt{vot\_mod\_2}. The Q-Q plot suggests one possible outlier with high $r_i$, and maybe some with low $r_i$.  The highest and lowest-$r_i$ observations are:

<<echo=1:3>>=
vot_mod2_sorted <- augment(vot_mod_2) %>%
  arrange(.std.resid) 
tail(vot_mod2_sorted, n = 1)
head(vot_mod2_sorted, n = 1)
outlier_df_1 <- augment(vot_mod_2) %>%
  arrange(-.std.resid) %>%
  head(n = 1)
outlier_df_2 <- augment(vot_mod_2) %>%
  arrange(.std.resid) %>%
  head(n = 1)
@

The first one turns out to be the voiced stop observation (\ttt{voicing}=\tsc{voiced}) with the highest VOT (\Sexpr{exp(outlier_df_1$log_vot)} msec; compare Figure~\ref{fig:resid_plot_2} right). $r_i$ is high because the observation has a very high VOT for a voiced stop. The second one is the lowest-VOT observation among observations with a consonant cluster (\ttt{cons\_cluster}=\tsc{yes}), which tends to dramatically raise VOT.  Both points bear further inspection; perhaps they are annotation errors.

\subsection{Influence}
\label{sec:influence}

There are different ways to quantify how influential an observation is for a model. Most commonly-used is \emph{Cook's distance}, $C_i$, which measures how much the regression coefficients change when the model is fitted with all data except  observation $i$.  This turns out to be:
%equivalent to 
\citep[][\S4.9]{chatterjee2012regression}:
\begin{equation}
C_i = \frac{r_i^2}{k+1} \frac{h_i}{1-h_i},
\label{eq:lin-reg-cd}
\end{equation}
where $k$ is the number of predictors. On the right-hand side of this equation, the first term is high for points with high outlier-ness (the squared standardized residual), and the second term is high for points with high leverage ($h_i$ closer to 1), so  Cook's distance (CD) will  be high for observations which are outliers or have high leverage.
Observations with high CD  are highly influential, and should be flagged and examined.  Again, it is better to decide what CD is `high' by visual inspection than by a numerical cutoff; 
%but it is good to know that a cutoff of
1 and $4/n$ are commonly-used cutoffs.
%(e.g.,\ CD$>$1 = `influential').

%Cook's distance (CD) is roughly 
%\textbf{how much the regression coefficients change} when the model is fitted with all data except this observation. 

% Data points with significantly higher CD than other points are highly influential, and should be flagged and examined as potential outliers.

Equation (\ref{eq:lin-reg-cd}) captures the intuition that highly influential points tend to be either outliers or have high leverage, but neither is a necessary or sufficient condition for high influence.  This intuition also holds for other measures of influence, such as DFBETA (\ttt{dfbeta()} in R), which quantifies the influence of each observation on a particular coefficient.

%Some examples are helpful here.
% 
% The important higher-level logic is:
% - we are most worried about high-\textbf{influence} points
% 
% - these tend to be outliers or high leverage points; both are worth examining in their own right
% 
% - furthermore, for many regression models (e.g., mixed effects models) it is easy to search for outliers/high leverage, but hard to compute influence.
% 
% - so we flag points with high leverage/outliers, and (if calculable) high influence; what to do next is trickier (Sec XX).

\paragraph{Examples}

CD can be calculated using \ttt{augment()} (the \ttt{.cooksd} column) or \ttt{cooks.distance()}.

For the \ttt{slr\_mod\_4} model, we saw above that there are high-leverage points, but the distribution of CD (Figure~\ref{fig:slr-mod-4-plots} right) suggests that there are no highly influential points---those with very different CD from other points, or with CD$>$1. This is because the high-leverage points are consistent with the general trend (higher $x$ $\Rightarrow$ lower $y$).  Similarly, it turns out there are no clear high-influence  points for the \ttt{vot\_mod\_2} model  even though there were outliers (Exercise~\ref{ex:vot-influence}). 
%% FUTURE:  depending on how one defines it there may be influential points..
% 
% - slr\_mod\_4 : high leverage points, but none *highly* influential, by Cook's D plot. they are all consistent with the general trend. 
% 
% - VOT model 2:  there are (arguably) outliers, but no clear high-influence points. 

%(exercise: show these)

% 
% 
% \hypertarget{example-12}{%
% \subsubsection*{Example}\label{example-12}}
% \addcontentsline{toc}{subsubsection}{Example}

To get an intuition for what it means for observations to have high influence, let's tweak the \ttt{english\_40} data (from Section~\ref{sec:predictor-transform-example}) so that the point with highest \ttt{Familiarity} has high \ttt{RTnaming}:

<<influence-ex-1, echo=1:3, out.width='90%', fig.width=default_fig.width*.9/default_out.width, fig.cap="Relationship between word familiarity and naming reaction time  for the \\ttt{english\\_40} data, with line of best fit and 95\\% CI, before (top left) and after (bottom left) changing one observation (the triangle point). Histogram (right) shows distribution of influence measures for the latter.">>=
# Tweak data to illustrate point
english_40_new <- english_40
english_40_new[english_40_new$Familiarity > 6, ]$RTnaming <- 6.35

## RTnaming ~ Familiary line, without tweaked point
p1<- lm(RTnaming ~ Familiarity, data = filter(english_40, Familiarity <= 6)) %>%
  augment() %>%
  ggplot(aes(Familiarity, RTnaming)) +
  geom_jitter(width = 0.1, height = 0.01) +
  geom_smooth(color=default_line_color, method = "lm") +
  labs(x = "Familiarity",
       y = "Naming reaction\ntime (log msec)") +
  ggtitle("Without outlier")

## RTnaming ~ Familiary line, with tweaked point
p2<- lm(RTnaming ~ Familiarity, data = english_40_new) %>%
  augment() %>%
  ggplot(aes(Familiarity, RTnaming)) +
  geom_smooth(color=default_line_color, method = "lm") +
    geom_jitter(width = 0.1, height = 0.01, data=english_40) +
  geom_point(aes(x=x, y=y), data=data.frame(x=6.27, y=6.35), size=2, shape=17, color='black') + 
  labs(x = "Familiarity",
       y = "Naming reaction\ntime (log msec)") +
  ggtitle("With outlier")

# p3<-lm(RTnaming ~ Familiarity, data = filter(english_40, Familiarity <= 6)) %>%
#   augment() %>%
#   ggplot(aes(x = .cooksd, color = .cooksd)) +
#   geom_histogram() +
#   labs(x = "Cook's distance") +
#   ggtitle("")

p4<-lm(RTnaming ~ Familiarity, data = english_40_new) %>%
  augment() %>%
  ggplot(aes(x = .cooksd, color = .cooksd)) +
  geom_histogram() +
  labs(x = "Cook's distance") +
  ggtitle("")

design <- "
  11#
  224
"
p1 + p2 + p4 +
  plot_layout(design = design) &
  theme(legend.position='none')
@

Figure~\ref{fig:influence-ex-1} shows the relationship between these variables predicted by a simple linear regression without and with  this point (left/center panels).  The point  with high \ttt{RTnaming} has much higher influence than the others, as measured by CD (right panel), with the effect that removing this point changes the direction of the \ttt{Familiarity} effect, and thus our qualitative conclusion
%Thus, whether this point included changes our qualitative conclusion 
from the model.  If writing up an analysis of this dataset it would be particularly important to flag and discuss this point.

\subsection{What to do about influential (etc.) observations}
\label{sec:what-to-do-influential}

Compared to {finding} influential (or high-leverage/outlier) observations, ``deciding what to do about them can be difficult'' \citep[][83]{faraway2015linear}.  There are different philosophies, and what makes sense to do depends on one's dataset and research questions.
%particular dataset and research questions.
% FUTURE discuss at length?  This is all a bit vague without examples.
Still, 
%There is no one right answer here, but 
a few general points can be made.

At a minimum, during data analysis:
\begin{enumerate}
\item You should  look for high-influence/outlier/high-leverage observations (often just called `outliers'). Just examining these observations often reveals gross errors, such as a participant who always gave the same response, or data coding errors that cannot be corrected; these data can be discarded. Examination could also reveal issues that will improve your analysis, like a predictor which should be transformed (e.g.,\ in the example above). 
\item Such observations which are not `errors' should be flagged and examined. You might try excluding them, and see what effect this has on the analysis. 
\item Report how influential/high-leverage/outlier points
were dealt with, what points were discarded, and ideally whether excluding non-error influential points (if step (2) is taken) would affect your analysis.  
\end{enumerate}

Note that (3) does not necessarily mean detailed reporting of the model validation itself (e.g., plots of Cook's distance). These could make sense for an appendix but are not usually %never 
reported in the text of a write-up.
%(in current practice). 
A short summary suffices.

By far the most common method to deal with influential points (etc.), including in linguistic research \citep[e.g., my own work:][]{stuart2019large}, is simply to exclude them automatically (e.g.,\ all points with standardized residual $>$2.5).  This is not a good method  \citep[][89]{faraway2015linear} because it can eliminate observations which are actually the most meaningful, but it is better than nothing, especially for cases where manual inspection of influential points is infeasible.
However, refitting a model after automated outlier exclusion is a nice way to check that your qualitative conclusions are not just due to influential points \citep[][279]{baayen2008analyzing}.

Another option for `dealing with' outliers is not to exclude them at all (even if they are clear `errors'), but to model them explicitly, by using a method which allows for outliers (e.g.,\ `robust regression': \citealp[][\S8.4]{faraway2015linear}).


% \hypertarget{outliers}{%
% \subsection{Outliers}\label{outliers}}
% 
% There are different philosophies on how to detect outliers (visual inspection versus numeric measures), and what to do with them. At a minimum:
% 
% \begin{itemize}
% \item
%   During data analysis, potential outliers should be flagged, examined, and considered for exclusion.
% \item
%   Note what effect excluding outliers \textbf{would} have on the analysis.
% \item
%   Report how outliers were dealt with, and ideally what effect they have on the analysis, in any write-up.
% \end{itemize}
% 
% In many cases inspection of gross outliers will reveal data points that should be clearly excluded, such as a participant who always gave the same response, or data coding errors that cannot be corrected. In other cases the decision is more subjective.

% \hypertarget{regression-assumptions-reassurance}{%
% \subsection{Regression assumptions: Reassurance}\label{regression-assumptions-reassurance}}


\section{Trade-offs between models}
\label{sec:overfitting}

Before discussing the problem of choosing between different regression models, we need a high-level understanding of consequences of using one model or another.  In building any statistical model there are closely-related trade-offs:  underfitting versus overfitting, and bias versus variance.
% Michaela pointed out: not discussed in this section, only in next one.
%or  fit versus parsimony.

We start by assuming the goal of model-building is estimation, as is often the case for linguistic data, to introduce the bias-variance trade-off.
%in this context. 
We then turn to underfitting and overfitting, which is easier to think about assuming our goal is  prediction. (However, both trade-offs hold whether the goal is estimation or prediction.)

% a model that doesn't fit the observed data well (\emph{underfitting}), and a model that doesn't generalize well to unobserved data (\emph{overfitting}).   These concepts are easiest to think about when the goal of modeling is prediction.  When the goal is inference, as is often the case for linguistic data,  it is easier to think in terms of a related trade-off, between bias and variance.

\subsection{The bias/variance trade-off}
\label{sec:omitted-variable-bias}

If our goal in building a model is coefficient estimates, different possible models trade off between two problems, corresponding to `too few' and `too many' predictors \citep[][\S11.2]{chatterjee2012regression}: high bias and high variance (defined in Section~\ref{sec:bias-def}).  Suppose for simplicity that we are particularly interested in estimating the effect of $x_1$, corresponding to coefficient $\beta_1$, and have to decide whether to include a term for $x_2$; the general points hold for the more general case (multiple effects of interest, multiple candidate predictors to add).

\paragraph{Bias} 

If $x_2$ does in reality affect the response $y$ ($\beta_2 \neq 0$), and $x_2$ isn't completely orthogonal to $x_1$, the bias in our estimate of $\beta_1$ will increase when $x_2$ is omitted:  on average, our estimate will be off by more.  This is called \emph{omitted variable bias}, and makes intuitive sense: if $x_1$ and $x_2$  affect $y$ but only $x_1$ is included in a model, some of what looks like an effect of $x_1$ could in reality be an effect of $x_2$ (or the effect of $x_1$ could be stronger once $x_2$ is controlled for, etc.).  

\paragraph{Variance}
On the other hand, if $x_2$ does not in reality affect $y$ ($\beta_2 = 0$) or is orthogonal to $x_1$, and we do not omit $x_2$, the degree of variability in our estimate of $\beta_1$ across runs of the experiment (its `variance') will increase.  This follows similar logic to a our discussion of collinearity, where we saw that adding a highly collinear predictor $x_2$ increases the standard error of the effect of $x_1$, because the model is `less sure' what variance in $y$ to attribute to $x_1$ versus $x_2$.  (Unless $x_1$ and $x_2$ are completely orthogonal.)  

\paragraph{Bias-variance trade-off}

Since we usually don't know whether $x_2$ does have an effect or not, the choice of whether to add $x_2$ to the model risks either increasing bias or variance---a specific instantiation of the \emph{bias-variance trade-off}.  Intuitively, too many predictors leads to overfitting to noise, but any method to control overfitting (like dropping some predictors, or using robust regression) introduces bias.
%In terms of Type I/II errors, higher bias/variance tend to make Type I/II errors more likely (spurious results, versus missed effects).
A more quantitative way to think of the bias-variance trade-off is by thinking of the goal of `estimation' as minimizing \emph{mean-squared error}: how off our estimate $\hat{\beta}$ of a parameter (whose true value is) $\beta$ is, on average: 
$E(\beta - \hat{\beta})^2$.  It turns out that \citep[][\S7.3]{hastie2009elements}:
$$
E(\beta - \hat{\beta})^2 = \text{Variance}(\hat{\beta} | \beta) + \text{Bias}(\hat{\beta} | \beta)^2
$$

That is, the mean-squared error is the sum of the variance and the square of bias.  We want an estimator (in our case, a linear regression model with certain predictors) to minimize both variance and bias, hence reducing error; in reality different estimators tend to trade off.  Our goal is to find the sweet spot between bias and variance.

As a concrete example, our VOT models above assume that \ttt{speaking\_rate} affects \tsc{voiced} and \tsc{voiceless} stops equally, but in fact this is probably wrong (Figure~\ref{fig:rate-voicing-resids} right): speaking rate affects voiceless stops much more, which we could account for by adding a \tsc{speaking\_rate:voicing} term to the model. Without this term, our estimate for `\ttt{speaking\_rate} effect for voiced stops' was biased away from 0, and our estimate for `\ttt{speaking\_rate} effect for voiceless stops' was biased towards 0.   On the other hand, there are many possible predictors whose effects might interact with \ttt{voicing}, and aren't orthogonal to speech rate (word frequency, etc.); the more interactions we include with \ttt{voicing} for these terms, the more the standard error of our \ttt{speaking\_rate} effects will go up, and the more variable the coefficient values would be if we repeat the experiment.  
The `best' model is one where the added terms, such as  \ttt{speaking\_rate:voicing} (which from previous work we are pretty sure has a large effect),  decrease variance more than the square of the bias the \textbf{not}-added terms introduce.


% 
% 
% 
% 
% 
% between two problems: the \emph{bias-variance} between  closely related  bias-variance tradeoff: between a model where an individual coefficient has less *bias* in its estimate (if we do the expt many times, on average the coeff value is its true value), and a model where it has less *variance* (how much the estimate varies, across many repetitions of the experiment).
%
% 
% \subsection{Omitted variables}
% \label{sec:omitted-variable-bias}
% 
% There is a `hidden assumption' in most regression models, which can be characterized as a problem involving predictors or the model: that there are no omitted predictors \citep[][39]{harrell2015regression}.  If the model does not include predictors which do in reality help explain the response, and these predictors are not completely orthogonal to those included in the model, the coefficients for the predictors included in the model will be biased.  This is called \emph{omitted variable bias}, and is at some level why we are doing multiple regression: our interpretation of how one predictor affects the response depends on controlling for other predictors.  If $x_1$ and $x_2$  affect $y$ but only $x_1$ is included in a model, some of what looks like an effect of $x_1$ could in reality be an effect of $x_2$; or the effect of $x_1$ could be stronger once $x_2$ is controlled for; or $x_2$ could modulate the effect of $x_1$ (i.e.,\ an interaction).
% 
% As a concrete example, our VOT models above assume that \ttt{speaking\_rate} affects \tsc{voiced} and \tsc{voiceless} stops equally, but in fact this is probably wrong (Fig XX): speaking rate affects voiceless stops much more, which we could account for by adding a \tsc{speaking\_rate:voicing} term to the model. Without this term, our estimate for `\ttt{speaking\_rate} effect for voiced stops' was biased away from 0, and our estimate for `\ttt{speaking\_rate} effect for voiceless stops' was biased towards 0.

% Unfortunately we can never be sure all relevant terms have been included, and adding extra terms to a regression runs other risks (overfitting) which we discuss later (XX).  So the risk of omitted variable bias is just something we have to always keep in mind while building regression models and interpreting their results.


\subsection{Overfitting and underfitting}
\label{sec:overfitting-underfitting}

% - In building any statistical model there is a trade-off between two problems: a model that doesn't fit the observed data well (underfitting), and a model that doesn't generalize well to unobservred data (overfitting). this is easiest to think about in terms of *prediction*. from perpsective of *estimation*, easier to think about the closely related  bias-variance tradeoff: between a model where an individual coefficient has less *bias* in its estimate (if we do the expt many times, on average the coeff value is its true value), and a model where it has less *variance* (how much the estimate varies, across many repetitions of the experiment).

A closely-related trade-off when terms are added/removed is between a model that doesn't fit the observed data well (\emph{underfitting}), and a model that doesn't generalize well to unobserved data (\emph{overfitting}); this is easiest to get an intuition for if our modeling  goal is prediction.  Underfitting we are used to thinking about; this is just goodness of fit on the dataset used to fit the model, captured for example by $R^2$.  For overfitting it's helpful to see an example.

\paragraph{Example}
%\label{sec:overfitting-ex}

Consider the \ttt{vot\_michael} data.  Suppose we had just 50 observations, which we simulate by choosing them randomly; this is the `training' data, and the rest of \ttt{vot\_michael} is the `test' data:

<<>>=
set.seed(100)
inds <- sample(1:nrow(vot_michael), 70)
vot_train <- vot_michael[inds, ]
vot_test <- vot_michael[-inds, ]
@

Now consider three different models of VOT fitted to the test data, each obtained by
adding more predictors to the previous model:
%subset of the previous model, adding with progressively more predictors:
\begin{itemize}
\item Model 1: \ttt{voicing} only ($k=1$)
\item Model 2: add  5 other plausible predictors, based on previous work ($k=6$)
\item Model 3: add all two-way interactions ($k=16$)
\end{itemize}
<<>>=
ou_mod_1 <- lm(log_vot ~ voicing, data = vot_train)
ou_mod_2 <- update(ou_mod_1, . ~ (voicing + speaking_rate + 
    foll_high_vowel + cons_cluster + log_corpus_freq + stress))
ou_mod_3 <- update(ou_mod_1, . ~ (voicing + speaking_rate + 
    foll_high_vowel + cons_cluster + log_corpus_freq + stress)^2)
## show formula, to see how many terms this is
formula(ou_mod_3)
@

Now consider the correlation between predicted and observed values of $y$---that is, $R^2$---on the training and test data (code not shown):
<<echo=FALSE>>=
model_acc <- function(mod, newdata, resp) {
  preds <- predict(mod, newdata = newdata)
  cor(preds, newdata[[resp]])^2
}
@

\begin{tabular}{ccc}
\toprule
& Train $R^2$ & Test $R^2$  \\
\midrule
Model 1 & \Sexpr{model_acc(ou_mod_1, vot_train, 'log_vot')} 
& \Sexpr{model_acc(ou_mod_1, vot_test, 'log_vot')} \\
Model 2 & \Sexpr{model_acc(ou_mod_2, vot_train, 'log_vot')
} &
\Sexpr{model_acc(ou_mod_2, vot_test, 'log_vot')}
\\
Model 3 & \Sexpr{model_acc(ou_mod_3, vot_train, 'log_vot')}
& \Sexpr{model_acc(ou_mod_3, vot_test, 'log_vot')} \\
\bottomrule
\end{tabular}
% 
% $R^2$ for these models on the training data and the test data
% 
% - Helpful to see an example first.  
% - Consider the vot michael data -- suppose we just had 50 observations. Fit three models:
% -- voicing alone
% -- voicing + some plausible terms
% -- throw in all 2-way interactions.

For the training data, $R^2$ is highest for the most complicated model (3), as we expect:  adding more predictors can only increase the proportion of variance explained.   A too-simple model like Model 1  underfits.\footnote{In this case we \textbf{know} Model 1 underfits, by construction. Much previous work suggests \ttt{voicing} is not the only thing which affects VOT.}

But on the test data, as predictors are added $R^2$ goes up and then down---it is Model 2 which has highest $R^2$ on data not used to fit the model. Model 3 has the lowest $R^2$ because it overfits to noise in the training data.

% 
% On the data used to fit the model, $R^2$ increases  as more predictors are increased.  This happens whenever Model 1 to Model 3;  the number of predictors ($k$) increases
% On the training data, R2 always goes up (show) -- model 1 underfits
% 
% On the test data (rest of vot\_michael), R2 goes up, then *down* -- model 3 certainly overfits.

% FUTURE: maybe?
% <<echo=FALSE>>=
% ou_df <- bind_rows(ou_mod_1 %>% tidy() %>% filter(term=='voicingvoiceless') %>% select(estimate, std.error),
% ou_mod_2 %>% tidy() %>% filter(term=='voicingvoiceless') %>% select(estimate, std.error),
% ou_mod_3 %>% tidy() %>% filter(term=='voicingvoiceless') %>% select(estimate, std.error)
% )
% ou_df$model <- c(1,2,3)
% ou_df %>% select(model, estimate, std.error)
% @
% 
% We can also get a sense of the bias-variance tradeoff in this example, focusing on the \ttt{voicing} coefficient. Suppose that the real value $\beta_{voicing}$ is in fact 
% 
% look at Voicing coefficient.  Suppose the right value is about 1.8--1.9 (this estimate comes from a very good model). going from mod 1-2, estimate 
% changes little, SE changes little. but comparing mod 1 to 3, estimate changes a lot, and SE a lot. The estimate is now less biased -- 95\% CI includes the true value -- but high variance. => not significant
% 
% -----

At the extremes (e.g.,\ Models 1, 3), we can see why overfitting and underfitting are bad.  We would like the model which trades off best between them, which  would predict best on unseen data. But we don't typically have unseen data, so we have to somehow find this   sweet spot using just the data we have.

% at the extremes (model 1, model 3), we can see why overfitting and underfitting are bad, either from prediction or estimation perspective. we want sweet spot between them. by any criterion we would *like* to choose model 2 -- but we don't have the unseen data, so have to somehow do this just using the data we have.

\paragraph{Assessing overfitting}


Different criteria covered elsewhere (adjusted $R^2$, $F$-test, AIC, BIC: Section~\ref{sec:goodness-of-fit-metrics}, \ref{sec:nested-model-comparison}, \ref{sec:non-nested-model-comparison}) try to balance over- and under-fitting when comparing different models, but they are not suitable for assessing the degree to which a \textbf{single} model overfits. %While it's not obvious how important not-overfitting is as a desideratum for regression models (Box~\ref{box:overfitting-importance}), it is useful to know some basic ideas (and where to read more).

A useful rule of thumb is that a linear regression model fitted to $n$ observations can contain about $n/15$ predictors before overfitting becomes a concern \citep[][\S4.4]{harrell2015regression}.  This rule of thumb is rough---other factors, especially predictor distributions, affect whether overfitting occurs---but is helpful for critically reading papers, or when a better method is computationally intractable.
%Otherwise a more rigorous method should be used.


There are various more rigorous methods to assess overfitting, called `internal validation' or `external validation', depending on whether you check for overfitting using held-out data \citep[][\S5.3]{harrell2015regression}.  In general external validation is better as long as the test set is large enough and you can truly keep it unseen; otherwise performance on the test set isn't a good metric.  
%This is the ideal aspired to in machine learning.
For regression modeling in scientific research (as well as machine learning for smaller datasets), this option is often unrealistic because data is too expensive to `give up' a substantial fraction and it is hard to keep it truly unseen.
%(REF to McElreath).

%- Two kinds of `validation' to assess overfitting.  `Internal' uses the dataset used to build the model, `External' uses new data (Harrell).  In general external is better, but also harder because you have to `give up' data.

Here are some common `internal' methods, ranked in order of worst to best \citep[][\S5.3]{harrell2015regression}. We describe these methods in terms of $R^2$, but they apply to any goodness-of-fit measure.
\begin{itemize}
\item Simply reporting a measure of goodness of fit (e.g.,\ $R^2$) on the data used to fit the model.  This is the most common in linguistic research.

\item  \emph{Data-splitting}, where the data is split into training and test sets, then  the model is fit and validated using these sub-datasets in some fashion, before refitting on the full dataset. For example, a method used in variationist sociolinguistics to avoid overfitting is to split the dataset in half and fit a model with all candidate terms to each half \citep[e.g.,][]{labov1994plc}. The final model, fitted to the full dataset, only contains terms which were significant in {both} sub-models.

\item \emph{Cross-validation}, where the data is split into $k$ groups. For each group, generate predictions using a model trained on the other $k-1$ groups.  Compute $R^2$ using the full set of predictions.

\item \emph{Bootstrapping}, where we resample from the dataset many (hundreds) of times to make new datasets; then refit the model to each dataset and calculate $R^2$ on the original dataset. This gives a distribution of $R^2$ values, which lets us know how much our observed $R^2$ values (on the original dataset) should be adjusted downwards for overfitting.
\end{itemize}

%whatever procedure for combining models with terms which `validate'. 
%Ex: in variationist sociolinguistics often done 50/50 then only keep terms which are in both modles
% 
% - Some `internal' methods, ranked in 'worst to best' order by Harrell:
% 
% - internal: just reporting goodness of fit measure / accuracy.

% - external: reporting predictive accuracy on a *small* test-sample of `held out' data.  This is common in machine learning.

% - internal: data-splitting, where the model is fit on `training' and `test' data separately, validate on `test' set, before fitting model on full dataset somehow.
% %whatever procedure for combining models with terms which `validate'. 
% Ex: in variationist sociolinguistics often done 50/50 then only keep terms which are in both modles
% 
% - Cross-validation: split the data into $k$ groups, repeatedly train on $k-1$ groups and test  on last group; compute accuracy.
% 
% - or bootstrapping: resample from the data 100s of times, refit model each time, gives a distribution of accuracies or whatever quantity, so we know how much to adjust for overfitting.

For linear and logistic regression models, there is excellent functionality in the {rms} package, discussed  for linguistic data by \citet[][\S6.2.4]{baayen2008analyzing} which makes the bootstrapping option easy.  (Other packages, such as caret, have better functionality but are a bit harder to use.)  We give a practical example in Section~\ref{sec:log-reg-overfitting}, for a case where overfitting is a real concern.  
%where the research questionssec:log-reg-overfitting sec:log-reg-overfitting   This is a feasible option as long as your model doesn't take too long to fit (since it must be refit many times), which is discussed further for linguistic data by \citet[][6.2.4]{baayen2008analyzing}.

<<echo=FALSE, include=FALSE, warning=FALSE, message=FALSE>>=
# To make this demo work we need to remove the 
# foll_high_vowel:stress interaction, which comes out as
# NA from data sparsity
ou_mod_3 <- update(ou_mod_3, . ~ . - foll_high_vowel:stress)

library(rms)
ou_mod_1_ols <- ols(formula(ou_mod_1), data = vot_train, x = T, y = T)
ou_mod_2_ols <- ols(formula(ou_mod_2), data = vot_train, x = T, y = T)
ou_mod_3_ols <- ols(formula(ou_mod_3), data = vot_train, x = T, y = T)
val1 <- validate(ou_mod_1_ols, B = 1000)
val2 <- validate(ou_mod_2_ols, B = 1000)
val3 <- validate(ou_mod_3_ols, B = 1000)
@


For our example (code not shown), $R^2$ adjusted for overfitting using a bootstrap method (from \ttt{rms})  ends up being \Sexpr{val1[1,'index.corrected']}, \Sexpr{val2[1,'index.corrected']}, and \Sexpr{val3[1,'index.corrected']} (Models 1, 2, 3), suggesting that Model 2 trades off the best between underfitting and overfitting. 
%For our example, R2 `adjusted for optimism' ends up being 0.66, 0.67, and -0.64 (!). suggests that Model 2 trades off best between underfitting and overfitting.  
This is not the same thing as `is the best model' (Box~\ref{box:overfitting-importance}), but it's important nonetheless.

\begin{boxedtext}{Broader context: How much should we care about overfitting?}
\label{box:overfitting-importance}

A thornier issue is how important avoiding overfitting is when building regression models for scientific understanding. 
%
% If you google `overfitting', you will mostly find pages about machine learning and artificial intelligence. In these fields, 
% %The vast majority of the time in these fields, 
% prediction on unseen data is almost always the goal. From this perspective it does not make sense to 
% %If you explain common practice in regression modeling to a machine learning practitioner (or have taken a course in this area yourself), chances are they will think it's crazy that it is common to 
% `train and test on the same dataset', whether that means literally getting predictions on a test set (goal = prediction), or assuming that scientific conclusions based on an analysis of a single dataset without held-out data will generalize to `the world' (goal = explanation).   
% If you talk to machine learning friends, they'll think it's crazy that it's common in regression modeling to `train and test' on the same dataset---whether that means literally getting predictions on a test set, or assuming that scientific conclusions based on an analysis of a single dataset without held-out data will generalize to `the world'. And in machine learning, where prediction on unseen data is 100\% the usual goal, it makes sense that overfitting is a major and primary concern.
%
%But in regression modeling for scientific understanding, our goals may be different.  
If we are \textbf{primarily} interested in prediction---building a model which predicts well on unseen data---overfitting
%drawn from the same distribution as our training data---overfitting 
is a primary concern.  This is the usual setting in machine learning and artificial intelligence. 
%Indeed, it's not obvious that regression modeling is the best tool if prediction is the primary goal; 
%it has advantages,
%but 
%there are a variety of powerful modern machine learning methods which may give better predictions than any regression model.
But commonly in language sciences, prediction is not our (only) goal; often estimation/explanation is more important (Section~\ref{sec:regression-general-introduction}).  From the estimation perspective, overfitting does matter but may not be as much of a concern \citep[][\S4.12.2]{harrell2015regression}. Biased estimates may be a larger concern than variable ones, if we are most concerned about Type I errors. And there are often important qualitative desiderata for a regression model: to test specific hypotheses, and to be interpretable in order to give insight into scientific questions. Both considerations can suggest including certain predictors in a model, whether they may lead to overfitting or not \citep[][\S12.4.1]{baguley2012serious}.
%
% But in regression modeling, our goals may be different. If we are 100\% interested in predicton on unseen data -- similar. But  commonly in lingusitics, *estimation* rather than prediction. from estimation perspective, overfitting does matter but isn't as much of a concern (Harrell quote). Biased estimates may be a bigger issue than variable ones, if we are most concerned about Type I errors. Also there are important desiderata for a model: to test hypotheses,
% %(which suggests including certain predictors whether they lead to overfitting or not), 
% and to be interpretable to give insight into scientific Qs---both can suggest including certain predictors whether they may lead to overfitting or not.   
Also, the concept of `unseen' data may not make sense. Sometimes our dataset \textbf{is} all the data (for example, all closed-class words of English), and
%in this case it is unclear whether overfitting is a concern at all.  And 
sometimes (e.g.,\ in observational studies, or when studying highly endangered languages),  we can't afford to actually `hold out' data, which is expensive to collect and includes crucial cases for our scientific questions. Put simply,  regression models developed to optimize prediction 
%if we developed regression models in the same way as machine learning models
(held-out test data, cross-validation to choose parameters, etc.) would not necessarily
%the resulting models would not necessarily 
be optimized for insight into the scientific questions which motivate us.  
%Then again, they might be---optimizing prediction is a well-specified and well-studied goal, and perhaps the models it leads us to may be best for other goals as well.
%(more qualitative) goals as well.

%We tell you all this 
We tell you all this because there are two corresponding sorts of high-level philosophies about regression modeling which you may encounter \citep{shmueli2010explain}, and it is useful to be aware that they exist because they lead to rather different recommendations, neither of which is `right'.  The first is that overfitting is not a primary issue,   unless the researcher's primary goal is prediction.
%how important it is depends on the researcher's goal in fitting the model. 
%As such, model validation is de-emphasized. 
This viewpoint underlies many regression textbooks, such as \citet{chatterjee2012regression,faraway2015linear}, where `overfitting' is barely mentioned.  The second is that the primary goal of modeling is prediction on unseen data, and even if your scientific interest is in estimation, prediction is a well-defined goal, optimizing which often leads to a model that is also optimized for estimation/explanation. This view is held by  others such as \citet{harrell2015regression,pinheiro2000mixed,mcelreath2020statistical}, 
% FUTURE: and Bates more generally
which have been influential in statistics-for-linguists.
% %is a well-defined goal, and devveloping  optimizing which often does what's best for estimation.  
% Books such as Harrell \& McElreath have this view, to some extent Bates, Baayen.  

%In the context of linguistic data, this 
%This is why e.g.,\ Baayen spends a lot of time on overfitting / model validation, but other texts don't.
\end{boxedtext}


\section{Model comparison}\label{sec:lm-model-comparison}


% So far in our discussion of regression, we have assumed that the form of the model is given: we already know what predictors will be used, and what terms will be included in the model. 

In realistic data analysis we usually do know know in advance the exact form of the regression model---which predictors will be used and what terms will be included---so it is necessary to compare several possible models of the same data.
%that differ in which which terms they include. 
Choosing between models is called \emph{variable selection} (or \emph{model selection}). In order to perform variable selection, we need a way to perform \emph{model comparison}: comparing two or more candidate models to assess which one is `better', in the sense of how well it balances fit and parsimony (number of predictors), which is very similar to
balancing bias/variance or overfitting/underfitting.
%the data relative to its expressive power (e.g.,~number of predictors).

We wish to compare models of the same dataset, with the same response ($y$) and different sets of predictors (the $x_i$).
% 
% \begin{itemize}
% \item
%   Same response (\(Y\))
% \item
%   Different sets of predictors (\(X_i\))
% \end{itemize}
Model comparison techniques differ on whether they can compare `nested' models only, or both nested and `non-nested' models.

\hypertarget{nested-model-comparison}{%
\subsection{Nested model comparison}\label{sec:nested-model-comparison}}

Two models are \emph{nested} if one is a subset of the other, in terms of the set of predictors included; they are called the \emph{full} (or \emph{superset}) and \emph{reduced} (or \emph{subset}) models.

A very common case is where the models differ in a single predictor, such as these two nested models predicting \texttt{RTlexdec} for the \texttt{english} dataset:

\begin{align*}
   \quad M_1 ~:~ \texttt{RTlexdec} &= \beta_0 + \beta_1 \cdot \texttt{WrittenFrequency} + \epsilon\\
   \quad M_2 ~:~ \texttt{RTlexdec} &= \beta_0 + \beta_1 \cdot \texttt{WrittenFrequency} + \beta_2 \cdot \texttt{Familiarity} + \epsilon
\end{align*}

To compare these two models, we wish to perform a  test of the null hypothesis that $\beta_2 = 0$. 
%\[
%H_0 : \beta_2 = 0
%\]

More generally, the two nested models can contain any number of predictors:
\begin{itemize}
\item
  \(M_1\): response $y$, predictors \(x_1, ..., x_q\)
\item
  \(M_2\): response $y$, predictors \(x_1, ..., x_k\) (where \(q < k\))
\end{itemize}

We would like to compare these models by testing the null hypothesis that all the additional predictors in the superset model have no effect:
\begin{equation}
H_0~:~\beta_{k + 1} = \beta_{k + 2} = \cdots = \beta_q = 0
\label{eq:nested-null}
\end{equation}

We do so using the residual sum-of-squares (RSS) for each model (equation~\ref{eq:rss-def}), \(RSS_1\) and \(RSS_2\), which measures how well the model fits the data (lower is better). The RSS of a regression model can be divided by \(n-k-1\)---the `degrees of freedom of the model', where $k$ is the number of predictors--- to measure how well the model fits the data given the sample size and number of predictors.

%\[
%RSS/(n-p-1)
%\]
In addition, the difference in RSS values between the two nested models can be scaled to give a measure of how much RSS has gone down, relative to what's expected given the dropped predictors:
\begin{equation*}
(RSS_1 - RSS_2)/(k-q)
\end{equation*}

Thus, the following test statistic measures how much the unexplained variance is reduced in the reduced model, with respect to the full model:
\begin{equation*}
F = \frac{(RSS_1 - RSS_2)/(k - q)}{RSS_2 / (n - k - 1)}
\end{equation*}

 Intuitively, \(F\) divides how much effect `dropping' the \(k-q\) extra predictors has on explained variance.  \(RSS_2\) will always be smaller than \(RSS_1\), since adding predictors to a model always improves fit. The hypothesis test checks whether the superset model fits the data {significantly} better than the subset model, given the added complexity.

This test statistic follows an \(F\) distribution (under the null hypothesis: equation~\ref{eq:nested-null})
%(under the null hypothesis above) 
with \(k-q\), \(n-k-1\) degrees of freedom, written as $F(k-q, n-k-1)$ \citep[e.g.,][\S3.9]{chatterjee2012regression}. We can thus perform hypothesis testing using an \emph{$F$-test}, 
%\href{https://en.wikipedia.org/wiki/F-test}{\(F\)-test}, 
which may be familiar if you have seen ANOVAs, and intuitively checks whether `explained variance' (significantly) increases relative to `unexplained variance'.

For instance, for the \ttt{english\_40} example above, $k-q = 1$ and $n-k-1 = 40 - 2 - 1 = 37$. So an $F$-test would be used, with test statistic $F(1,37)$.
%with $1, 37$ degrees of freedom would be used.  

Another common case of nested model comparison appears at the bottom of the \ttt{summary()} of every linear regression, which tests whether the $k$ predictors together significantly affect the response. This is a comparison of the model with $k$ predictors to a model containing only the intercept.   This $F$-test can be thought of as the `significance' of the regression, so it is reported next to  measures of goodness-of-fit (MSE, adjusted/raw $R^2$), which give complementary information (`effect size').  This $F$-test is usually unsurprising because `significantly different from a model containing just the intercept' is a very low bar, but in some circumstances it can be interesting.

%so it is reported next to (adjusted) $R^2$.   
For example, consider model \ttt{english\_mod\_5} from Section~\ref{sec:credit-assignment-ex}, where no single predictor had a low $p$-value despite the predictors together explaining a lot of variance ($R^2$):

<<output.lines=17:19>>=
summary(english_mod_5)
@

The $F$-test confirms that the $k=3$ predictors together significantly reduce unexplained variance.  
%The MSE and $R^2$ measures give complementary information (effect sizes).   
  % What are the reduced model and full model in the \(F\) test reported at the bottom of every linear regression's output?

\subsubsection{Effect size}
\label{sec:cohens-f}

The $F$-test above only addresses the significance of the difference between two nested linear regressions, but does not give an effect size, which is important to report with any hypothesis test (Section~\ref{sec:reporting-hypothesis-tests}).

An intuitive effect size in this case
%comparing two nested linear regressions 
is the fraction by which the unexplained variance, $1-R^2$, increases when the extra predictors are dropped from the full model.  If we write $R^2$ for models $M_1$ and $M_2$ as $R_1^2$ and $R_2^2$, this measure is:
\begin{equation}
f^2 = \frac{R_2^2 - R_1^2}{1-R_2^2}
\label{eq:cohens-f}
\end{equation}

We'll call this \emph{Cohen's $f$} (one of a family of measures with this name). A rule of thumb is that $f^2$ of 0.01, 0.06, and 0.16 are small/medium/large \citep{cohen1992power}, keeping in mind the caveats on rules of thumb (Section~\ref{sec:effsize-rules-of-thumb}).

There is not an R function to calculate $f^2$ for this exact case, but it's simple to write one ourselves (using the \ttt{r2()} function from {performance}):

<<>>=
## m1, m2: full model, reduced model
cohens_f2 <- function(m1, m2) {
  as.numeric((r2(m2)$R2 - r2(m1)$R2) / (1 - r2(m2)$R2))
}
@

Also commonly used are variants of $\eta^2$, but these are less intuitive.

\subsubsection{Examples}

Suppose for the purposes of these examples that $\alpha=0.01$. To do  model comparisons in R you fit nested models and then conduct the $F$-tests using \ttt{anova()}. For the first \ttt{english} example above:

<<>>=
m1 <- lm(RTlexdec ~ WrittenFrequency, english)
m2 <- update(m1, . ~ . + Familiarity)
anova(m1, m2)
@

This model comparison suggests that adding \ttt{Familiarity} to \ttt{m1} significantly reduces its unexplained variance.  Cohen's $f^2$ is \Sexpr{cohens_f2(m1, m2)}, %(\ttt{cohens\_f2(m1, m2)}), 
suggesting this is a small effect.
<<>>=
cohens_f2(m1, m2)
@


This kind of model comparison, to check if dropping or adding a single term to a regression model is justified, is extremely common in practice.
%% (FUTURE: exercise - not the case if you use english\_40 instead)  


As an example comparing two models differing in multiple terms, let's compare the first and second models from our overfitting example (Section~\ref{sec:overfitting-underfitting}), which used $q=1$ and $k=6$ predictors to explain log(VOT):

<<output.lines=6:8>>=
anova(ou_mod_1, ou_mod_2)
@

Adding the five predictors \textbf{together} does not significantly improve the model (at $\alpha = 0.01$)---even though there are individual predictors (like \ttt{cons\_cluster}) which would significantly improve the model if added in isolation, and the effect size of adding these predictors is large ($f^2 = \Sexpr{cohens_f2(ou_mod_1, ou_mod_2)}$).

This kind of comparison is sometimes done for testing whether a group of conceptually-related predictors together improve a model.  Another common case is testing whether a factor with 3 or more levels improves a model (Section~\ref{sec:omnibus-post-hoc}).
% we dicuss in Chapter~\ref{chap:practical-regression-topics} on multi-level factors).



It is useful to know that \texttt{anova()} can be used for nested model comparison of 3+ models as well (example in Section~\ref{sec:f-test-comparison-ex}), e.g.,\ \ttt{anova(ou\_mod\_1, ou\_mod\_2, ou\_mod\_3)}, in which case \ttt{ou\_mod\_2} is compared to \ttt{ou\_mod\_1} and \ttt{ou\_mod\_3} to \ttt{ou\_mod\_2}.


%\texttt{anova(x,y,z)} can be used as shorthand when \texttt{x}, \texttt{y}, \texttt{z} are nested models, and so on (\texttt{anova(w,x,y,z)}\ldots).


\begin{boxedtext}{Practical notes: $F$-tests and nested model comparison}

In linguistics and psychology, the $F$-test is sometimes reported without comment or \emph{model comparison} used as shorthand, in the case of  nested model comparison via an $F$-test to  compare two linear regressions.
%because this is the most common method. It is also common to just report an $F$-test when the effect of 1+ predictors are evaluated.
%. We will sometimes use this shorthand as well.


<<echo=FALSE>>=
ou_mod_comp <- tidy(anova(ou_mod_1, ou_mod_2))
df1 <- as.numeric(ou_mod_comp[2,'df'])
df2 <- as.numeric(ou_mod_comp[2,1])
sumsq <- as.numeric(ou_mod_comp[2,'sumsq'])
@

To report a model comparison, you follow the same guidelines as for any NHST test (Section~\ref{sec:reporting-hypothesis-tests}), reporting at minimum degrees of freedom, test statistic,  $p$-value, and effect size.  For instance, for the second example above:
\begin{quote}
{\footnotesize
``These five additional predictors expected to affect VOT based on prior research did not significantly improve the model of log(VOT) at the $\alpha=0.01$ level ($F(\Sexpr{df1}, \Sexpr{df2}) = \Sexpr{sumsq}$, \Sexpr{formatP(as.numeric(ou_mod_comp[2,'p.value']))}), though the effect size of adding these predictors was large ($f^2 = \Sexpr{cohens_f2(ou_mod_1, ou_mod_2)}$).''
}
\end{quote}

In current practice it is common to report model comparisons without an effect size.
%(in which case the last clause would be left out of the report). 
%such as (for the second example above):

<<echo=FALSE>>=
english_mod_comp <- tidy(anova(m1, m2))
df1_1 <- as.numeric(english_mod_comp[2,'df'])
df2_1 <- as.numeric(english_mod_comp[2,1])
sumsq_1 <- as.numeric(english_mod_comp[2,'sumsq'])
@

%A report including Cohen's $f^2$ as an effect size would look like this for the first example above:
%We should include an effect size in such reports. A simple one which works for any nested model comparison is the change in adjusted-$R^2$.  For example, for the first example above:
% \begin{quote}
% {\footnotesize ``Word familiarity affected lexical decision RT beyond the effect of word frequency (Cohen's $f^2$ = \Sexpr{cohens_f2(m1, m2)}, $F(\Sexpr{df1}, \Sexpr{df2}) = \Sexpr{sumsq_1}$, \Sexpr{formatP(as.numeric(english_mod_comp[2,'p.value']))}).''}
% \end{quote}

% The results of such a hypothesis test are usually reported in parentheses, like ``Subject age and the word's length in letters together affect RT beyond word frequency \((F(2, 4564) = 3663, p<0.0001)\)''.
\end{boxedtext}

% 
% 
% \hypertarget{c2ex1}{%
% \subsubsection*{Example}\label{c2ex1}}
% \addcontentsline{toc}{subsubsection}{Example}
% 
% This model comparison addresses the question: does adding \texttt{AgeSubject} and \texttt{LengthInLetters} to \texttt{m1} significantly reduce its unexplained variance?
% 
% <<>>=
% m1 <- lm(RTlexdec ~ WrittenFrequency, english)
% m2 <- lm(RTlexdec ~ WrittenFrequency + AgeSubject + LengthInLetters, english) 
% anova(m1, m2)
% @
% 
% The highly significant \(F\)-test means that adding these two variables does significantly improve the model.
% 
% \textbf{Practical notes}:
% 
% \begin{itemize}
% \item
%   In experimental literature (at least in linguistics), ``model comparison'' is often used as shorthand for ``nested model comparison via an F test'' when comparing two linear regressions, because this is the most common way to do so. We will sometimes use this shorthand as well.
% \item
%   The results of such a hypothesis test are usually reported in parentheses, like ``Subject age and the word's length in letters together affect RT beyond word frequency \((F(2, 4564) = 3663, p<0.0001)\)''.
% \end{itemize}
% 
% ----


\hypertarget{non-nested-model-comparison}{%
\subsection{Non-nested model comparison}\label{sec:non-nested-model-comparison}}


In non-nested model comparison, one model isn't a subset of the other. For example, these three models of \ttt{RTlexdec} for the \ttt{english\_40} data are not nested, because each uses a different set of predictors:
<<>>=
nn_m1 <- lm(RTlexdec ~ WrittenFrequency + Familiarity, english_40)
nn_m2 <- lm(RTlexdec ~ WrittenFrequency + LengthInLetters, english_40)
nn_m3 <- lm(RTlexdec ~ Familiarity + LengthInLetters, english_40)
@


Non-nested models can no longer be compared using sums-of-squares (via an \(F\) statistic). Instead, a very different approach is used: compare models using a metric that captures how well they meet two conflicting goals we saw in Section~\ref{sec:overfitting}:  fit (fitting the data as well as possible) and parsimony (have as few predictors as possible).  We have seen one such metric already: adjusted $R^2$ (Section~\ref{sec:goodness-of-fit-metrics}), which measures  how much variance is explained given the number of predictors.

More commonly used are \emph{information criteria}, which
%Different information criteria measures 
combine model likelihood ($L$: higher is better) and number of predictors ($k$: lower is better) into a single value.  The most widely-used criteria are the \emph{Akaike information criterion} (AIC):
\begin{equation}
AIC = 2k - 2 \log(L),
\label{eq:AIC}
\end{equation}
and the \emph{Bayesian information criterion} (BIC):
\begin{equation}
BIC = k\log(n)  - 2 \log(L),
\label{eq:BIC}
\end{equation}
where $n$ is the number of observations. 

`Corrected AIC' (AICc; use \ttt{AICc()} in the {MuMIn} package) is a variant of AIC, recommended for small enough sample size ($n/df < 40$: \citealp{burnham2002model}):
\begin{equation}
AICc = 2k \left[1 + \frac{k+1}{n-k-1}\right] - 2 \log(L)
\label{eq:AICc}
\end{equation}
Written this way, we see that AICc is AIC increased by a penalty term that goes to zero as sample size increases.\footnote{$n-k-1$ in this equation is $df$, for a linear regression.}
 
To apply an information criterion you calculate its value for each model in a set of candidate models, and pick the model with the lowest value.  Information criteria can be used to compare both nested and non-nested models.

%Note that information criteria can be used to compare models regardless of whether they are nested or non-nested.

\paragraph{Example}
% 
% \hypertarget{c2ex4}{%
% \subsubsection*{Example}\label{c2ex4}}
% \addcontentsline{toc}{subsubsection}{Example}

Consider the three candidate models used in our overfitting example (Section~\ref{sec:overfitting-underfitting}). We can use AIC, AICc, or BIC to compare the models:
% 
% Consider four possible models of \ttt{RTlexdec} for the \ttt{english} data:
% 
% <<>>=
% m1 <- lm(RTlexdec ~ WrittenFrequency, english) 
% m2 <- lm(RTlexdec ~ WrittenFrequency + AgeSubject, english)
% m3 <- lm(RTlexdec ~ WrittenFrequency + AgeSubject + LengthInLetters, english)
% m4 <- lm(RTlexdec ~ WrittenFrequency + LengthInLetters, english)
% @
% 
% Using AIC and BIC to compare the models:

<<>>=
AIC(ou_mod_1, ou_mod_2, ou_mod_3)
@

<<>>=
AICc(ou_mod_1, ou_mod_2, ou_mod_3)
@

<<>>=
BIC(ou_mod_1, ou_mod_2, ou_mod_3)
@

AICc is a better choice than AIC here since sample size is small for these models; using either criterion 
%Using AIC c1
we would choose \ttt{ou\_mod\_2} (with 7 predictors) .
%because it has the lowest value. 
Using BIC we would choose \ttt{ou\_mod\_1} (with 1 predictor).   This illustrates a couple of points: different model selection criteria don't necessarily give the same answer, and BIC tends to choose more parsimonious models than AIC, because it penalizes additional parameters more. (AICc lies between the two.)
% AICc  penalizes more than AIC and less than BIC, and reduces to AIC for large $n$.\footnote{\citeauthor{burnham2002model} argue AICc should be the default, as AIC = AICc anyway if the dataset is large enough.}
%NNote that BIC pen is because the BIC penalty is larger than the AIC penalty for the same number of parameters ($k$) as long as the sample size is not tiny ($n>7$): $2k$ vs.\ $k \log(n)$.}\footnote{T}

These are nested models, but we could do the same thing to compare non-nested models, such as \ttt{nn\_m1}, \ttt{nn\_m2}, and \ttt{nn\_m3} defined above.  We would choose Model 3 whether AIC, AICc, or BIC is used (not shown).
%we would choose Model 3 using  AIC or BIC.

\begin{boxedtext}{Broader context: Which information criterion to use?}

There is a vast literature on information criteria and their use for model selection (e.g.,\ \citealp{burnham2002model}; \citealp[][chap.~6]{mcelreath2020statistical}).
% FUTURE: better REFS here, obviously, also after complex and much-debated. would be better to have a couple accessible summaries
%(REFS).  
Besides AIC/AICc/BIC some common ones are
%AICc (= AIC corrected for sample size),
DIC, and WAIC.  Which one is `better' is a complex and much-debated issue, which depends on your modeling goals and the type of data you're modeling. It is also an important issue because AIC(c) and BIC often give different results; this is not an analytical choice where we can say `if the choice you make affects the conclusions you can draw, beware'.  Things are even less clear for more complex regression models (e.g.,\ mixed-effects models: Section~\ref{sec:lmm-information-criteria}).  
%To my knowledge there is not a good discussion of any of this for linguistic data, so 
If model selection is important for your analysis you should read up on what method makes sense.  

% Cut - Michaela advice
% For example, in \citet{sonderegger2017medium} we used AIC to choose between different models of how much the accents of individual speakers vary over time, because the existing literature suggested AIC was best for comparing the kind of models we were fitting (GAMMs).  This method led us to conclude there was a lot of variation over time, which is a major finding of the paper.  But if we had used BIC we would have found a lot less variation over time.

At least for non-mixed-effects models, a crude rule of thumb is that AIC is better when the goal is prediction and BIC when the goal is estimation (\citealp{shmueli2010explain}; \citealp[][\S10.3]{faraway2015linear}).  The intuition is that AIC will tend to keep terms in that help in prediction even while increasing the variance of `large' effects you'd like to estimate, while BIC will remove such terms leading to much more precise (lower-variance) estimates.  (But BIC might eliminate terms that you want to estimate given your research questions, showing a limitation of this rule of thumb.)  Another rule of thumb is that AIC is better when the true model contains effects of progressively smaller magnitude (`tapering'), while BIC is better when the true model contains zero or a few large effects, and all other effects are zero \citep{burnham2002model}.  The `tapering' scenario sounds more plausible for corpus data, but I am not sure about other kinds of linguistic data.

\end{boxedtext}

% 
% 
% \begin{itemize}
% \item
%   Different model selection criteria don't necessarily give the same answer.
% \item
%   BIC tends to choose more parsimonious models than AIC.
% \end{itemize}

\section{Variable selection}
\label{sec:variable-selection}
%\label{c2varselect}}

Now that we have seen some methods to compare models with different sets of predictors, we can turn to variable selection: how to decide what predictors to keep in a given model.

There is no single best way to do variable selection, out of context---each method has pros and cons, and what method to use depends on the goals of your study: both in terms of your research questions, and whether your objective is prediction, explanation/estimation, or description (Section~\ref{sec:regression-general-introduction}).

A priori, your research questions will help determine which predictors you consider for your model, and---depending on your modeling philosophy---may dictate that you leave certain predictors in the model no matter what.  When your objective is explanation, as is often the case for linguistic studies, typically it does not make sense to consider dropping terms which directly test your research questions---regardless of significance---because these effect estimates are effectively why you are doing the analysis \citep[][\S12.4.1]{baguley2012serious}.  
% definitely a refernce for that but i forget where, maybe it's in my notes
% FUTURE: REF for that

But, even given your research questions, there are very different methods for variable selection, and it is important to know that these methods can 
%to variable selection
%It is important to know that different variable selection methods 
result in different final models, appropriate for different goals.   Many papers which report variable selection simply say what they did, without justification. It is up to the reader to think, what might the final model be if a different variable selection method were used?  We already saw one example above: using AIC for model selection tends to lead to `larger' models than using BIC. 

% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   \textbf{Prediction} (estimating \(Y\) for unseen data as accurately as possible)
% \item
%   \textbf{Explanation} (choosing the right model from one of several possible pre-specified choices)
% \item
%   \textbf{Exploratory} study
% \end{enumerate}

% In \#1 and \#2, our higher-level goal is generalization about ``the world'' (unseen data). In \#3, we are interested primarily in the data we have, and don't claim that our results generalize to the world. We'll assume going forward that our goal is \#1 or \#2---many studies in language sciences are in fact exploratory, but at present this isn't a goal that leads to publication.
% 
% It is important to know that different variable selection methods result in different final models, appropriate for different goals, because most papers that report variable selection simply say what they did, without justification. It is up to the reader to think, what might the final model be if a different variable selection method were used?

There are three \textbf{components} of any variable selection method: 
\begin{enumerate}
\item How models are \textbf{compared} (e.g.,~\(F\) test on nested models, AIC).
\item How the \textbf{quality} of a single model is evaluated.
\item How it's decided which of a set of models to \textbf{select}, based on Components 1 and 2.
\end{enumerate}
   
% \begin{table}
% \begin{tabular}
% Components & Examples
% 1. How are models compared?  &
% $F$-test, AIC, ... \\
% 2. How is the quality of a single model evaluated? &
% domain knowledge, residual diagnostics \\
% 3. How is a final model chosen given \#1 and \#2 ? \\
% Automatic
% \end{tabular}
% \end{table}
% 
% \end{tabular}
%   
  
We have already seen a few possible methods for variable selection, by comparing models containing different sets of variables (Component 1):
\begin{itemize}
\item Comparison using $F$-tests (nested linear regression models only)
\item Comparison of information criterion values (e.g.,\ AIC, BIC)
\item Comparison of adjusted-$R^2$ values
\end{itemize}
In each example we implicitly used `choose automatically' for Component 3.

We'll consider a couple more methods here.  The first, stepwise model selection, is widely-used but problematic. The second  is an example of `holistic' variable selection methods which take theory and domain knowledge into account. This type of method is not
%which are not 
currently in wide use in linguistic data analysis, but should in my opinion %think should
be the default.

% \subsection{Nested model comparison}
% \label{sec:nested-model-comparison}
% 
% As noted above, ``model comparison'' is often used as shorthand for ``comparison of nested models using F tests'', when referring to linear regressions.
% 
% \hypertarget{exercise-5}{%
% \subsubsection*{Exercise}\label{exercise-5}}
% \addcontentsline{toc}{subsubsection}{Exercise}
% 
% Which model of \texttt{m1}, \texttt{m2}, \texttt{m3} (from \protect\hyperlink{c2ex4}{the example above}) would you choose based on nested model comparison (\(F\) test)?
% 
% Note: \texttt{anova(x,y,z)} can be used as shorthand when \texttt{x}, \texttt{y}, \texttt{z} are nested models, and so on (\texttt{anova(w,x,y,z)}\ldots).

\subsection{Stepwise variable selection}
\label{sec:stepwise-variable-selection}

% \hypertarget{method-2-stepwise-variable-selection}{%
% \subsubsection{Method 2: Stepwise variable selection}\label{method-2-stepwise-variable-selection}}

The most common type of variable selection procedure, \emph{stepwise} variable selection, 
%(at least in linguistics) 
also uses `choose automatically' for Component 3 above (how do we decide which model to prefer).
%In \emph{stepwise} variable selection,
Using this method we decide whether to add or drop terms based on a model comparison procedure (Component 1): AIC, BIC, \(F\) test with \(p<0.05\), etc.

Stepwise model selection can be conducted `forwards' (start with an intercept-only model and add terms), `backwards' (start with a full model and drop terms), or both.
%empty model (intercept only) and add terms, and \emph{backwards}, where you start with a full model and drop terms, or both.

A common option is that used by the \ttt{step()} function in R by default, stepwise backwards selection using AIC:
%
% \subsubsection{Example: Stepwise backwards selection using AIC}
% % \label{example-stepwise-backwards-selection-using-aic}}
% % \addcontentsline{toc}{subsubsection}{Example: Stepwise backwards selection using AIC}
% 
% This is what the \texttt{step} function in R does by default.
%
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start with the complete regression model (all possible predictors) and obtain AIC.
\item
  At each `step', remove the predictor which would decrease AIC the most.
  %try all possible ways of removing one of the predictors, and whichever yields the lowest AIC value is kept.
\item
  Repeat steps 1--2 until the model has lower AIC than any of the models resulting from removing a predictor.
\end{enumerate}

Stepwise model selection is very popular, because it is easy to do using statistical software, doesn't require much thought, and feels  objective.  But it has serious drawbacks, as we illustrate through an example. Box~\ref{box:stepwise} gives more context.
% 
% 
% 
% We illustrate some drawbacks through an example; Box~\ref{box:stepwise} gives more context.

\paragraph{Example: 34 potential predictors}

One basic property a variable selection method should have is consistency: if we apply the method to two similar datasets, we should get similar results.  In this example we split the \ttt{english} data into two random halves, then fit a full model of lexical decision time (\ttt{RTlexdec}) with 34 potential predictors to each half, and apply stepwise backwards selection.  This procedure simulates repeating the experiment twice which generated the \ttt{english} dataset.

The 34 predictors are simply most of the columns of the \ttt{english} data, all presumably included in the dataset because they might help explain reaction time.\footnote{Note that this is not an unreasonable number of predictors, since we could have up to about 150 predictors before worrying about overfitting (by the divide-by-15 rule).}

<<>>=
## Split the English data in half, randomly
set.seed(2903)
english_half_1 <- english %>% sample_n(nrow(english) / 2)
english_half_2 <- english[!(row.names(english) %in% 
    row.names(english_half_1)), ]

step_mod_1 <- lm(RTlexdec ~
WrittenFrequency + Familiarity + AgeSubject +
  FamilySize + DerivationalEntropy + WrittenSpokenFrequencyRatio +
  WordCategory + InflectionalEntropy + NumberSimplexSynsets +
  NumberComplexSynsets + LengthInLetters + Ncount +
  MeanBigramFrequency + FrequencyInitialDiphone +
  FrequencyInitialDiphoneWord + FrequencyInitialDiphoneSyllable +
  Obstruent + Voice + CV + Frication + CorrectLexdec +
  ConspelV + ConspelN + ConphonV + ConphonN +
  ConfriendsV + ConfriendsN + ConffV + ConffN +
  ConfbV + ConfbN, english_half_1)

step_mod_2 <- update(step_mod_1, data = english_half_2)
vars <- names(step_mod_1$coefficients)

step_mod_1 <- step(step_mod_1, trace = 0)
step_mod_2 <- step(step_mod_2, trace = 0)
@

To compare the two resulting models you should examine their summaries (not shown):
<<eval=FALSE>>=
summary(step_mod_1)
summary(step_mod_2)
@

Here we do some basic comparisons.  Consider the set of variables selected by each model:
<<>>=
vars_1 <- tidy(step_mod_1) %>% .$term
vars_2 <- tidy(step_mod_2) %>% .$term

## Variables selected by both
intersect(vars_1, vars_2)

## Variables selected in one model but not the other
union(setdiff(vars_1, vars_2), setdiff(vars_2, vars_1))
@

While the models agree on 29 potential predictors (13 included, 16 excluded), they differ on 5 others.  This is worrying given that the main goal of variable selection is to end up with a set of predictors. 
Furthermore, 2 of these 5 predictors have $p<0.01$ and two have $p$ around 0.05, so if we used $p$-values to decide which terms are `important' we would make different qualitative conclusions from the two models.

Now consider the terms which both models have `kept'.  Ideally the two models would give similar results for the same term.  We consider just the coefficient value ($\hat{\beta}$) and $p$-value for exposition.   Comparing the coefficient tables should show that while the broad qualitative conclusions are largely the same (coefficients have the same sign, and have the same significance using a $\alpha = 0.05$ criterion), their actual coefficient values and $p$-values differ greatly. 
% FUTURE: consider cutting the following sentence + R code (Micheala).  Is there a much shorter way to make this point or push to exercise?
To quantify this, we compute the ratio of coefficient values (\ttt{coeff\_rat}) and the log of the ratio of $p$-values for the two models (\ttt{log\_p\_rat}):
<<>>=
tidy(step_mod_1) %>%
  left_join(tidy(step_mod_2), by = "term") %>%
  na.omit() %>%
  mutate(
    coeff_rat = estimate.x / estimate.y,
    log_p_rat = log(p.value.x) - log(p.value.y)
  ) %>%
  select(term, coeff_rat, log_p_rat)
@

If the models gave the same result, \ttt{coeff\_rat} would be near 1 and \ttt{log\_p\_rat} would be near 0, but this is far from the case.  Our estimated effect  for \ttt{MeanBigramFrequency} differs by 20\% between the two models, the $p$-value for \ttt{CV} is 0.06 versus 0.01, and so on.  

In sum, stepwise variable selection can easily give unreliable models which don't meet even the basic standard of consistency.


% 
% 
% \begin{itemize}
% \item
%   How similar or different are the resulting terms in the model, and coefficient values?
% \item
%   Why is the degree of (non-)similarity a problem?
% \end{itemize}

% \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

% \citet[][67]{harrell2015regression}
% \begin{quote}
% Stepwise variable selection has been a very popular technique for many years, but if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing.
% \end{quote}

\begin{boxedtext}{Broader context: Why stepwise is unwise}
\label{box:stepwise}

Statistical texts generally cover stepwise variable selection because it is so widely used, while strongly recommending against it. \citet[][67]{harrell2015regression} is representative: ``[stepwise variable selection]...
%procedure had just been proposed as a statistical method, it would most likely be rejected because it
violates every principle of statistical estimation and hypothesis testing.''
% \begin{quote}
% {\footnotesize ``Stepwise variable selection has been a very popular technique for many years, but if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing.''}
% \end{quote}

%Among the problems is that 
Harrell then lists eight major problems with stepwise variable selection, including that  every aspect of the resulting model (coefficient estimates, $p$-values, standard errors) is biased, usually anti-conservatively (i.e., $p$-values are too low, confidence intervals are too narrow), meaning that one easily finds spurious effects. Essentially, automatically dropping terms with high \(p\)-values leads to lowered $p$-values for remaining terms.  Stepwise procedures can also (to a lesser extent) miss true effects. 

But in general, stepwise model selection  
%(and to some extent any fully-automatic model selection procedures) 
is dangerous, and some statisticians recommend it not be used at all.   It can be helpful as a tool in some settings, like selecting among a huge set of predictors as a first pass, but should never be used as the only tool for variable selection. We tend to agree with \citet[][277]{winter2019statistics} that, ``Simply put, linguists should not use stepwise regression.'' 
%At a minimum, automatic model selection procedures should never be used alone for variable selection, in the absence of other methods such as priori theorizing and careful exploratory data analysis. 

% Nonetheless, stepwise procedures have been widely used in linguistics (as in many fields), so it is a good idea to critically examine any model arrived at in this way in a paper. For example,
% %%are there terms that should have been included, based on previous work---regardless of the result of a model comparison? 
% does a key result for a research question rely on a term in the model with borderline significance, whose $p$-value might have been higher if other terms were not excluded by stepwise selection?   The good news is that very broad conclusions drawn from a model (which predictors have large effects, of what sign? which have $p$ much smaller or larger than 0.05?) are often the same whether the model was arrived at by stepwise  selection or another method.
% %%but more you don't need to immediately be suspicious of all results
% %Are there terms that should have been included, based on previous work, regardless of the result of stepwise selection?

\end{boxedtext}
%Are there terms with borderline significance, that could be spuriously included due to automated selection? 


% %(from elsewhere) : It can be helpful to select among a huge set of predictors as a first pass..
% 
% In general, fully automatic model selection procedures are \textbf{dangerous}: they can easily find spurious effects, because automatically dropping terms with high \(p\)-values leads to inflation (= lower values) of significances for remaining terms. Stepwise procedures can also (to a lesser extent) miss true effects.
% 
% Some statisticians recommend that automatic model selection should never be used at all, as it is simply too easy and too dangerous. 
% 
% At a minimum, automatic model selection procedures should never be used \textbf{alone} for variable selection---that is, in the absence of another method, such as careful exploratory data analysis. Despite this, stepwise procedures are widely used in language research, and you should critically examine any model arrived at in this way.\footnote{For example: are there terms that should have been included, based on previous work---regardless of the result of a model comparison? Are there higher-order interactions with borderline significance, that could be spuriously included due to automated selection?}

\subsection{Holistic approaches---\citet{gelman2007data}}
\label{sec:gh-model-selection}

Better than automated variable selection are holistic approaches where you combine quantitative tests with thinking about your data and problem setting. 
%Such `holistic' approaches are subjective, but statistical modeling is inherently subjective.
%
An example is the approach suggested by
\citet[][69]{gelman2007data}, which takes common sense into account in evaluating model quality (Component 2 above) in addition to quantitative model comparison (Component 1):
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Include all predictors that, \textbf{for substantive reasons}, are expected to be important, e.g.,\ they are part of the study's design or  are covariates known from previous work to have large effects on the response.\footnote{For example, any phonetic study of vowel duration where the research questions are based on effects of predictors $x_1$ and $x_2$ should include these predictors (regardless of significance), as well as (minimally) measures of speech rate and vowel height because these have large effects on vowel duration.}
\item
  For predictors with large effects, consider interactions as well.
\item
  Now decide whether to exclude predictors, one at a time:
  \begin{itemize}
  \item
    Not significant, coefficient has expected sign: Consider leaving in.
  \item
    Not significant, wrong sign: Remove.
  \item
    Significant, wrong sign: Think hard if something's wrong.
  \item
    Significant, right sign: Keep in.
  \end{itemize}
\end{enumerate}


This approach requires you to have a sense of what the `right and `wrong' sign of different coefficients are, ideally for substantive reasons (previous work suggests an effect direction).  It is also possible to decide what `right' and `wrong' are based on exploratory plots, but if your sense of right/wrong for a given model coefficient is based \textbf{solely} on exploratory data analysis, remember that you may simply be modeling this dataset well 
%(
%\textbf{purely} on EDA, it's important to remember that you may be simply modeling this dataset well 
(goal = description/exploratory analysis) rather than obtaining results that generalize to unseen data (goal = prediction or explanation).  

\subsection{Example: Building a VOT model}

Let's consider the models we would build under different strategies above, to address a concrete research question for the \ttt{vot\_michael} data: \textbf{what factors modulate the voicing contrast}---is the \tsc{voiced}/\tsc{voiceless} VOT difference larger in slower speech, for higher-frequency words, and so on?  In terms of variables in this dataset, this means `What interactions are there with \ttt{voicing}?'  We model $y$ = log(VOT) for the reasons discussed in Section~\ref{sec:vot-xfm-example}.

The variables we consider as possible modulators of \ttt{voicing} for this exercise are \ttt{speaking\_rate}, \ttt{foll\_high\_vowel}, \ttt{cons\_cluster}, \ttt{place}, and \ttt{log\_corpus\_freq}.  Only one of these, speaking rate, is certain to modulate \ttt{voicing} from previous work.

We consider the three different model selection strategies introduced above. Let's first `rescale' all predictors, as defined in 
%center the two continuous predictors and divide by 2 SD so their coefficients will be comparable with the others (
Section~\ref{sec:centering-scaling},
%, as well as make the factors numeric
so that their coefficients are comparable as effect sizes:
%so the model table is more legible:

<<>>=
vot_michael <- vot_michael %>%
  mutate(
    speaking_rate = rescale(speaking_rate),
    log_corpus_freq = rescale(log_corpus_freq),
    ## Categorical variables -> 0/1-valued
    voicing = rescale(voicing),
    cons_cluster = rescale(cons_cluster),
    foll_high_vowel = rescale(foll_high_vowel),
    place = rescale(place)
  )
@

In the following we do not consider any three-way interaction terms,  to keep things simpler.

\subsubsection{Model comparison}
\label{sec:f-test-comparison-ex}

We consider these three candidate models:
<<>>=
mc_mod_1 <- lm(log_vot ~ voicing * speaking_rate + foll_high_vowel +
    cons_cluster + log_corpus_freq + place, data = vot_michael)
mc_mod_2 <- update(mc_mod_1, . ~ . + voicing:(foll_high_vowel + 
    cons_cluster + log_corpus_freq + place))
mc_mod_3 <- update(mc_mod_2, . ~ . + (foll_high_vowel + 
    cons_cluster + log_corpus_freq + place)^2)
@

These are all reasonable candidate models a priori:
\begin{enumerate}
\item Only speaking rate modulates voicing.
\item All variables modulate voicing.
\item All variables modulate voicing once we control for all possible two-way interactions.
\end{enumerate}

Taking $\alpha=0.05$ we would choose Model 3:
<<output.lines=12:17>>=
anova(mc_mod_1, mc_mod_2, mc_mod_3)
@

In this model, the interactions with \tsc{voicing} are:
<<>>=
tidy(mc_mod_3) %>% 
  # Regular expression to grab "voicing...:" terms
  filter(str_detect(term, "voicing.*:"))
@

Using $p<0.05$ as a filter, we
%Using the $p$-value, 
%$\alpha = 0.05$
%we 
would conclude that \ttt{place},  \ttt{cons\_cluster}, and \ttt{speaking\_rate}, and
%, and \ttt{log\_corpus\_freq} 
modulate \ttt{voicing}, \ttt{place} most strongly (highest effect size).
%with the highest effect size ofr \t---frequency less so (in terms of effect size) than the other variables.

\subsubsection{Stepwise variable selection}

Now we obtain a model by applying stepwise backwards model selection to the largest model (\ttt{mc\_mod\_3}):

<<results='hide'>>=
# Remove 'trace=0' to see details about terms dropped
step_mod <- step(mc_mod_3, trace = 0)
@

In this model, the interactions with \tsc{voicing} are:
<<>>=
tidy(step_mod) %>%
  filter(str_detect(term, "voicing.*:"))
@

If we take `X:\ttt{voicing} remains in the stepped-down model' to mean that `X modulates \ttt{voicing}' for our research question, we would conclude that the same three variables modulate \ttt{voicing} as we did using model comparison, as well as \ttt{log\_corpus\_freq}. The frequency effect is `weaker' than the others, with a smaller effect size and $p> \alpha$.
%in terms of effect size and $p$-value.
%similar effect sizes. However, the frequency modulation of \ttt{voicing} is even more tentative, as now $p>0.05$. 

%`X:\ttt{voicing} remains in the model \textbf{and} has $p< \alpha$' (with $\alpha = 0.01$, say) to mean that X modulates \ttt{voicing}.  Either \ttt{voicing} is modulated by speaking rate, a following consonant cluster, and word frequency, with roughly equal effect sizes, or \ttt{voicing} is modulated by speaking rate, word frequency and a combination of properties of the following phones (\ttt{foll\_high\_vowel}, \ttt{cons\_cluster}).  We assume the first interpretation, for simplicity.

%Note that although the model-comparison and stepwise methods reach the same conclusion about which variables modulate the effect of \ttt{voicing}, they differ in the relative importance assigned to the variables (frequency $<$ others; or all roughly equal).  

% 
% 
% \begin{quote}
% \textbf{Questions}:
% 
% \begin{itemize}
% \item
%   Which model ends up being chosen, and what do you conclude about the research question?
% \item
%   What would be different about your answer to the research question using stepwise backwards selection versus nested model comparison? Why?
% \end{itemize}
% \end{quote}

\subsubsection{Gelman \& Hill's method}
\label{sec:gh-method-example}

This method requires more domain-specific knowledge, which we'll walk through; see Appendix~\ref{sec:vot-data} for more.
%or \citealp{sonderegger2017medium} for details.

We begin with model \ttt{mc\_mod\_1}, where all predictors are included as main effects---they are all expected to affect VOT for substantive reasons---as well as the interaction with \ttt{voicing} expected from previous work. The expected effect directions are:
\begin{itemize}
\item \ttt{speaking\_rate}:  negative effect for \tsc{voiceless} stops and a very small effect for \tsc{voiced} stops
\item \ttt{log\_corpus\_freq}:  negative effect
%(durations tend to shorten in more frequent words)
\item \ttt{cons\_cluster}, \ttt{foll\_high\_vowel}, \ttt{place}: positive effects expected %(VOT higher for non-labial stops, etc.)
\end{itemize}

The fitted model is:
<<>>=
gh_mod_1 <- mc_mod_1
tidy(gh_mod_1)
@

The \ttt{cons\_cluster}, \ttt{foll\_high\_vowel},  \ttt{voicing}, and \ttt{place} terms have much larger effect sizes than other main effects (remembering that the effect sizes are comparable because  we have standardized predictors), so we consider their two-way interactions.  Our expected directions for these interactions would be:
\begin{itemize}
\item \ttt{foll\_high\_vowel}:\ttt{place}: \textbf{positive}, because non-labial voiceless stops in English are often aspirated before high vowels \citep[e.g.,][]{nearey1994effects}, which would make the high/non-high difference larger when \ttt{voicing} is 1 (\tsc{voiceless})

\item \ttt{foll\_high\_vowel}:\ttt{cons\_cluster}, \ttt{cons\_cluster}:\ttt{place}: \textbf{unclear}
\item \ttt{voicing:cons\_cluster}: \textbf{unclear}
%perhaps negative due to English allophonic rules.\footnote{In English voiceless stop-approximant clusters (`train', `quick'), the approximant is often produced without voicing because some of the aspiration is produced on the approximant rather than the stop, which would decrease VOT. The same doesn't happen for voiced-stop approximant clusters. So we might think that the consonant cluster effect would be smaller for voiceless stops (\ttt{voicing}=1).  But this is a justification we thought of post-hoc.}
\item \ttt{voicing}:\ttt{foll\_high\_vowel}: \textbf{positive}, because some voiceless stops are aspirated before high vowels in English (same as above).
\item \ttt{voicing}:\ttt{place}: \textbf{negative}, because no interaction would be expected in VOT itself, which implies a larger \ttt{place} effect for \tsc{voiced} stops in log(VOT).\footnote{This is my guess, from sparse discussion of the place effect differing between voiced and voiceless stops in previous work. If we assume that it doesn't differ,
%from the observation
%Our thinking here, without delving into the whole VOT literature, is 
%that while the place of articulation effect is often mentioned for VOT it isn't often mentioned that this differs between voiced and voiceless stops, so let's assume that it doesn't.  Since voiced stops have much lower VOT, 
this means a larger difference in log(VOT) for voiced stops.}
\end{itemize}

We do not consider three-way interactions, where the large \ttt{voicing:speaking\_rate} interaction would be interacted with other terms with large effect sizes, to keep the model interpretable for this example and because we would not have clear predicted directions for these three-way interactions.

We could add in the interactions one at a time,  but for brevity let's add them all at once:

<<>>=
gh_mod_2 <- update(gh_mod_1, . ~ . +
    (voicing + foll_high_vowel + cons_cluster + place)^2)
tidy(gh_mod_2)
@

We now move on to the step where we consider each term based on expected sign and its significance ($p<0.05$):
\begin{itemize}
\item Most terms have expected sign---leave them in.
\item \ttt{voicing}:\ttt{foll\_high\_vowel}: wrong sign and non-significant---remove.
\item \ttt{cons\_cluster}:\ttt{place}, \ttt{foll\_high\_vowel}:\ttt{cons\_cluster}: not significant and expected sign unclear---remove.
\item \ttt{voicing}:\ttt{cons\_cluster}:  significant, and expected sign unclear. Since interactions with \ttt{voicing} are of primary interest, it is prudent to keep  this term in the model, in case it's controlling for something that might otherwise  give spurious interactions of other variables with \ttt{voicing}.
\end{itemize}

The final model is:
<<>>=
gh_mod_3 <- update(gh_mod_2, . ~ . - voicing:foll_high_vowel -
    cons_cluster:place - foll_high_vowel:cons_cluster)
tidy(gh_mod_3)
@


We would conclude that \ttt{place}, \ttt{cons\_cluster}, and \ttt{speaking\_rate} modulate the \ttt{voicing} effect, with the \ttt{place} effect stronger than the others.

This conclusion differs from the stepwise method in not concluding that word frequency modulates \ttt{voicing}, rather than concluding that it has a weak but real effect.  Whether this difference matters is a matter for interpretation in the context of our study. 
%If we asked the research question primarily motivated by frequency effects (with other predictors considered as controls), this is an important difference.  If it'

% 
\begin{boxedtext}{Practical note: A fallible master recipe}
 
The last two chapters have contained a lot of information, and it seems natural to sum up by giving you a suggested recipe for building a multiple linear regression model.  But it should also be clear by now that there is no single recipe; what steps your analysis should contain depends on your research questions, data, and modeling philosophy. 

Nonetheless, there is a list of steps you should always consider in regression analysis, and they could be thought of as a `master recipe' if you would like somewhere to start.  Below is a possible master recipe, which I have given in instructions for linear regression class assignments. 

{\scriptsize
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Preliminaries}

  \begin{itemize}
  \item
    State the problem (what are the goals of this analysis?)
  \item
    Select the response(s) and relevant predictors
  \item
    Continuous variables: center and scale, possibly transform to normality
  \item
    Categorical variables: center / choose coding scheme
  \end{itemize}
\item\textbf{Do exploratory data analysis:}

  \begin{itemize}
  \item Examine exploratory plots and summary statistics relevant for analysis goals
    %What patterns are there that `should' come out in a statistical model?
  \item Identify potential outliers
  \item (If applicable) potential interactions, by-subject, by-item plots
  \end{itemize}
\item
  \textbf{Choose models to fit}, based on steps 1-2
\item
  \textbf{Fit candidate model(s)}
\item
  \textbf{Model validation}

  \begin{itemize}
  \item
    Assess linearity assumption
  \item
    Examine distribution of (standardized) residuals
  \item
    Examine plots of residuals against fitted values, predictors
  \item
    Look for influential, outlier, high-leverage points
  \item
    Check for high collinearity and possibly address if high
  \item Check for overfitting (optional)
%    (Other steps possible---especially checking model \emph{robustness}---but not covered in this book.)
  \end{itemize}
\item
  \textbf{Revise}: Based on step 5, possibly:

  \begin{itemize}
  \item
    Exclude some data
  \item
    Transform the predictor and/or response
  \item
    Then refit the model (Step 4)
  \end{itemize}
\item
  \textbf{Iterate}: repeat Steps 4-6 until model validates well
\end{enumerate}
}
\end{boxedtext}

%% FUTURE: revisit this recipe.  Seems like could suggest some places to read? 
%% At least some kind of summary seems justified here...
% 
% \section{Summary: building a multiple linear regression model}
% 
% The last two chapters have contained a lot of information, and it seems natural to sum up by giving you a suggested recipe for building a multiple linear regression model.  But it should also be clear by now that there is no single recipe; what steps your analysis should contain depends on your research questions, data, and modeling philosophy. 
% 
% Instead here is a list of steps 
% 
% Nonetheless, there is a list of steps you should always consider in regression analysis, and they could be thought of as a `master recipe' if you would like somewhere to start.  Below is a possible master receipe, which I have given in instructions for linear regression class assignments.  Others are given in 
% 
% % 
% % 
% % 
% % Caveat: there is no single recipe for building a regression model of a dataset to address a research question! Nonetheless, here is one possible recipe, which we have given as part of linear regression project directions in classes.\footnote{Note that this recipe explicitly includes exploratory data analysis, which may not be appropriate depending on your analytic philosophy or conventions in your subfield.}
% 
% \begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \item
%   \textbf{Preliminaries}
% 
%   \begin{itemize}
%   \item
%     State the problem (what are the goals of this analysis?)
%   \item
%     Select the reponse(s) and relevant predictors
%   \item
%     Continuous variables: center and scale, possibly transform to normality
%   \item
%     Categorical variables: center / choose coding scheme
%   \end{itemize}
% \item
%   \textbf{Do exploratory data analysis}:
% 
%   \begin{itemize}
%   \item
%     Examine exploratory plots and summary statistics. What patterns are there that `should' come out in a statistical model?
%   \item
%     Identify potential outliers
%   \item
%     (if applicable) Possible interactions? By-subject, by-item plots.
%   \end{itemize}
% \item
%   \textbf{Choose models to fit}, based on steps 1-2
% \item
%   \textbf{Fit candidate model(s)}
% \item
%   \textbf{Model criticism}
% 
%   \begin{itemize}
%   \item
%     Assess linearity assumption
%   \item
%     Examine distribution of (standardized) residuals
%   \item
%     Examine plots of residuals against fitted values, predictors
%   \item
%     Look for influential, outlier, high-leverage points
%   \item
%     Check for high collinearity and possibly address if high
%   \item Check for overfitting (optional)
% %    (Other steps possible---especially checking model \emph{robustness}---but not covered in this book.)
%   \end{itemize}
% \item
%   \textbf{Revise}: Based on step 5, possibly:
% 
%   \begin{itemize}
%   \item
%     Exclude some data
%   \item
%     Transform the predictor and/or response
%   \item
%     Then refit the model (Step 4)
%   \end{itemize}
% \item
%   \textbf{Iterate}: repeat Steps 4-6 until model criticism is satisfactory
% \end{enumerate}
% 
% \end{boxedtext}



\section{Other reading}

Our discussions of regression diagnostics and model validation draw on \citet[][chap.~4, 6, 9]{chatterjee2012regression}, \citet[][chap.~6, 7, 9]{faraway2015linear}, and \citet[][chap.~2, 4, 5]{harrell2015regression}.  Other valuable sources are \citet[][chap.~8]{fox2019r},
%(which describes extensive functionality for diagnostic plots), 
\citet[][chap.~4--5]{gelman2007data};
%(especially on interpretation of linear regressions)---and 
for linguistic data \citet[][\S6.2]{baayen2008analyzing}, \citet[][\S5.6]{gries2021statistics}, and \citet[][\S4.4, 6.3]{winter2019statistics} are most extensive.
%and  (from whom we got the Faraway ``not grossly wrong'' quote) are most extensive.  
\citet{zuur2010protocol} summarizes all these issues and gives a protocol for addressing them through `data exploration'; \citet{zuur2007analyzing} gives more detail. Both are part of the rich literature on data analysis for ecologists, which tends to be readable, thorough, and easily portable to linguistic data.  

Many sources cover the bias/variance and overfitting/underfitting trade-offs, such as \citet[][chap.~6]{mcelreath2020statistical} and \citet[][chap.~7]{hastie2009elements}, and model comparison/variable selection (e.g.,~\citealp[][chap.~11]{chatterjee2012regression}; \citealp[][chap.~10]{faraway2015linear}). \citet[][chap.~16]{winter2019statistics} and \citet[][\S5.5]{gries2021statistics} discuss model selection for linguistic data. All sources emphasize the role of hypotheses and domain knowledge.

\section{Exercises}

\exer{We claimed in Section~\ref{sec:non-normal-resid-ex} that the residuals of model \ttt{english\_mod\_1} are bimodal because the \ttt{AgeSubject} predictor has not been included in the model.  Make a plot that confirms this. \label{ex:non-normal-resid}}

%\exer{Exercise FUTURE shows how non-constant variance could indicate a missing interaction term.}

%% this is: fit a model of
%% temp_mod <- lm(log_vot ~ speaking_rate+voicing, data=vot_michael)
%% can add other effects if you want.  

\exer{We suggested in Section~\ref{sec:linearity-assumption} that the  residual plots in Figure~\ref{fig:rate-voicing-resids} look better once two missing nonlinearities---an interaction and a nonlinear \ttt{speaking\_rate} effect---are included in \ttt{vot\_mod\_1}.  Verify that this is the case:  \label{ex:vot-missing-terms}}

\subexer{Refit \ttt{vot\_mod\_1} such that the effect of \ttt{speaking\_rate} is allowed to be quadratic, and to interact with \ttt{voicing}.}

\subexer{Examine the coefficient table for the resulting model.  Can you interpret what each row involving \ttt{speaking\_rate} means? (There should be four such rows.) Which row(s) confirm that there is a non-linear \ttt{speaking\_rate} effect in this data, and that \tsc{voiced} and \tsc{voiceless} stops show different rate effects?}

\subexer{Make the same residual plot as in Figure~\ref{fig:rate-voicing-resids} (left), but for the new model. (The code for the original plot is in the code file for this chapter if you need it.)  In what ways does the new plot look `better' than the original plot?  What remaining issue with residuals is indicated by the new plot?}

%% FUTURE: these exercises
% \exer{This is a simple example of a common cause of high leverage: a non-normally distributed predictor.  When this is the case, often transforming the predictor to normality results in more equal leverage across obseravtions.  (For this case, Exercise ...) \label{ex:better-leverage}}
% 

\exer{Verify this statement from Section~\ref{sec:influence}: ``There are are arguably no highly influential points for model \ttt{vot\_mod\_2}, even though there were outliers.'' You should use notions of `outliers' and `influential' defined in the text. \label{ex:vot-influence}}

% FUTURE: 
% \exer{ For the \ttt{vot\_michael} data, $R^2$ for pairs of (1)--(3) are 0.5--0.43, while $R^2 = 0.75$ for predicting (1) from (2) and (3).: exercise show this}

% FUTURE: maybe put this in
%\exer{Do model selection example but using BIC instead of AIC. How does the qualitative conclusion differ?  (A: frequency interaction now gone)}

%% FUTURE: add this exercise in and reference to it where it's referred to in text
%% (FUTURE: exercise - not the case if you use english\_40 instead)  



% 
% \hypertarget{c2solns}{%
% \section{Solutions}\label{c2solns}}
% 
% \hypertarget{multiple-linear-regression-solutions}{%
% \subsection{Multiple linear regression: Solutions}\label{multiple-linear-regression-solutions}}
% 
% \textbf{Q}: What does it mean that this coefficient is positive?
% 
% \textbf{A}: The effect of \texttt{WrittenFrequency} is closer to zero (= less negative) for younger speakers.
% 
% \hypertarget{c2sol1}{%
% \subsubsection*{Regression equation 1}\label{c2sol1}}
% \addcontentsline{toc}{subsubsection}{Regression equation 1}
% 
% \begin{equation*}
%   \text{Predicted } \texttt{RTlexdec} = \underbrace{6.552}_{\hat{\beta}_0} + (\underbrace{-0.011}_{\hat{\beta}_1} \cdot 3) + (\underbrace{-0.365}_{\hat{\beta}_2} \cdot \underbrace{0}_{\texttt{AgeSubject=='old'}}) + (\underbrace{-0.005}_{\hat{\beta}_3} \cdot 3 \cdot \underbrace{0}_{\texttt{AgeSubject=='old'}}) = 6.519
% \end{equation*}
% 
% \hypertarget{c2sol2}{%
% \subsubsection*{Regression equation 2}\label{c2sol2}}
% \addcontentsline{toc}{subsubsection}{Regression equation 2}
% 
% \begin{equation*}
%   \text{Predicted } \texttt{RTlexdec} = \underbrace{6.552}_{\hat{\beta}_0} + (\underbrace{-0.011}_{\hat{\beta}_1} \cdot 3) + (\underbrace{-0.365}_{\hat{\beta}_2} \cdot \underbrace{1}_{\texttt{AgeSubject=='young'}}) + (\underbrace{-0.005}_{\hat{\beta}_3} \cdot 3 \cdot \underbrace{1}_{\texttt{AgeSubject=='young'}}) = 6.139
% \end{equation*}
% 
% \hypertarget{linear-regression-assumptions-solutions}{%
% \subsection{Linear regression assumptions: Solutions}\label{linear-regression-assumptions-solutions}}
% 
% \textbf{Q}: Is this the case?
% 
% \textbf{A}: It looks like \texttt{rintensity} and \texttt{rpitch} have a roughly linear relationship, while the relationships between \texttt{rpitch} and \texttt{rduration} as well as between \texttt{rduration} and \texttt{rpitch} are nonlinear.

% \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
% 
% \textbf{Q}: Why do the residuals have this distribution?
% 
% \textbf{A}: The distribution of \(Y\) is highly right-skewed: there is a lot of data with \(Y \approx 1\), and a long tail of data with \(Y \in [2,5]\) (examine \texttt{hist(data\$rhymeRating)}). As a result, the fitted line is near \(Y=1\) for most values of \(X\), so the residuals also have a right-skewed distribution: points with \(Y \approx 1\) correspond to residuals near the mode, and other points correspond to residuals in the right tail. In short, the residual distribution is highly skewed because the distribution of the response (\(Y\)) is highly skewed.

% \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
% 
% \textbf{Q}: Is the linearity assumption met in each case?
% 
% \textbf{A}: No (left), Yes (right). The linearity assumption isn't met when \(Y\) is untransformed word frequency --- or at least it's unclear -- whereas the assumption looks safe for log-transformed \(Y\).
% 
% \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
% 
% \hypertarget{model-comparison-solutions}{%
% \subsection{Model comparison: Solutions}\label{model-comparison-solutions}}
% 
% \textbf{Q}: What are the reduced model and full model in the \(F\) test reported at the bottom of every linear regression's output?
% 
% \textbf{A}: The full model is the model you fitted in the linear regression. The reduced model is the model including only an intercept. For example, in this model:
% 
% <<>>=
% m2 <- lm(RTlexdec ~ WrittenFrequency + AgeSubject + LengthInLetters, english) 
% summary(m2)
% @
% 
% The \(F\) test is for the comparison with the model \texttt{m0\ \textless{}-\ lm(RTlexdec\ \textasciitilde{}\ 1,\ english)}.
% 
% \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
% 
% \textbf{Q}: Which model of \texttt{m1}, \texttt{m2}, \texttt{m3} (from \protect\hyperlink{c2ex4}{the example above}) would you choose based on nested model comparison (\(F\) test)?
% 
% \textbf{A}: Model 3 \textgreater{} Model 2 \textgreater{} Model 1
% 
% \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
% 
% \textbf{Q}: How do these interpretations differ from the coefficient interpretations for the model using raw predictors (m6)? What is the relative importance of the three predictors for predicting reaction time?
% 
% \textbf{A}:
% 
% \begin{itemize}
% \item
%   Original model: change in RT per change of 1 in word frequency
% \item
%   New model: change in RT per change of 2 SD in word frequency
% \end{itemize}
% 
% The interpretation of the \texttt{LengthInLetters} coefficient is:
% 
% \begin{itemize}
% \item
%   Original model: change in RT for each additional letter in the word
% \item
%   New model: change in RT per change of 2 SD word length (in letters)
% \end{itemize}
% 
% The interpretation of the \texttt{AgeSubject} coefficient is the same in the two models. Although \texttt{AgeSubject} is a two-level factor in one model and a centered numeric variable in the other model, the regression coefficient captures the difference between the two levels (old and young speakers) in each case.
% 
% The relative importance of the three predictors follows from the magnitudes of the coefficients in the model using standardized predictors: \texttt{AgeSubject} \textgreater{} \texttt{WrittenFrequency} \textgreater{} \texttt{LengthInLetters}.
